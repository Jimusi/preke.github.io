<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ACL 2019 一些感兴趣的paper</title>
      <link href="/2019/07/09/acl_2019/"/>
      <url>/2019/07/09/acl_2019/</url>
      
        <content type="html"><![CDATA[<p>不久前放出了ACL2019的paper list, 但是还没有具体的文章，这里立一些flag, 找一些感兴趣的paper来读。</p><h2 id="text-style-transfer">Text style transfer</h2><p><strong><em>Towards Fine-grained Text Sentiment Transfer</em></strong> Fuli Luo, Peng Li, Pengcheng Yang, Jie Zhou, Yutong Tan, Baobao Chang, Zhifang Sui and Xu SUN</p><p>这个跟我之前做的问题比较像，比较感兴趣…</p><p><strong><em>A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer</em></strong> Chen Wu, Xuancheng Ren, Fuli Luo and Xu SUN</p><h2 id="open-domain-dialog-system-text-generation">Open-domain Dialog System &amp; Text Generation</h2><h3 id="emotion">Emotion</h3><p><strong><em>Generating Responses with a Specific Emotion in Dialog</em></strong> Zhenqiao Song, Xiaoqing Zheng, Lu Liu, Mu Xu and Xuanjing Huang</p><p>这个很直接的老问题…不知道为什么还被录，可能是效果确实很牛或者模型很新吧..</p><p><strong><em>Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset</em></strong> Hannah Rashkin, Eric Michael Smith, Margaret Li and Y-Lan Boureau</p><p>这个像是一个开坑的工作…虽然 Empathetic computing 之前也有一些work, 但是不知道 ACL 为什么今年只有一篇…</p><p><strong><em>Adversarial Attention Modeling for Multi-dimensional Emotion Regression</em></strong> Suyang Zhu, Shoushan Li and Guodong Zhou</p><p>Emotion Regression 可以科普一下</p><p><strong><em>Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing</em></strong> Sijie Mai, Haifeng Hu and Songlong Xing</p><p>Affective Computing的</p><p><strong><em>Entity-Centric Contextual Affective Analysis</em></strong> Anjalie Field and Yulia Tsvetkov</p><p>Affective Computing的</p><h3 id="personalization">Personalization</h3><p><strong><em>Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good</em></strong> Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang and Zhou Yu</p><p>特殊场景？</p><p><strong><em>Personalizing Dialogue Agents via Meta-Learning</em></strong> Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu and Pascale Fung</p><p>Meta-learning没了解过，通过这个科普一下</p><p><strong><em>Automatic Generation of Personalized Comment Based on User Profile</em></strong> Wenhuan Zeng, Abulikemu Abuduweili, Lei Li and Pengcheng Yang</p><p>可以看一下是如何利用 User Profile 的</p><p><strong><em>Incorporating Textual Information on User Behavior for Personality Prediction</em></strong> Kosuke Yamada, Ryohei Sasano and Koichi Takeda</p><p>关于心理学定义的人格的paper感觉都可以关注一下…</p><h3 id="others">Others</h3><p><strong><em>Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention</em></strong> Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan and William Yang Wang</p><p>William Yang Wang在微博上推广过这个…</p><p><strong><em>Learning from Dialogue after Deployment: Feed Yourself, Chatbot!</em></strong> Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare and Jason Weston</p><p>看标题像是一个online-learning的问题</p><p><strong><em>Dialogue Natural Language Inference</em></strong> Sean Welleck, Jason Weston, Arthur Szlam and Kyunghyun Cho</p><p>这个title, 结合了不知道什么意思</p><p><strong><em>Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References</em></strong> Lisong Qiu, Juntao Li, Wei Bi, Dongyan Zhao and Rui Yan</p><p>看起来像是一个Grounding的问题</p><p><strong><em>Pretraining Methods for Dialog Context Representation Learning</em></strong> Shikib Mehri, Evgeniia Razumovskaia, Tiancheng Zhao and Maxine Eskenazi</p><p>问题很感兴趣，应用场景应该是多轮对话吧</p><p><strong><em>Self-Supervised Dialogue Learning</em></strong> Jiawei Wu, Xin Wang and William Yang Wang</p><p>自监督？</p><p><strong><em>Domain Adaptive Dialog Generation via Meta Learning</em></strong> Kun Qian and Zhou Yu</p><p>每个名词都很感兴趣…</p><p><strong><em>Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment</em></strong> Siqi Bao, Huang He, Fan Wang, Rongzhong Lian and Hua Wu</p><p>关于策略的也很感兴趣，看是如何用其他Knowledge</p><p><strong><em>Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study</em></strong> Chinnadhurai Sankar, Sandeep Subramanian, Chris Pal, Sarath Chandar and Yoshua Bengio</p><p>多轮对话？Bengio？</p><p><strong><em>Boosting Dialog Response Generation</em></strong> Wenchao Du and Alan W Black</p><p>Boosting有点意思…</p><p><strong><em>Implicit Discourse Relation Identification for Open-domain Dialogues</em></strong> Mingyu Derek Ma, Kevin Bowden, Jiaqi Wu, Wen Cui and Marilyn Walker</p><p>这个一作好像认识…</p><p><strong><em>ConvLab: Multi-Domain End-to-End Dialog System Platform</em></strong> Sungjin Lee, Qi Zhu, Ryuichi Takanobu, Xiang Li, Yaoqin Zhang, Zheng Zhang, Jinchao Li, Baolin Peng, Xiujun Li, Minlie Huang and Jianfeng Gao</p><p>感兴趣 Multi-Domain</p><p><strong><em>ADVISER: A Dialog System Framework for Education &amp; Research Daniel Ortega, Dirk Väth, Gianna Weber, Lindsey Vanderlyn, Maximilian</em></strong> Schmidt, Moritz Völkel, Zorica Karacevic and Ngoc Thang Vu</p><p>特定应用场景</p><p><strong><em>Dialogue-Act Prediction of Future Responses based on Conversation History</em></strong> Koji Tanaka, Junya Takayama and Yuki Arase</p><p>Prediction 感兴趣</p><p><strong><em>Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation</em></strong> Cao Liu, Shizhu He, Kang Liu and Jun Zhao</p><p>刘康老师的work呀</p><p><strong><em>Learning to Abstract for Memory-augmented Conversational Response Generation</em></strong> Zhiliang Tian, Wei Bi, Xiaopeng Li and Nevin L. Zhang</p><p>Memory-augmented感兴趣</p><p><strong><em>Neural Response Generation with Meta-words</em></strong> Can Xu, wei wu, Chongyang Tao, Huang Hu, Matt Schuerman and Ying Wang</p><p>Meta-words很感兴趣</p><p><strong><em>OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs</em></strong> Seungwhan Moon, Pararth Shah, Anuj Kumar and Rajen Subba</p><p>和Knowledge graph的结合</p><p><strong><em>E3: Entailment-driven Extracting and Editing for Conversational Machine Reading</em></strong> Victor Zhong and Luke Zettlemoyer</p><p><strong><em>Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling</em></strong> Yifan Gao, Piji Li, Irwin King and Michael R. Lyu</p><p><strong><em>Proactive Human-Machine Conversation with Explicit Conversation Goal</em></strong> Wenquan Wu, Zhen Guo, Xiangyang Zhou, Hua Wu, Xiyuan Zhang, Rongzhong Lian and Haifeng Wang</p><p><strong><em>Fine-Grained Sentence Functions for Short-Text Conversation</em></strong> Wei Bi, Jun Gao, Xiaojiang Liu and Shuming Shi</p><p><strong><em>Target-Guided Open-Domain Conversation</em></strong> Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing and Zhiting Hu</p><p><strong><em>Microsoft Icecaps: An Open-Source Toolkit for Conversation Modeling</em></strong> Vighnesh Leonardo Shiv, Chris Quirk, Anshuman Suri, Xiang Gao, Khuram Shahid, Nithya Govindarajan, Yizhe Zhang, Jianfeng Gao, Michel Galley, Chris Brockett, Tulasi Menon and Bill Dolan</p><p>小冰升级了么…</p><p><strong><em>Sentence Level Curriculum Learning for Improved Neural Conversational Models</em></strong> Sean Paulsen</p><p>Curriculum Learning很感兴趣</p><h2 id="general-nlp">General NLP</h2><p><strong><em>Towards Explainable NLP: A Generative Explanation Framework for Text Classification</em></strong> Hui Liu, Qingyu Yin and William Yang Wang</p><p>这个看标题挺厉害的…</p>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> conversation generation </tag>
            
            <tag> emotion </tag>
            
            <tag> text style transfer </tag>
            
            <tag> personalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小冰如何生成个性化回复</title>
      <link href="/2019/07/08/xiaobing/"/>
      <url>/2019/07/08/xiaobing/</url>
      
        <content type="html"><![CDATA[<p>小冰被设计为一个18岁的女孩，可靠，富有同情，感性而幽默。</p><blockquote><p>The XiaoIce persona is designed as a 18-year-old girl who is always reliable, sympathetic, affectionate, and has a wonderful sense of humor.</p></blockquote><p>那这些属性在生成回复的时候如何产生影响呢？ 这里就涉及到两个模块：Empathetic Computing 和 Core Chat</p><h2 id="empathetic-computing">Empathetic Computing</h2><p>这部分包括：</p><ul><li>Contextual Query Understanding</li><li>User Understanding</li><li>Interpersonal Response Generation</li></ul><h3 id="contextual-query-understanding">Contextual Query Understanding</h3><p>这一部分就可以理解为是根据上下文进行query expansion, 做一些指代消歧，上下文补充等。 总的来说就是对输入query <span class="math inline">\(Q\)</span>, 结合上下文 <span class="math inline">\(C\)</span> (历史会话信息），生成带有上下文信息的query <span class="math inline">\(Q_c\)</span>:</p><p><span class="math display">\[Q_c = f_{cqu}(Q, C)\]</span></p><h3 id="user-understanding">User Understanding</h3><p>这一部分是根据上下文 <span class="math inline">\(C\)</span> 和 query <span class="math inline">\(Q_c\)</span> 生成用户的信息 <span class="math inline">\(e_Q\)</span>，是一个用户共情向量，包括用户的属性信息（如果有的话），对话的topic, intent；也还包括emotion 和 sentiment的分析；<span class="math inline">\(e_Q\)</span>就如下图（一直用neural来表示中立，不知道是不是typo…)：</p><figure><img src="https://upload-images.jianshu.io/upload_images/2675254-09a2c70e80ce9bca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><figcaption>image.png</figcaption></figure><p>形式化就是：</p><p><span class="math display">\[e_Q = f_{uu}(Q_c, C)\]</span></p><h3 id="interpersonal-response-generation">Interpersonal Response Generation</h3><p>这一部分是通过<span class="math inline">\(e_Q\)</span>中的会话信息和小冰预设的 persona profile (key-value pairs) 生成一个回复共情向量<span class="math inline">\(e_R\)</span>:</p><figure><img src="https://upload-images.jianshu.io/upload_images/2675254-129d192aff85f8f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image1.png"><figcaption>image1.png</figcaption></figure><p>形式化就是：</p><p><span class="math display">\[e_R = f_{irg}(e_R, Persona\  profile\  of \ Xiaoice)\]</span></p><h2 id="core-chat">Core Chat</h2><p>有了上述的 <span class="math inline">\(\{Q_c, C, e_Q, e_R\}\)</span> 之后，在这部分结合这些信息生成回复。我这里只比较关系如何用generation-based方法去生成，如下图所示：</p><figure><img src="https://upload-images.jianshu.io/upload_images/2675254-ce68704903dae579.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><figcaption>image.png</figcaption></figure><p>模型基于RNN的seq2seq model, 将 <span class="math inline">\(Q_c\)</span> 作为 encoder的输入，得到hidden state vector再输入到decoder; 而在解码每个词的时候，都是通过一个包含了<span class="math inline">\(e_Q, e_R\)</span> 信息的向量 <span class="math inline">\(v\)</span> 去加入上下文和小冰的属性信息；而这个 <span class="math inline">\(v\)</span> 是通过如下公式来得到：</p><p><span class="math display">\[v = \sigma (W^T_Qe_Q + W^T_Re_R)\]</span></p><p>（我猜想每次结合的方式就是 <span class="math inline">\(v\)</span> 和 当前输入词向量<span class="math inline">\(e_t\)</span> concat一起之后再过一个线性层去规范维度</p><hr><p>通过以上这两个部分，小冰就能生成个性化的回复； 其实还是有两个点比较简单：</p><ol type="1"><li>对于属性的描述，用了很简单的key-value直接输入，对于具体属性如何影响回复生成并没有更深的modeling;</li><li>属性的表达上，其实也是比较简单的作用在了每次解码结合同样的信息，参考Emotional Chatting Machine这篇paper的思路的话，其实表达方式上，也是有一些多变的因素的。</li></ol><p>当然，可能具体使用上没办法去部署比较复杂的模型，也可能是因为小冰的数据足够多，而且其实大部分都还是用检索式，所以整体呈现的效果还确实不错~</p><h2 id="ref">Ref:</h2><p>Zhou L, Gao J, Li D, et al. The design and implementation of XiaoIce, an empathetic social chatbot[J]. arXiv preprint arXiv:1812.08989, 2018.</p>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> conversation generation </tag>
            
            <tag> personalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generating Multiple Diverse Responses for Short-Text Conversation</title>
      <link href="/2019/07/04/Mul_response/"/>
      <url>/2019/07/04/Mul_response/</url>
      
        <content type="html"><![CDATA[<p>Paper链接: https://arxiv.org/abs/1811.05696</p><p>腾讯AI Lib的一篇paper, 发在 AAAI2019, 主要是解决对话中“一对多”的问题</p><p>目前我们做对话生成的模型大多都是基于seq2seq, 因为在 machine translation or text summarization 的任务中，文本生成的效果确实不错。 但是本质上，seq2seq是一个“一对一”的问题，然而对话，可能存在一个post，多种回复（语义不相同，没有词overlap)都是合适的。退一步讲，我们经常用的对话数据如微博, twitter，这些数据本身也是一对多（多条评论）的。</p><h3 id="problem-formulation">Problem formulation</h3><p>训练数据为 <span class="math inline">\(\{ (x, \{ y \} )\}\)</span>, 即给定一个query <span class="math inline">\(x\)</span>, 目标是去生成一个responses的集合 <span class="math inline">\(\{ y \}\)</span>, 通过引入一个中间变量 <span class="math inline">\(z\)</span> 建立 <span class="math inline">\(x\)</span> 和 $ { y } $ 的联系; 具提来说是去最小化loss:</p><p><span class="math display">\[ J(\theta) = \mathcal L( \{ y \}| x) = \mathbf E_{p(z|x) }[\mathcal L(\{y\}|x,z)]\]</span></p><p>而中间这个 <span class="math inline">\(z\)</span> , 作者是用一些采样出来的words来表示的。相应的，<span class="math inline">\(p(z|x)\)</span> 就是words 的 distribution. 作者用了一个一个双向 GRU 来 encode <span class="math inline">\(x\)</span> to <span class="math inline">\(h_x\)</span>, 再过一个softmax去算<span class="math inline">\(z\)</span> 的概率分布，就像一个简单的分类器：</p><p><span class="math display">\[p(z|x) = softmax(W_2 \cdot tanh(W_1h_x + b_1) + b_2)\]</span></p><p>这个中间变量 <span class="math inline">\(z\)</span> 应该遵循</p><ul><li>可解释性：能解释与 <span class="math inline">\(x\)</span> 和 $ { y } $ 的联系</li><li>有区分度， 不同的 <span class="math inline">\(x\)</span> 应该产生不同的 <span class="math inline">\(z\)</span></li></ul><blockquote><p>虽然用采样出来的 words 能够满足这两点，但还是不很理解为什么用 words 作为中间变量，离散的words相当于割裂了两部分，直接用分布不好吗？去掉中间的采样过程？</p></blockquote><p>而对于 <span class="math inline">\(\mathcal L(\{y\}|x,z)\)</span>, 由于是估计多个 <span class="math inline">\(\mathbf y\)</span>, 那么作者做了一个简单的架设，用一个可微的函数 <span class="math inline">\(f\)</span> 把预测单个 <span class="math inline">\(\mathbf y\)</span> 的估计联合在了一起： <span class="math display">\[\mathcal L(\{y\}|x,z) = f_{\mathbf y\in\{y\}}(\mathcal l(\mathbf y|x, z))\]</span></p><h3 id="model">Model</h3><p>模型架构如下：</p><figure><img src="https://upload-images.jianshu.io/upload_images/2675254-9f6ba40e800dfdab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><figcaption>image.png</figcaption></figure><p>每次估计单个 <span class="math inline">\(\mathbf y\)</span> 的时候， 作者每一步将</p><ul><li>当前步骤的 hidden states <span class="math inline">\(h_y(t)\)</span></li><li><span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span> 的attention</li><li>解码过程 <span class="math inline">\(h_{\mathbf y}(t)\)</span> 和 <span class="math inline">\(x\)</span> 的 attention</li></ul><p>结合在一起去解码每一个词。</p><p>至于可微函数 <span class="math inline">\(f\)</span>, 考虑到多个 <span class="math inline">\(\mathbf y\)</span> 中，与 之前的 $ z $ 最相关的<span class="math inline">\(\mathcal L(\{y\}|x,z)\)</span> 应该最小，所以，这个 <span class="math inline">\(f\)</span> 就简单采用了 min函数：</p><p><span class="math display">\[f(\{y\}|x,z) = \min_{\mathbf y \in \{ y\}} \mathcal l(\mathbf y |x, z)\]</span></p><h3 id="training-with-rl">Training with RL</h3><p>前半部分 Modeling <span class="math inline">\(p(z|x)\)</span>, 是一个典型的生成词 (采样词) 的过程，作者这里用了RL里面的Policy Gradient去优化这个过程;</p><ul><li>为了缩小采样空间，作者先基于 $ x$ 和所有可能的<span class="math inline">\(\{y\}\)</span>构建了一个候选集 <span class="math inline">\(\mathbf Z_x\)</span>;</li><li>为了增加 <span class="math inline">\(K\)</span> 次采样 <span class="math inline">\(z\)</span> 的多样性, 作者增加了一些在 <span class="math inline">\(\mathbf Z_x\)</span> 聚类和判重的技巧；</li></ul><p>Reward function 被定义为了简单的 F1 score 来衡量每个生成的句子 <span class="math inline">\(\mathbf{\hat{y}}\)</span> 和 ground truth <span class="math inline">\(\mathbf y\)</span> 的 overlapping.</p><p>训练结束后，就可以用 <span class="math inline">\(p(z|x)\)</span> 概率 top-1000 的词作为候选集 <span class="math inline">\(\mathbf Z_x\)</span>, 然后再进行如上聚类和采样的过程，去生成多个responses.</p><h3 id="实验">实验</h3><p>实验数据就是用了微博和Twitter的数据，后面作者有公开源码和数据； Evaluation matrics用了 BLEU 和 Distinct-1/2, 其实感觉作者定义的 reward 函数就有点像 BLEU 这个metric…</p><blockquote><p>不太知道这样定义合不合适，因为BLEU，和作者定义的reward函数都是比较粗糙的去衡量生成文本的质量。这样就导致了这个问题有些强行拟合指标的嫌疑…</p></blockquote><h3 id="总结">总结</h3><p>这个问题很新，也比较切合实际，算是在挖一个很好的坑；</p><p>不过中间的一些过程可以尝试改进的地方应该还有很多，比如中间变量的设置，比如后面的reward函数</p><p>甚至，如果没有中间的离散变量，是不是也可以不用RL的方法去优化呢？</p><p>作者开放了源码：https://ai.tencent.com/ailab/nlp/dialogue.html</p>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> conversation generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读博第一年（2018~2019）</title>
      <link href="/2019/06/17/2018_2019/"/>
      <url>/2019/06/17/2018_2019/</url>
      
        <content type="html"><![CDATA[<p>其实意识到，很多事情都会随着成长而改变。</p><p>所以就想记录一下自己对各个问题当下的看法，也算是记录一些经历。</p><h2 id="读博学术">读博（学术）</h2><p>今年是2019， 去年的BERT一出来，原来那种改模型拼performance的想法慢慢的就佛了；</p><p>曾经想过一段时间如何去定位自己的工作；</p><p>后来渐渐的理解是，想要水论文，做一些有意思的新应用场景;</p><p>大厂，业界短期内不会商用的，或是现有阶段不适合，不支持业界部署的问题</p><p>像是做demo，绕开正面刚技术；</p><p>这种问题重在innovation, 如果好的想法做的完备，就比较适合通用的AI的刊和会去投；</p><p>至于NLP领域内，这类work的 contribution可能不会被大多做技术的认可。</p><h2 id="爱情">爱情</h2><p>目前的想法，其实没有什么精力去分给爱情…</p><p>感觉每天分给爱情的时间其实也就半个小时跟女票聊聊天</p><p>女票不粘人，我们各自都有很好的发展和规划，也远不到一起认真规划以后结婚生活的阶段</p><p>所以现在就是享受爱情</p><h2 id="金钱">金钱</h2><p>现阶段每个月会存下一些钱，但是也不是很多</p><p>买买港股小米的股票，或者定投就好了..</p><p>平时对生活质量的要求不算太高，有什么硬需求尽量满足就可以了</p><p>这一部分，没什么钱，也就没什么规划了</p><h2 id="人际关系">人际关系</h2><p>感觉学术里social其实还是个圈子的问题…</p><p>其实主动可以，我其实加了很多能接触到的NLP内比较厉害的人</p><p>但是更多的就是朋友圈点个赞</p><p>自己没有什么成果的，或者比较好的想法的话，空去谈合作，成功率可能不高</p><p>没人有义务带自己</p><p>所以我的理解就是先证明自己有能力发论文，然后再主动去谈合作吧…</p><p>不知道算不算拖延..</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Text style transfer papers</title>
      <link href="/2019/03/30/Text_style_transfer_papers/"/>
      <url>/2019/03/30/Text_style_transfer_papers/</url>
      
        <content type="html"><![CDATA[<h2 id="text-style-transfer">Text style transfer</h2><p>总结一下，Text style transfer可以分为两个部分：</p><ol type="1"><li>adverserial方法抽取semantic特征和style特征</li><li>如何生成特定style的句子</li></ol><p>模型，基本上都是VAE的天下。</p><p>存在的问题：</p><ol type="1"><li>目前没有人给text style一个很清晰的定义</li><li>很难把semantic 特征和style特征分得开(一些词既可以表达semantic信息，同时也可以表达style信息)</li><li>特定的style 和 特定的semantic 的合成句子质量很难有很好的表现</li><li>没有suitable的 evaluation方法</li></ol><p><strong>语言是基于符号的，符号的规则又是有限的（又是离散符号</strong></p><p><strong>或许从更上一层，考虑到人在生成想要表达的语言的思维（或许是连续的），如何去拟合可能限制会更少，并且更加流畅</strong></p><h3 id="what-is-wrong-with-style-transfer-for-texts"><font color="green">What is wrong with style transfer for texts?</font></h3><p>主要介绍了一些目前text style transfer的问题，比较适合去找research gap.</p><ul><li>Text style 的定义不太清晰，无法准确建模</li><li>Adverserial net 的方法很难剥离 content 和 style</li></ul><h3 id="style-transfer-from-non-parallel-text-by-cross-alignmentnips-2017">Style Transfer from Non-Parallel Text by Cross-Alignment（NIPS 2017)</h3><p>重点在于非平行数据的分析； <strong>最印象深刻的是约束两个styles对应的content同分布</strong>； 同时，做法也是adverserial的方式分离content 和 style 然后再生成</p><h3 id="evaluating-style-modification-in-text">Evaluating Style Modification in Text</h3><p>一个Master的毕业论文吧。。 首先提到了一个 <strong>word mover’s distance on texts with style masked out 我觉得这个应用到我们的model里去找同样content的句子是一个很好的方式 （这个应该是一个保留了sequencial和content信息的相似度衡量）</strong></p><p>主要从这几个方面入手：</p><figure><img src="http://static.zybuluo.com/Preke/9t4yzetlwbmu8q3x2nds8lnv/image_1d76fo7fc1n5a1hl6gjnju9k57m.png" alt="Evaluating Style Modification in Text"><figcaption>Evaluating Style Modification in Text</figcaption></figure><p><strong>提到一个mask style words的方法去做content preserving</strong>； 这里可能可以用到去：</p><ul><li>训练auto-encoder</li><li>寻找对应sentence</li></ul><p>有个Wordnet的Style-lexicon</p><p>这几种度量方法可以借鉴</p><h3 id="style-transfer-in-text-exploration-and-evaluation">Style Transfer in Text: Exploration and Evaluation</h3><p>用对抗的方法去剥离style信息 from content信息</p><p>propose two novel evaluation metrics：</p><ul><li>transfer strength</li><li>content preservation</li></ul><h3 id="evaluating-prose-style-transfer-with-the-bible">Evaluating prose style transfer with the Bible</h3><p>主要是提供了一个Bible的Text style transfer的平行数据集</p><h3 id="a-monolingual-tree-based-translation-model-for-sentence-simplification">A Monolingual Tree-based Translation Model for Sentence Simplification</h3><p>简化句子，类似于text summerization, 和 style transfer的区别和联系呢？</p><p>利用传统方法（语法解析树）的方式去做</p><h3 id="generating-sentences-from-a-continuous-space"><font color="green">Generating Sentences from a Continuous Space</font></h3><p>本文讲的这个问题也是我比较感兴趣的一个问题： &gt; However, by breaking the model structure down into a series of next-step predictions, the rnnlm does not expose an interpretable representation of global features like topic or of high-level syntactic properties.</p><p>用VAE能够学到一个全局的特征like style， topic and high-level syntactic features 去解决imputing missing words(补全缺失词) 的问题</p><p>我觉得肯定是可以用到style-transfer里去抽取特征的</p><h3 id="multiple-attribute-text-rewriting"><font color="green">MULTIPLE-ATTRIBUTE TEXT REWRITING</font></h3><p>用back-translation的方法去掉style，看一下是怎么论述不需要分开style-attribute这个说法的</p><h3 id="toward-controlled-generation-of-text">Toward Controlled Generation of Text</h3><p>用VAE的方法去生成特定的sentiment 或者 tenses的句子</p><h3 id="unpaired-sentiment-to-sentiment-translation-a-cycled-reinforcement-learning-approach">Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach</h3><p>用强化学习的方法去转换情感 还是一个先remove词语，再去生成的一个方法</p><h3 id="adversarially-regularized-autoencoders">Adversarially Regularized Autoencoders</h3><p>VAE, CVAE, AAE, ARAE, DAE</p>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Text style transfer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sentiment analysis papers</title>
      <link href="/2019/03/27/Sentiment_analysis_papers/"/>
      <url>/2019/03/27/Sentiment_analysis_papers/</url>
      
        <content type="html"><![CDATA[<h2 id="sentiment-analysis12-papers">Sentiment analysis（12 papers)</h2><p>直观感受是，但从sentiment这个角度讲</p><ul><li>单纯bag-of-words 目前已经不够了</li><li>Lexicons很重要</li><li>Word roles(POS)很重要</li><li>句法信息也很重要，但是不太好直接用</li><li>模型方面基本上baseline就是(Bi)LSTM+Attn了</li></ul><h3 id="sentiment-lexicon-enhanced-attention-based-lstm-for-sentiment-classification">Sentiment Lexicon Enhanced Attention-Based LSTM for Sentiment Classification</h3><p><font color="red">Current NN works 很少用到 Lexicons</font></p><p>Title 很好描述了本文工作</p><h3 id="a-multi-sentiment-resource-enhanced-attention-network-for-sentiment-classification">A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification</h3><p>Deep learning approaches for sentiment classification do not fully exploit <font color="red">sentiment linguistic knowledge.</font></p><ul><li>sentiment lexicon(此处有词干化，精细化处理）</li><li>negation words</li><li>intensity words</li></ul><p>sentiment resource 做 attention</p><h3 id="encoding-syntactic-knowledge-in-neural-networks-for-sentiment-classification">Encoding Syntactic Knowledge in Neural Networks for Sentiment Classification</h3><p>Rich <font color="red">syntactic knowledge</font> has not been fully explored when composing a longer text from its shorter constituent words</p><p>We discover that <strong>encoding syntactic knowledge (part-of-speech tag) in neural networks can enhance sentence/phrase representation</strong></p><p><code>tree-structured LSTM</code></p><p>其实出发点就是把句法信息加到模型中，这么说，其实语言也不单纯是一个sequencial的信息，但是为什么Tree-LSTM没有更加流行起来可能就是因为语法解析树太难了….不是目前大量跳AI坑的人说做就做的…</p><p>这个想法很有意思:</p><p><font color="red"><strong>We define sentiment-favorable representation as what is learned by a proper way of expressing sentiment, and is usually optimized with a sentiment-specific loss.</strong></font></p><h3 id="a-lexicon-based-supervised-attention-model-for-neural-sentiment-analysis">A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis</h3><p>Allows a recurrent neural network to <strong>focus on the sentiment content</strong>, thus generating <strong>sentiment-informative representations</strong>.</p><p>Sentiment degree based on sentiment lexicons (Attention 的权重）</p><h3 id="leveraging-multi-grained-sentiment-lexicon-information-for-neural-sequence-models">Leveraging Multi-grained Sentiment Lexicon Information for Neural Sequence Models</h3><p><strong>Multi-grained Sentiment Lexicon</strong> 多粒度级别Lexicons</p><p>The proposed method first <font color="red">encodes the fine-grained labels into sentiment embedding</font> and <font color="red">concatenates it with word embedding</font>.</p><figure><img src="http://static.zybuluo.com/Preke/9wma6ml3vpoglune4o3udj3n/image_1d6v2lakt12a69gtt1p1lr77l99.png" alt="image_1d6v2lakt12a69gtt1p1lr77l99.png-117.2kB"><figcaption>image_1d6v2lakt12a69gtt1p1lr77l99.png-117.2kB</figcaption></figure><p>这里说到表示程度的，否定词，都对分类有帮助。</p><p>提供了一个 Sentiment lexicon which contains:</p><ul><li>2759 positive words,</li><li>5111 negative words,</li><li>35 negation words and</li><li>62 intensifiers</li></ul><p>https://github.com/zengyan-97/Sentiment-Lexicon</p><h3 id="context-sensitive-lexicon-features-for-neural-sentiment-analysis"><font color="green">Context-Sensitive Lexicon Features for Neural Sentiment Analysis</font></h3><p>Most existing methods use sentiment lexicons without considering context, typically taking the count, sum of strength, or maximum sentiment scores over the whole input.(应该是一个比较重要的research gap)</p><p>Previous works:</p><ul><li>they do not explicitly handle semantic compositionality</li><li>they cannot effectively deal with word sense variations</li></ul><p>Model looks like:</p><figure><img src="http://static.zybuluo.com/Preke/u0zdwflmj1hpcqa2qxqocct3/image_1d6v3ll4hhk81mhkl1c1dat1ecb9.png" alt="image_1d6v3ll4hhk81mhkl1c1dat1ecb9.png-73.7kB"><figcaption>image_1d6v3ll4hhk81mhkl1c1dat1ecb9.png-73.7kB</figcaption></figure><h3 id="linguistically-regularized-lstm-for-sentiment-classification"><font color="green">Linguistically Regularized LSTM for Sentiment Classification</font></h3><p>在一个句子中不同的词，功能不同，对表征的贡献不同，本文是去find the role of the different kind of words.</p><p>Models are able to capture the <strong>linguistic role of sentiment words, negation words, and intensity words</strong> in sentiment expression.</p><p>our central idea is <font color="red">to regularize the difference between the predicted sentiment distribution of the current position, and that of the previous or next positions, in a sequence model</font>.</p><p>理解sentiment distribution? 和 style distribution 有何联系</p><h3 id="incorporating-lexicons-into-lstm-for-sentiment-classification">Incorporating Lexicons into LSTM for Sentiment Classification</h3><p>印鉴老师的paper.</p><p>也是考虑了词语在不同语境下的语义不同这个问题；</p><p>根本上也是在根据 word 的 role 去增加knowledge</p><p>Words in different part-of-speech correspond to different meanings, emotional polarity and score. The same word may have multiple meanings.</p><h3 id="imbalanced-text-sentiment-classification-using-universal-and-domain-specific-knowledge">Imbalanced text sentiment classification using universal and domain-specific knowledge</h3><p>考虑情感分析的两个问题：</p><ul><li>domain-sensitive</li><li>data imbalance</li></ul><p>Builds a <strong>domain-adaptive sentiment classification model</strong> that incorporates universal and domain-specific knowledge into a unified learning framework.</p><h3 id="saan-a-sentiment-aware-attention-network-for-sentiment-analysis">SAAN: A Sentiment-Aware Attention Network for Sentiment Analysis</h3><p>3步工作：</p><ul><li>a word-level mutual attention mechanism to model word-level correlation</li><li>a phrase-level convolutional attention is designed to obtain phrase-level correlation</li><li>a sentence-level multi-head attention mechanism is proposed to capture various sentimental information from different subspaces.</li></ul><h3 id="laan-a-linguistic-aware-attention-network-for-sentiment-analysis">LAAN: A Linguistic-Aware Attention Network for Sentiment Analysis</h3><p>同上文一样…</p><h3 id="attention-based-bilstm-network-with-lexical-feature-for-emotion-classification">Attention-Based BiLSTM Network with Lexical Feature for Emotion Classification</h3><p>Propose two simple models to fully learn the emotional features of the POS of words.</p><ol type="1"><li>把每个词的POS-tag 和 LSTM 输出 拼在一起做 attention</li><li>分别用LSTM+attn 对 context 表示 和 POS标签（应该是做输入）学习，然后把最终特征拼一起</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sentiment analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dialog system记录</title>
      <link href="/2019/01/01/dialog_system/"/>
      <url>/2019/01/01/dialog_system/</url>
      
        <content type="html"><![CDATA[<h2 id="section">2019.1.1</h2><p>找了一个代码实现了一下seq2seq(GRU+attention)在一个小的task based的数据集（44M）上的结果 发现BLEU最高也只能到6，而用英法翻译数据（1.5g）一个epoch之后，BLEU就达到了13+（时间太长没有继续跑下去</p><p>生成的很多有包含thanks for your feedback类似的，就是有可能生成一些general的response，并不能直接应用到生产环境中。</p><p>分析来看(推测）：</p><ol type="1"><li>数据量很小，不足以去训练seq2seq的model</li><li>task based可能还是更适合retrieval 的方法，generation的方法可能更适合free-talk</li></ol><p>code source: https://github.com/preke/FSE2019</p><h2 id="section-1">1.3</h2><p>昨晚和师兄讨论了一个domain adaption style transfer的idea; 师兄给了我一些已经写好的domain adaption的代码，然后结合这边原来写的seq2seq再完善一下，应该可以跑出来看看效果； 不过目前是要找到可以用的style transfer的数据集。</p><p>code source: https://github.com/preke/domain_adaption_style_transfer</p><h3 id="ref">ref:</h3><p>https://arxiv.org/abs/1804.06437 https://arxiv.org/pdf/1409.7495.pdf （目前参考这两个，但是应该会能够找到更多的一些paper在做这件事吧…）</p>]]></content>
      
      
      <categories>
          
          <category> 工作记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Style Transfer in Text Exploration and Evaluation</title>
      <link href="/2018/11/14/style_transfer/"/>
      <url>/2018/11/14/style_transfer/</url>
      
        <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>2 main problems in Style Transfer:</p><ul><li>Lack of parallel data<ul><li>Model learn from <font color="red">non-parallel data</font></li><li>Learn separate <strong>content representations</strong> and <strong>style representations</strong> using <strong>adversarial networks</strong>.</li></ul></li><li>Lack of reliable metrics<ul><li>propose two novel evaluation metrics that measure two aspects of style transfer: <strong>transfer strength</strong> and <strong>content preservation</strong></li></ul></li></ul><h2 id="contribution">Contribution</h2><ul><li>Compose a <a href="https://github.com/fuzhenxin/textstyletransferdata" target="_blank" rel="noopener">dataset</a> of paper-news titles to facilitate the research in language style transfer</li><li>Propose <strong>two general evaluation metrics</strong> for style transfer, which considers both transfer strength and content preservation. The evaluation metric is highly correlated to the human evaluation.</li></ul><h2 id="model">Model</h2><p>(both only contains the content information)</p><h3 id="multi-encoder">multi-encoder</h3><ul><li>The multi-decoder model uses different decoders, one for each style, to generate texts in the corresponding style.</li></ul><p>困难在于encoder如何生成只含有content信息的representation，不包含原来的style信息<font color="red">（有点不理解是为什么…)</font></p><p>设置目标函数用adversarial network 处理post的style分类：目标是 to separate the content representation from the style. 这里有两个loss: <img src="http://static.zybuluo.com/Preke/qhmh4o4giu7h1z3krd3n9qnb/image_1cs8n1gskhhu11r3ket1g1kaqbs.png" alt="Loss1"> to minimizes the negative log probability of the style labels in the training data. (这里<span class="math inline">\(\theta_c\)</span> 是 predict style的分类器的参数） <img src="http://static.zybuluo.com/Preke/y8oee035db5o9qpekcx5mhok/image_1cs8n5qv9v9k11611sqv1tu415sq19.png" alt="Loss2"> by maximize the entropy (minimize the negative entropy) of the predicted style labels, make the classifier unable to identify the style of <span class="math inline">\(x\)</span></p><p>然后，对于多个不同style的decoder: 还有一个传统的loss, 使得输入输出的语义更相似<font color="red">（这里我也有点不太认同）</font> <img src="http://static.zybuluo.com/Preke/9b6a2z85xixmuk43xn11b85b/image_1cs8nfl7k14a91ro31stf5r11l5o1m.png" alt="loss gen"></p><p>所以这个multi-encoder的总体loss为： <img src="http://static.zybuluo.com/Preke/ee85f56nen42jcewrseks92w/image_1cs8ngdnh1m6h127l4nc1jol15823.png" alt="total loss"></p><h3 id="style-embedding">style embedding</h3><p>与上述模型类似，只是在参数中加入了所有style categoris的embeddings的矩阵<span class="math inline">\(E\in R^{N*d_s}\)</span> <span class="math inline">\(N\)</span> for the number of styles <span class="math inline">\(d_s\)</span> for the dim of style embeddings</p><p>这部分的Loss为： <img src="http://static.zybuluo.com/Preke/vo7860x0z6e0jd2643citxrq/image_1cs8nq9t11a4r71e1nh69k2l7q2g.png" alt="Loss2"></p><ul><li>The style-embedding model learns style embeddings addition to the content representations.</li></ul><figure><img src="http://static.zybuluo.com/Preke/e1obhx207tcemqi309feb66d/image_1cs8ea9g6ic51tk367h1ob07gs9.png" alt="2 models"><figcaption>2 models</figcaption></figure><h2 id="metrics">Metrics:</h2><ul><li>Transfer Strength<ul><li>evaluates whether the style is transferred to target style(用LSTM-sigmoid构建一个分类器，通过acc衡量）</li></ul></li><li>Content Preservation<ul><li>evaluate the similarity between source text and target text</li></ul></li></ul><h2 id="dataset">Dataset</h2><ul><li>paper-news title dataset</li><li>positive-negative review dataset</li></ul><h2 id="acquisition">Acquisition：</h2><p>Model很直观，只是对于adversarial 的思路还是不太理解，有一些不理解的点标红了 （感觉没有体现出Contribution中说的parallel数据的特点）</p>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> style transfer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation</title>
      <link href="/2018/11/14/slstm/"/>
      <url>/2018/11/14/slstm/</url>
      
        <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><ul><li><p>poor logic and no emotion</p></li><li><p>a syntactically con- strained bidirectional-asynchronous approach for <code>emotional conversation generation</code> (E- SCBA) is proposed</p></li><li><p>In our model, pre-generated emotion keywords and topic keywords are asynchronously introduced into the process of decoding.</p></li></ul><h2 id="contribution">Contribution</h2><ol type="1"><li>It conducts a study of compound information, which constitutes the syntactic constraint in the conversation generation.</li><li>A bidirectional-asynchronous decoder with multi-stage strategy is proposed to utilize the syntactic constraint.</li><li>Our experiments show that E-SCBA work better on emotion, logic and diversity than the general seq2seq and other models that consider only a sin- gle factor during the generation.</li></ol><h2 id="dataset">Dataset</h2><ul><li>Emotional conversation dataset NLPCC2017</li><li>1,119,201 Chinese post-reply pairs</li><li>randomly sampled 8,000 for validation, 3,000 for testing and the rest for training</li></ul><h2 id="model">Model</h2><figure><img src="http://static.zybuluo.com/Preke/57nvin5t51ekhs4b8mtfki27/image_1cs61nfsh165dh4c1ph8bjpjagp.png" alt="image_1cs61nfsh165dh4c1ph8bjpjagp.png-328.8kB"><figcaption>image_1cs61nfsh165dh4c1ph8bjpjagp.png-328.8kB</figcaption></figure><p>Keywords dictionary: 1. Emotion dictionary 2. Topic dictionary</p><ul><li>Step1: 输入Post, 得到topic keyword and emotion keyword<ul><li>pretrained LDA 主题推断</li><li>emotion transfer network 得到emotion</li><li>(一些网络处理）得到topic keyword and emotion keyword that are expected to appear in the reply.</li></ul></li><li>Step2:<ul><li>Post过LSTM(加入Emotion keyword), 得到一系列hidden vectors <span class="math inline">\(\{s_i^{et}\}\)</span><br></li><li>Post过LSTM(加入Topic keyword, Emotional attention), 得到一系列hidden vectors <span class="math inline">\(\{s_j^{tp}\}\)</span></li><li><font color="red">LSTM和Attention的常用结合方式？</font></li><li>用<span class="math inline">\(\{s_i^{et}\}\)</span> 去 attention <span class="math inline">\(\{s_j^{tp}\}\)</span> 得到 <strong>Middle Sequence(<span class="math inline">\(y^{md}\)</span>)</strong> (这里<span class="math inline">\(y^{md}\)</span> 不代表输出的words, 仅仅是中间变量而已）如图：get middle sequence</li><li>然后通过<span class="math inline">\(y^{md}\)</span> 得到 <span class="math inline">\(y^{ce}\)</span> 和 <span class="math inline">\(y^{ct}\)</span> 如图：get ce and ct <img src="http://static.zybuluo.com/Preke/4edy641z8lopisgm15bogwu5/image_1cs728vnp1bsp1743gko17mcn379.png" alt="get middle sequence"> <br></li></ul></li></ul><figure><img src="http://static.zybuluo.com/Preke/qt9plkibwr5fvzt7wzwncnvi/image_1cs72kfu14s0s3b15am52fv88m.png" alt="get ce and ct"><figcaption>get ce and ct</figcaption></figure><blockquote><p>提醒自己一点，LSTM每个输出也只是hidden states, 具体应用到decoder是还要加入一个选择当前输出词语的softmax:<span class="math inline">\(P(w_i|w_{i-1}, h_i, c_i)\)</span></p></blockquote><ul><li>Step3:<br>concat 所有的东西 together: <span class="math inline">\(y^f=(y^{ct,b}, w_{tp}^k, y^{md,f}, w_{et}^k, y^{ce,f})\)</span> <span class="math inline">\(y^b\)</span> 就是 <span class="math inline">\(y^f\)</span> 的反向; And then 得到最终的hidden states 然后生成words:</li></ul><figure><img src="http://static.zybuluo.com/Preke/zptciuvc45fjyf2ffwj4ep1d/image_1cs739mit1j9lb0916ii3e7vvm9.png" alt="get hidden states"><figcaption>get hidden states</figcaption></figure><h2 id="experiment-and-metrics">Experiment and Metrics</h2><ul><li><strong>Embedding-based Metrics</strong>: We measure the similarity computed by <strong>cosine distance</strong> between a candidate reply and the target reply using <strong>sentence-level embedding</strong></li><li><strong>Distinct Metrics</strong>: By computing the number of different unigrams (Distinct-1) and bigrams (Distinct-2), we measure information and diversity in the candidate replies</li><li><strong>Human Annotations</strong></li></ul><h2 id="acquisition">Acquisition：</h2><p>方向的变换看不太懂，lstm的一些认识加深了一点</p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> conversation generation </tag>
            
            <tag> sentiment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Eliciting Positive Emotion through Affect-Sensitive Dialogue Response Generation A Neural Network Approach</title>
      <link href="/2018/11/14/EmoHERD/"/>
      <url>/2018/11/14/EmoHERD/</url>
      
        <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><ul><li><p>We build a fully data driven chat-oriented dialogue system that can dynamically mimic affective human interactions by utilizing a neural network architecture.</p></li><li><p>we propose a sequence-to-sequence response generator that <strong>considers the emotional context</strong> of the dialogue.</p></li><li><p>shows that incorporation of emotion into the training process helps reduce the perplexity of the generated responses,</p></li></ul><h2 id="model">Model</h2><ul><li>经典HERD</li></ul><figure><img src="http://static.zybuluo.com/Preke/r5an3or08it4vnq8btaamoy7/image_1cs8ufb1t1i1o17ep19m0162r11nqm.png" alt="HERD"><figcaption>HERD</figcaption></figure><p>(这个可以做多轮对话呀） <br></p><ul><li>Emo-HERD<ul><li>Incorporate an <strong>emotion encoder</strong> into the HRED architecture</li><li>The emotion encoder is placed in the same hierarchy as the dialogue encoder, capturing emotion at dialogue-turn level and maintaining the emotion context history throughout the dialogue</li></ul></li></ul><figure><img src="http://static.zybuluo.com/Preke/nvbhd3dqvy32dot5ihytdz20/image_1cs8sg97q1ij3ts92kc1s8a15sv2t.png" alt="Emo-HERD"><figcaption>Emo-HERD</figcaption></figure><p>在对单轮对话生成 <span class="math inline">\(h_{dlg}\)</span> 之后，把 <span class="math inline">\(h_{dlg}\)</span> 输入到一个 emotion encoder中结合历史信息 <span class="math inline">\(h_{m-1}^{emo}\)</span> 生成emotion embedding <span class="math inline">\(h_{emo}\)</span></p><figure><img src="http://static.zybuluo.com/Preke/rr7tkqgcm4oid3taya7446e8/image_1cs8uknnk9h014du17h0825bcl13.png" alt="generate hemo"><figcaption>generate hemo</figcaption></figure><p>emotion encoder这里有自己的损失函数 <span class="math inline">\(cost_{emo}\)</span> 总的Emo-HERD的损失函数为: <img src="http://static.zybuluo.com/Preke/cazlc4pzfpab4iq08x1qn6dz/image_1cs8uvs1ni5qr501061v9t1q0f1g.png" alt="cost"></p><h2 id="metrics">Metrics:</h2><ul><li>perplexity</li><li>subjective evaluation to measure the <strong>naturalnes</strong>s and <strong>emotional impact</strong> of the generated responses</li></ul><h2 id="dataset">Dataset</h2><ul><li>SubTle, a large scale conversational corpus</li><li>Spontaneous affective conversational corpus</li><li>Constructing positive-emotion eliciting data</li></ul><h2 id="section"></h2><h2 id="acquisition">Acquisition：</h2><ul><li>utterance: Dialog中一轮对话中的一个<font color="red">(单方的?）</font> sentences (感觉从原文看是这样的）</li></ul><p>感觉有点蹭热点的嫌疑呀，14年的HERD, 加上了一个emotion encoder就发了… 反正帮助我理解HERD了吧…</p>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> conversation generation </tag>
            
            <tag> emotion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Emotional Chatting Machine Emotional Conversation Generation with Internal and External Memory</title>
      <link href="/2018/11/14/ECM/"/>
      <url>/2018/11/14/ECM/</url>
      
        <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent).</p><p>ECM addresses the factor using three new mechanisms that respectively： - Models the high-level abstraction of emotion expressions by embedding emotion categories - Captures the change of implicit internal emotion states - Uses explicit emotion expressions with an <strong>external emotion vocabulary</strong></p><h2 id="contribution">Contribution</h2><ul><li>It proposes to address the <strong>emotion factor</strong> in large-scale conversation generation</li><li>It proposes an end-to-end framework (called ECM) to incorporate the emotion influence in large-scale conversation generation. It has three novel mechanisms: emotion category embedding, an internal emotion memory, and an external memory.</li><li>It shows that ECM can generate responses with higher content and emotion scores than the traditional seq2seq model.</li></ul><h2 id="problem">Problem:</h2><p>Given post $ X = [x_1,…x_n] $ and emotion factor <span class="math inline">\(e\)</span>, Output response <span class="math inline">\(Y = [y_1,...y_m]\)</span> coherent with the emotion <span class="math inline">\(e\)</span></p><p><span class="math display">\[P(Y|X,e)= \prod_{i=1}^m(P(y_i|y_{&lt;i},X,e)\]</span></p><h2 id="model">Model</h2><p><img src="http://static.zybuluo.com/Preke/3w8q9xm5wvlwbhfw133fpz7z/image_1csodth9h1ka6q9mcm7m3k1huf9.png" alt="ECM model"> 整体流程是： 先训练一个Emotion Classidier 来 annotate 训练数据的emotion; 然后将三元组(Post, response, target emotion)输入到ECM中，在decoder中结合target emotion的信息，依次生成每个word, (生成response) 然后再对response用同样的Emotion Classidier 做分类，得到response的情感来衡量效果</p><figure><img src="http://static.zybuluo.com/Preke/m8ewuvddl5icqvaw4wey1tqu/image_1csoduq9ah88e04f5m1qcso18m.png" alt="Internal Memory"><figcaption>Internal Memory</figcaption></figure><figure><img src="http://static.zybuluo.com/Preke/ded8bdig3g83wz5xe2w5a3di/image_1csodvkvn1af2b9n1r6c1mk51ilo13.png" alt="External Memory"><figcaption>External Memory</figcaption></figure><p>(中间的数学推断，看倒是看明白了，有点懒得誊一次…</p><h2 id="dataset">Dataset</h2><ul><li>NLPCC emotion classification dataset</li><li>STC conversation dataset *could found in the github: <a href="https://github.com/tuxchow/ecm" target="_blank" rel="noopener">source code</a></li></ul><h2 id="experiment-and-metrics">Experiment and Metrics</h2><ul><li>BLEU is not suitable for measuring conversation generation due to its low correlation with human judgment.</li><li>Perplexity(在读过的paper中最常见的）</li><li>Emotion accuracy</li></ul><figure><img src="http://static.zybuluo.com/Preke/0okuk2t5cip926nwjr4kokoa/image_1csoe3n91314cc21l9i16s8fom1g.png" alt="result1"><figcaption>result1</figcaption></figure><ul><li>Manual Evaluation</li></ul><h2 id="acquisition">Acquisition：</h2><p>过程感觉有点琐碎，细节上的修改和优化还是挺多的，不能说是一个很眼前一亮的工作吧（不是效果和问题不好，而是没那么直观），但是运行一下代码应该会有更加深的体会</p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> conversation generation </tag>
            
            <tag> emotion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Steering Output Style and Topic in Neural Response Generation</title>
      <link href="/2018/11/13/STO/"/>
      <url>/2018/11/13/STO/</url>
      
        <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><ul><li>We propose simple and flexible training and decoding methods for <strong>influencing output style and topic</strong> in neural encoder-decoder based language generation</li></ul><h2 id="model">Model</h2><h3 id="encoder-decoder">encoder-decoder</h3><p>Chapter 3 &amp; 4 解释了一对如何利用Bayes更好的设定目标函数从而s2s的decoder效果更好（更好的选择words)的数学解释，有点晕晕的。 （总之是在实验时对encoder-decoder进行了微调改进吧）</p><h3 id="chapter-5-style">Chapter 5: Style</h3><p>如果我们有一个数据集（database of target sentences) 在 neural encoder-decoder framework 的框架下，如何去影响style</p><ul><li>5.1 Ranking</li><li>Target is fixed</li><li><p>As a retrieval problem</p></li><li>5.2 Multiply<ul><li>2 models<ul><li>encoder-decoder model trained on a open-domain dataset</li><li>language model trained on target dataset</li></ul></li><li>在RNN step-by-step 生成each word 的概率为： <span class="math inline">\(P(t|S)^{\lambda_1}P(t)^{\lambda_2}\)</span><ul><li>其中 <span class="math inline">\(t\)</span> 是中间要生成的词，<span class="math inline">\(\lambda_1\)</span> 表示第一个model，<span class="math inline">\(\lambda_2\)</span> 表示第二个model；<span class="math inline">\(S\)</span> 是source, 个人理解这里为latent state vector(decoder的参数）</li></ul></li></ul></li><li>5.3 Finetune<ul><li>For each sentences in the target dataset, we assume there is a pseudo context; （也就是说，都有一个隐藏的上下文（概率模型），这里有点像VAE)</li></ul></li></ul><h3 id="chapter-6-topic">Chapter 6: Topic</h3><p>Counting Grid(完全不懂…）</p><h2 id="dataset">Dataset</h2><ul><li><a href="http://webscope.sandbox.yahoo.com/%20catalog.php?datatype=l" target="_blank" rel="noopener">Yahoo! Comprehensive Questions and Answers dataset</a> 来做ranking</li><li>Twitter Conversation Dataset（Sordoni et al., 2015）（HERD用的）</li></ul><h2 id="experiment-and-metrics">Experiment and Metrics</h2><p>Ranking problem: MRR P@1</p><p>Style: Human Evaluations</p><figure><img src="http://static.zybuluo.com/Preke/zd7zsvglmpulvy4k9276jjic/image_1cs6k3u35i70163t1hu12qf1b779.png" alt="Ranking result"><figcaption>Ranking result</figcaption></figure><hr><h2 id="acquisition">Acquisition：</h2><p>整体来说，对目前研究的问题帮助不明显，对于模型（encoder-decoder)改进方面有一些数学经验值得借鉴</p>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> conversation generation </tag>
            
            <tag> style transfer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sentiment Adaptive End-to-End Dialog Systems</title>
      <link href="/2018/11/12/Sentiment_Adaptive_End-to-End_Dialog_Systems/"/>
      <url>/2018/11/12/Sentiment_Adaptive_End-to-End_Dialog_Systems/</url>
      
        <content type="html"><![CDATA[<h3 id="abstract">Abstract</h3><p>Include user sentiment from <strong>multimodal information(acoustic, dialogic and textual)</strong> in the end-to-end learning framework to make systems <strong>more user-adaptive and effective</strong>.</p><p>First attempt to <strong>incorporate multimodal user information</strong> in the adaptive end-to- end dialog system training framework</p><ul><li>Multimodal Sentiment Classification</li></ul><h3 id="contribution">Contribution</h3><ul><li><a href="https://www.dropbox.com/s/nl5j4ts7kewx47v/sample_50.zip?dl=0" target="_blank" rel="noopener">an audio dataset</a> with sentiment annotation (dialog history)</li><li>an automatic sentiment detector that considers conversation history by using dialogic features, textual features and traditional acoustic features</li><li>end-to-end trainable dialog policies adaptive to user sentiment in both supervised and rein- forcement learning settings</li></ul><h3 id="dataset">Dataset</h3><ul><li><a href="https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/" target="_blank" rel="noopener">DSTC1</a></li><li>人工标注DSTC1的Audio dataset: 50 dialogs consisting of 517 conversation turns for user sentiment (<code>negative</code>, <code>neutral</code> and <code>positive</code>)</li></ul><h3 id="model-and-experiments">Model and Experiments</h3><ul><li>Multimodal Sentiment Classification<ul><li>openSMILE: 提取audio feature</li><li>four categories of dialogic features(人工统计feature)<ul><li>Interruption</li><li>Button usage</li><li>Repetitions</li><li>Start over</li></ul></li><li>Textual features: A <code>tf-idf vector</code> for each utterance as textual features</li></ul></li><li>Classification results<ul><li>Trained on 50 dialogs annotated with sentiment labels<br></li><li>Separated to be 60% for training, 20% for validation and 20% for testing</li><li>run 20 times and get avg</li></ul></li></ul><figure><img src="http://static.zybuluo.com/Preke/r5p3nprhvwb1bzochta2x3y7/image_1cs35h0hp19stdhm10tu1p7rs469.png" alt="image_1cs35h0hp19stdhm10tu1p7rs469.png-50kB"><figcaption>image_1cs35h0hp19stdhm10tu1p7rs469.png-50kB</figcaption></figure><ul><li>Supervised Learning (SL)<ul><li><strong>Detected user sentiment from the previous section</strong> into a supervised learning framework for training end-to-end dialog systems (<strong>classification problem</strong>)</li><li><strong>LSTM with 128 hidden-units and AdaDelta optimizer</strong>(HCN)前人提出的方法，用作baseline，这里的贡献就是加特征 <img src="http://static.zybuluo.com/Preke/18mdpf2tfcf254ulyjb43cla/image_1cs361o15v4u9ob1qo31m061vd8m.png" alt="image_1cs361o15v4u9ob1qo31m061vd8m.png-27.5kB"></li></ul></li></ul><p>Metric 解释：Turn-level F-1 score and dialog accuracy which indicates if all turns in a dialog are correct.</p><ul><li>Reinforcement Learning (RL) (实时从response的sentiment中获取reward, 目的是让response满意）<ul><li>Each turn receives a measurement of goodness called reward</li><li>Include user sentiment as immediate rewards to expedite the reinforcement learning training process</li><li>两个Metric:<ul><li>dialog length: 越小说明完成任务越快</li><li>task success rate: 任务具体如何定义，这里还暂时不太清楚…</li></ul></li></ul></li></ul><figure><img src="http://static.zybuluo.com/Preke/trqn1tlt87jxdztvg3tody61/image_1cs374srqplq71nhf51bh87mo13.png" alt="image_1cs374srqplq71nhf51bh87mo13.png-121.5kB"><figcaption>image_1cs374srqplq71nhf51bh87mo13.png-121.5kB</figcaption></figure><figure><img src="http://static.zybuluo.com/Preke/3hqtlct3wx9ritgzoa9fqggh/image_1cs375eqg1r27ovkci7802mlh1g.png" alt="image_1cs375eqg1r27ovkci7802mlh1g.png-30.7kB"><figcaption>image_1cs375eqg1r27ovkci7802mlh1g.png-30.7kB</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> Multimodal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Style Transfer Through Back-Translation</title>
      <link href="/2018/11/12/bst/"/>
      <url>/2018/11/12/bst/</url>
      
        <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>First learn a latent representation of input sentences by NMT model, then use adversarial generation to make the output with desired style.</p><p>Evaluate on three different style transformations:</p><ul><li>sentiment</li><li>gender</li><li>political slant</li></ul><p>Results in:</p><ul><li>automatic evaluation of style transfer</li><li>manual evaluation of meaning preservation and fluency</li></ul><h2 id="contribution">Contribution</h2><ul><li>A new approach to style transfer</li><li>A new task that we propose to evaluate style transfer: transferring politi- cal slant</li></ul><h2 id="methodology">Methodology</h2><p>Given two datasets: <span class="math inline">\(X_1=\{x_1^{(1)},...,x_1^{(n)}\}\)</span> <span class="math inline">\(X_2=\{x_2^{(1)},...,x_2^{(n)}\}\)</span> represent two different styles <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> (也就是说，所有的<span class="math inline">\(x_1^{(n)}\)</span>的style 都是<span class="math inline">\(s_1\)</span>, 所有的<span class="math inline">\(x_2^{(n)}\)</span>的style 都是<span class="math inline">\(s_2\)</span>)</p><p>Then, generate samples of dataset <span class="math inline">\(X_1\)</span> such that they belong to style <span class="math inline">\(s_2\)</span> and samples of <span class="math inline">\(X_2\)</span> such that they belong to style <span class="math inline">\(s_1\)</span></p><blockquote><p>state-of-the-art</p><ul><li>variational auto-encoders</li><li>cross-aligned autoencoders</li></ul></blockquote><p>aim is to design a latent code <span class="math inline">\(z\)</span>: 1. represents the meaning of the input sentence grounded in back-translation 2. weakens the style attributes of author’s traits</p><h2 id="dataset">Dataset</h2><ul><li>Reddy and Knight’s (2016) dataset of reviews from Yelp annotated for two genders cor- responding to markers of sex.</li><li>comprised of top-level comments on Facebook posts from all 412 current members of the United States Sen- ate and House who have public Facebook pages (Voigt et al., 2018)</li></ul><figure><img src="http://static.zybuluo.com/Preke/mozltnt19hb1zegjijdm3wqk/image_1cs60ditq1v151n5umbgimr95l9.png" alt="Dataset"><figcaption>Dataset</figcaption></figure><h2 id="model-and-experiments">Model and Experiments</h2><figure><img src="http://static.zybuluo.com/Preke/9wlzvypbynyf7buxj6f165m3/image_1cs39rk0g13dka7q7j812vls0g9.png" alt="Style transfer pipeline"><figcaption>Style transfer pipeline</figcaption></figure><figure><img src="http://static.zybuluo.com/Preke/p3fa7ra9emhb721o2sbo6tgt/image_1cs5vrov11tfu1b5p49bkfv1uiap.png" alt="style classifier feedback"><figcaption>style classifier feedback</figcaption></figure><ul><li>CNN去做二分类问题，人工label</li><li>A convolutional neural network (CNN) classifier to accurately predict the given style</li><li>concatenate binary style indicators to each input word embedding in the classifier</li></ul><figure><img src="http://static.zybuluo.com/Preke/4fqmqt59kcva12h34i69rpt5/image_1cs60jlg81stb1tdm1oda1v8ibhup.png" alt="result of classification"><figcaption>result of classification</figcaption></figure><figure><img src="http://static.zybuluo.com/Preke/oau4c5uc3c9aes5fxofkwv81/image_1cs60em6ts2c7sb1bkv12nu18sr16.png" alt="result of human preferance"><figcaption>result of human preferance</figcaption></figure><hr>]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> sentiment </tag>
            
            <tag> Style transfer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>奇怪的AEs</title>
      <link href="/2018/11/01/AEs/"/>
      <url>/2018/11/01/AEs/</url>
      
        <content type="html"><![CDATA[<h2 id="autoencoder">AutoEncoder</h2><figure><img src="http://static.zybuluo.com/Preke/f9r48wnmez3um3ci2sxb3tmh/image_1cqnhm30i9d218c3128h1vha7he9.png" alt="AutoEncoder"><figcaption>AutoEncoder</figcaption></figure><p>AutoEncoder直观理解是一个压缩(降维)技术，中间的representation保留了输入的信息，便于decoder去还原信息，训练的loss 则是输入和输出的 MSE(mean-square error)</p><h3 id="useful-links">Useful links:</h3><p>https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f</p><h2 id="variational-autoencoder">Variational Autoencoder</h2><p>Why VAE: 以下两短话是网上一些博客的解释： &gt; 但是，我们想建一个产生式模型，而不是一个只是储存图片的网络。现在我们还不能产生任何未知的东西，因为我们不能随意产生合理的潜在变量。因为合理的潜在变量都是编码器从原始图片中产生的。<br> 这里有个简单的解决办法。我们可以对编码器添加约束，就是强迫它产生服从单位高斯分布的潜在变量。正式这种约束，把VAE和标准自编码器给区分开来了。<br> 现在，产生新的图片也变得容易：我们只要从单位高斯分布中进行采样，然后把它传给解码器就可以了。</p><hr><blockquote><p>但是这样我们其实并不能任意生成图片，因为我们没有办法自己去构造隐藏向量，我们需要通过一张图片输入编码我们才知道得到的隐含向量是什么，这时我们就可以通过变分自动编码器来解决这个问题。<br> 其实原理特别简单，只需要在编码过程给它增加一些限制，迫使其生成的隐含向量能够粗略的遵循一个标准正态分布，这就是其与一般的自动编码器最大的不同。<br> 这样我们生成一张新图片就很简单了，我们只需要给它一个标准正态分布的随机隐含向量，这样通过解码器就能够生成我们想要的图片，而不需要给它一张原始图片先编码。</p></blockquote><p>如何理解中间的潜在向量服从高斯分布呢？又如何理解采样的过程呢？ 看到一个博客里的一张图可以比较简单的理解： <img src="http://static.zybuluo.com/Preke/i4633iw9mjxb27geln7cxg1u/image_1cr5aotjkb1didtqas1i1uoge9.png"></p><p>也就是每个输入样本输入后，会产生:</p><ul><li>一个均值的representation</li><li>一个方差的representation</li></ul><p>那么这里的均值是谁的均值，方差又是谁的方差呢？ 联想到和VAE有点像的GAN, 如果输入样本本身就是一个分布的话， 那么这个均值和方差就是输入样本的均值和方差 如果是这样的话，如何约束他们服从一个正态分布呢？ 我想这里就是encoder部分所做的约束</p><p><img src="http://static.zybuluo.com/Preke/nyzp95bo8lljrwhqnpgeujl5/image_1cr5e33e3188nsclsassl100r9.png"></p><h3 id="误差">误差</h3><p>对于AE来说，误差其实就是原始信息（图片)和生成信息(图片）之间的MSE （重构误差） 那么VAE在训练的时候，除了重构误差之外，其实还有每个样本的分布和标准正态分布之间的KL散度；</p><h3 id="理解">理解：</h3><p>经过看完这些材料，理解完上述的东西，用自己的话说：</p><p>我有一些样本，我假设每个样本都是服从一个正态分布的，可是我们不知道这个分布。 VAE试图去估计每个样本背后分布的均值和方差，并且进行采样，对每个样本得到一个隐层表示向量，然后通过这个向量去生成新的样本； 通过生成样本和原有样本的对比误差，以及中间估计的分布与标准正态分布的KL散度，去修正参数，使得：</p><p>VAE生成的样本是具有原有样本的“灵魂”而又不完全相同的样本 （好吧，这是一句没有灵魂的话）</p><h3 id="why-vae-again">Why VAE again：</h3><p>还是引用大佬们的话吧, 我觉得是我能读懂并且可以接受的： <img src="http://static.zybuluo.com/Preke/shxs61d5xpwo4iw4i3sebxdz/image_1cr5eqlh01kgu1qpbjll1ln715n8m.png" alt="image_1cr5eqlh01kgu1qpbjll1ln715n8m.png-177.8kB"></p><h3 id="useful-links-1">Useful links:</h3><p>墙裂推荐以下链接的文章，真的写的很好，（大部分我都是引用于此的）….</p><p>https://www.cnblogs.com/huangshiyu13/p/6209016.html<br> https://zhuanlan.zhihu.com/p/27549418<br> https://zhuanlan.zhihu.com/p/34998569</p><h2 id="cvae">CVAE</h2><p>这个部分称为Conditional VAE</p><p>无论是AE, 还是VAE，任务都是去拟合，去生成， 都是没有用到一些label信息去做分类的</p><h3 id="useful-links-2">Useful links:</h3><p>https://zhuanlan.zhihu.com/p/34998569</p><h2 id="ae-s2s">AE &amp; S2S</h2><h2 id="vae-for-nlp">VAE for NLP</h2>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AE </tag>
            
            <tag> VAE </tag>
            
            <tag> CVAE </tag>
            
            <tag> S2S </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOJITALK Generating Emotional Responses at Scale</title>
      <link href="/2018/10/22/Scale/"/>
      <url>/2018/10/22/Scale/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Paper notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dialog system </tag>
            
            <tag> emotion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>恰同学少年</title>
      <link href="/2018/10/21/%E6%81%B0%E5%90%8C%E5%AD%A6%E5%B0%91%E5%B9%B4/"/>
      <url>/2018/10/21/%E6%81%B0%E5%90%8C%E5%AD%A6%E5%B0%91%E5%B9%B4/</url>
      
        <content type="html"><![CDATA[<blockquote><p>独立寒秋，湘江北去，橘子洲头。 看万山红遍，层林尽染；漫江碧透，百舸争流。 鹰击长空，鱼翔浅底，万类霜天竞自由。 怅寥廓，问苍茫大地，谁主沉浮？ 携来百侣曾游。忆往昔峥嵘岁月稠。 恰同学少年，风华正茂；书生意气，挥斥方遒。 指点江山，激扬文字，粪土当年万户侯。 曾记否，到中流击水，浪遏飞舟？</p></blockquote><p>就突然想到小时候看的电视剧《恰同学少年》，当时爷爷还给我讲过那些蔡和森的故事；爷爷去世很多年了，这也是为数不多的我关于他的记忆了。</p><p>谁年轻的时候不是粪土当年万户侯？ 遇到社会矛盾，谁又不是针砭时弊，痛斥当权者？</p><p>以前我相信，这些肉食者思想固化，跟不上时代；等我们这一代人当权，必能改变世界，必能大有作为。 可是几千年来，哪个时代的热血青年不是如此想的呢？ 江山易主，旧历史总会重复 朝代更迭，新矛盾总会出现 各国各时代又有何区别？ 权力集中的地方必然有腐败，矛盾出现时一定要有人负责； 如何去和人性做抗争呢？ 毛泽东尚有 驱张行动，愤青们的不理智谁又能买单呢？</p><p>然而，愤青好就好在有一群人坚持自己的信念， 信念可能很不现实，但是一群人依然坚信 再困难，再失落，总有人鼓励 一起奋斗，是只有青春才能迸发的激情</p><p>恰同学少年，风华正茂；书生意气，挥斥方遒。 指点江山，激扬文字，粪土当年万户侯</p><p>青春而又豪迈</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>三尺微命，一介书生</title>
      <link href="/2018/10/20/%E4%B8%89%E5%B0%BA%E5%BE%AE%E5%91%BD,%E4%B8%80%E4%BB%8B%E4%B9%A6%E7%94%9F/"/>
      <url>/2018/10/20/%E4%B8%89%E5%B0%BA%E5%BE%AE%E5%91%BD,%E4%B8%80%E4%BB%8B%E4%B9%A6%E7%94%9F/</url>
      
        <content type="html"><![CDATA[<p>也是大四几个人在喝酒吹牛，</p><p>聊到各自的不得志，</p><p>醉醺醺的最先想到的就是《滕王阁序》</p><p>于是便有人慷慨激昂：穷且益坚，不坠青云之志</p><p>高二初读《滕王阁序》，印象十分深刻，</p><p>可能就是本身喜欢骈文，读起来气势恢宏，朗朗上口</p><p>然而其中意境，十不能明一二</p><p>但是读到这句话，真的分量很重，很能切身感受到郁郁不得志的心情</p><blockquote><p>勃，三尺微命，一介书生<br>无路请缨，等终军之弱冠<br>有怀投笔，慕宗悫之长风</p></blockquote><p>对照注释中的愿乘长风破万里浪，哪个意气风发的少年不识如此呢？</p><p>何况初唐四杰之首，天妒英才的王勃呢</p><p>就算是平庸的自己，读到此处也很容易就被代入到才华横溢，壮志难酬的作者当中去</p><p>眼神望去天边的晚霞，有年少时的才华横溢，有不尽的赏识和艳羡，有官场失足的困顿，也有受到连累困苦的父亲</p><p>从文章本身去想：前面引经据典，信手拈来，意境华丽，恢弘壮阔</p><p>转而 关山难越，谁悲失路之人；萍水相逢，尽是他乡之客</p><p>继而 孟尝高洁，空余报国之情；阮籍猖狂，岂效穷途之哭</p><p>竟是对比强烈，让人慨叹</p><p>我也不知道我为什么会喜欢上这句话，</p><p>或许是很能对照我们比较大众的现状：能力和才华，配不上自己的野心</p><p>亦或许是对自己的安慰，我不想给自己承担太多，仅仅是三尺微命，一介书生</p><p>三尺微命啊三尺微命</p><p>十人有九堪白眼的三尺微命</p><p>一介书生啊一介书生</p><p>饮冰十年难凉热血的一介书生</p><p>亦不是说每个人心中都有难酬的壮志</p><p>只是没到满足于老婆孩子热坑头的年龄</p><p>面对现实，总会觉得渺小，觉得心有余而力不足</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>《射雕》的一点印象</title>
      <link href="/2018/08/26/%E3%80%8A%E5%B0%84%E9%9B%95%E3%80%8B%E7%9A%84%E4%B8%80%E7%82%B9%E5%8D%B0%E8%B1%A1/"/>
      <url>/2018/08/26/%E3%80%8A%E5%B0%84%E9%9B%95%E3%80%8B%E7%9A%84%E4%B8%80%E7%82%B9%E5%8D%B0%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<p>这几天花了好久时间去看射雕，也算是Kindle没白费。</p><p>《射雕》给我的几个印象是：</p><ol type="1"><li>为了造成冲突，上来就打，混不解释；如桃花岛上江南五怪被杀，黄蓉看到种种线索，心中推测的疑点也不说出，任郭靖独自乘船离开；郭靖也是，啥都不清楚，就认准了黄老邪，对蓉儿也是瞬间色变；如黄老邪愣是心高气傲，不解释不开脱自己没杀江南五怪；又如轩辕台上，郭靖脱身的第一反应是先打个痛快，打了好久才让黄蓉道出真相；种种情结真是让人读的时候，皇上不急太监急。</li><li>人物不立体（不过这个金老先生也在后记中说到了，脸谱化，春秋笔法）。</li><li>郭靖开挂太明显：傻头傻脑，蒙古大汗儿子结拜，女儿许配，拜江南七怪，8岁杀陈玄风，得马钰传内功，得唯一的汗血宝马和双雕，喝蛇血百毒不侵，洪七公教降龙十八掌，周伯通教九阴真经和一心二用，还有黄蓉倾心（死心塌地，不离不弃的倾心，我觉得这个是最大的开挂！）；因为之前看过《天龙八部》，混觉得郭靖简直就是虚竹和萧峰的合体。</li><li>巧巧巧，也所谓无巧不成书；无论是大漠草原，太湖陆家庄，小小牛家村一间小屋，各路豪杰都能聚齐；</li><li>书中各种唱词，真是美如画，金庸融进来的道家文化和宋词文化十分丰富。</li><li>民族大义，倒是很像《天龙八部》；无论是郭靖，杨康，还是乔峰，是生我的祖先父辈的国家是祖国，还是养我的国家是祖国呢？郭靖是和黄蓉一起了解了岳爷爷的历史，了解了韩世忠，上官剑南等忠义之士的事迹，才会力保武穆遗书；听蓉儿讲了各种唱词，以及词中的故事，才会对大宋有了文化认同吧。这些杨康有吗？他成年之时也是大金国的小王爷，接触的全是大金的民族教育和文化认同，他没有太多的culture shock，也不易改变；不过让他坚定决心的就是祠堂中完颜洪烈对他说以后富贵不可限量。</li><li>铁木真，还真的就像《天龙八部》的耶律洪基。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>The Pilgrims</title>
      <link href="/2017/09/12/The%20Pilgrims/"/>
      <url>/2017/09/12/The%20Pilgrims/</url>
      
        <content type="html"><![CDATA[<p>We are the Pilgrims, Master</p><p>We shall go always a little further,</p><p>It may be beyond the last blue mountain barred with snow,</p><p>Across that angry or glimmering sea …</p><p>Go as a pilgrim and seek out danger</p><p>Far from comfort and the well lit-avenues of life.</p><p>Pit your very soul against the unknown</p><p>And seek stimulation in the company of the brave.</p><p>Experience cold and hunger, heat and thirst</p><p>And survive to see another challenge and another dawn.</p><p>Only then will you be at peace with yourself</p><p>And able to know and to say:</p><p>“I looked down on the farthest side of the mountain</p><p>And, fulfilled and understanding all, I am cold and hunger, heat and thirst</p><p>And survive to see another challenge and another dawn.</p><p>Only then will you be at peace with yourself</p><p>And able to know and to say:</p><p>“I looked down on the farthest side of the mountain</p><p>And, fulfilled and understanding all, I am truly content</p><p>That I lived a full life</p><p>And one that was of my own choice”.</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>马云的年会演讲</title>
      <link href="/2017/09/12/%E9%A9%AC%E4%BA%91%E7%9A%84%E5%B9%B4%E4%BC%9A%E6%BC%94%E8%AE%B2/"/>
      <url>/2017/09/12/%E9%A9%AC%E4%BA%91%E7%9A%84%E5%B9%B4%E4%BC%9A%E6%BC%94%E8%AE%B2/</url>
      
        <content type="html"><![CDATA[<p>今儿看了马云的年会演讲</p><p>感触很深就是，阿里是一个最强调社会责任感的公司了。也许跟他的愿景有关</p><p><strong>让天下没有难做的生意</strong></p><p>马云不是一个做技术的，所以决定了阿里不是一个纯技术公司</p><p>去各个大学演讲，与特朗普谈话承诺就业岗位，出席达沃斯，马云俨然是一个经济体的领头人。</p><p>演讲中提到了不作恶，当然这是Google的信条，我猛一想，去年Google 也18岁了，好像没见拉里佩奇演讲啊…</p><p>腾讯去年总办发红包…小马哥也没有强调什么，要做中国人沟通的桥梁类似的话…</p><p>所以我觉得，明年的百度？似乎能看到robin 吹一波人工智能对未来，对现在的影响，百度其实也承担了挺多的社会责任的</p><p>回归正题呢，我觉得马云提出的，服务农村，消除贫困，全球化等等未来的方向，这不是政府的工作吗？阿里可能是不那么独立的公司(没说不是好事)，为政府服务，为社会服务，反过来，政府也提供充分的资源，社会资源。是一个良好的合作关系。这个公司的发展和社会的进步相得益彰，也是一种独特的模式</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Apriori算法</title>
      <link href="/2017/09/07/Apriori%E7%AE%97%E6%B3%95/"/>
      <url>/2017/09/07/Apriori%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>从大规模数据集中寻找物品间的隐含关系被称作<strong>关联分析（association analysis）</strong>或者<strong>关联规则学习（association rule learning）</strong>。这里的主要问题在于，寻找物品的不同组合是一项十分耗时的任务，所需的计算代价很高，蛮力搜索方法并不能解决这个问题，所以需要用更智能的方法在合理的时间范围内找到频繁项集。</p><p>关联分析是在大规模数据集中寻找有趣关系的任务。这些关系可以有两种形式：</p><ul><li>频繁项集</li><li>关联规则</li></ul><p><strong>频繁项集（frequent item sets）</strong>是经常出现在一块儿的物品的集合， <strong>关联规则（association rules）</strong>暗示两种物品之间可能存在很强的关系。</p><p>借用一个《机器学习实战》里的例子： <img src="http://static.zybuluo.com/Preke/qimxp8bjnwzse97myfz04djc/image_1bp3v8eit353acj4l41pf7fmgm.png" alt="image_1bp3v8eit353acj4l41pf7fmgm.png-7.6kB"></p><p>比如说 <strong>{尿布，豆奶}</strong> 就是一个频繁项集，因为这个组合在例子中经常出现； 然后我们从这个频繁项集出发分析，是不是买尿布的人就经常也买豆奶呢？ 就得出了一个： <strong>尿布 <span class="math inline">\(\rightarrow\)</span> 豆奶</strong> 这样一个关联规则，或者 <strong>豆奶 <span class="math inline">\(\rightarrow\)</span> 尿布</strong>， 但是两者不等价，后面会说</p><p>再来两个概念 <strong>支持度（support)</strong> 和 <strong>可信度（confidence)</strong> 支持度：数据集中包含该项集的记录所占的比例 可信度：比如有两个集合 P,H; 则关联规则P <span class="math inline">\(\rightarrow\)</span> H 的可信度为： <span class="math inline">\(\frac{support(P \cup H)}{support(P)}\)</span></p><p>举几个简单的例子：</p><ul><li>{尿布，豆奶} 的 support 为 3/5，因为在 5 个交易中，{尿布，豆奶} 的组合出现了 3 次</li><li>那如果我们求 尿布 <span class="math inline">\(\rightarrow\)</span> 豆奶 的 confidence 就是 <span class="math inline">\(\frac{support(\{尿布，豆奶\})}{support(\{尿布\})}=\frac{3/5}{4/5}=\frac{3}{4}\)</span></li><li>那么豆奶 <span class="math inline">\(\rightarrow\)</span> 尿布 的 confidence 就是 <span class="math inline">\(\frac{support(\{尿布，豆奶\})}{support(\{豆奶\})}=\frac{3/5}{4/5}=\frac{3}{4}\)</span> ，…还是 3/4 ，很不幸的巧合，但是我们还是可以看出来，其实计算过程是不一样的</li></ul><h2 id="apriori原理">Apriori原理</h2><h3 id="频繁项集">频繁项集</h3><blockquote><p>支持度和可信度是用来量化关联分析是否成功的方法，假设想找到支持度大于 0.8 的所有项集，应该如何去做？一个办法是生成一个物品所有可能组合的清单，然后对每一种组合统计它出现的频繁程度，但是当物品数量成千上万时，上述做法非常慢</p></blockquote><p>我们可以这么想，如果一个项集是频繁的， 那么它的所有子集也都是频繁的； 接着：<strong>如果一个项集是非频繁的，那么它的所有超集也都是非频繁的。</strong></p><p>这条结论就在于，之前我们说，列出所有可能的组合很麻烦，有了这个结论，我们如果找到一些非频繁项集，那么他们的超集就可以全部不用考虑了，这样就简化了很大一部分计算；这就是 <strong>Apriori原理</strong></p><p>基于这样的思想，我们可以设计一个这样的算法，目标就是找到 support 大于等于一个阈值的所有频繁项集； 那么 Input: 最小support, 数据集 Output: 满足条件的所有频繁项集 主体思想就是：我们先算比较小的集合的support（最小就是单个元素的集合）, 把计算过程中不满足support的集合去掉，以留下来的集合和元素组合成新的集合，然后继续筛选，直到留下的项集都满足最小support，返回结果。</p><p><strong>所以我们可以看到核心部分就是，如何在一轮迭代之后生成新的数据集；因为每次迭代，我们的频繁项集都只加1，那么，我们生成新的候选的频繁项集的方法就是：用上一轮的项集中只相差1个元素的集合的并集作为新一轮候选的频繁项集。</strong></p><p>明确这一点，代码就不难写了； 用一个python代码来表示： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aprioriGen</span><span class="params">(Lk, k)</span>:</span></span><br><span class="line">    retList = []</span><br><span class="line">    lenLk = len(Lk)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(lenLk):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, lenLk):</span><br><span class="line">            L1 = list(Lk[i])[:k<span class="number">-2</span>]; L2 = list(Lk[j])[:k<span class="number">-2</span>]</span><br><span class="line">            L1.sort(); L2.sort()</span><br><span class="line">            <span class="keyword">if</span> L1 == L2:</span><br><span class="line">                retList.append(Lk[i] | Lk[j]) <span class="comment"># 前k-2项相同时，将两个集合合并</span></span><br><span class="line">    <span class="keyword">return</span> retList</span><br></pre></td></tr></table></figure></p><p>这里，是要生成集合中元素个数为 k 的候选频繁项集的，所以当前频繁项集元素个数为 k-1, 所以才有前k-2项相同时，将两个集合合并，来生成新的候选频繁项集；</p><h3 id="关联规则">关联规则</h3><p>那么其实找规则也是类似的，找到confidence 大于等于一个阈值的所有关联规则； 在此使用Apriori算法，说到这样一句话：<strong>如果某条规则并不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求</strong></p><blockquote><p>怎么理解一条规则的子集？ 假设项集为 <strong>{1，2，3，4}</strong>，假设 <strong>0，1，2 -&gt; 3</strong> 的 confidence 不满足最小可信度的要求，那么任意前件为<strong>{0，1，2}</strong>的子集的关联规则的 confidence 都不满足要求； 或者说 任何后件中包含 <strong>3</strong> 的关联规则的confidence都不满足要求。</p></blockquote><blockquote><p>这就产生了一个问题….一个项集产生的关联规则中一定要包含项集里所有的元素吗？比如 <strong>{0，1，2，3}</strong> 产生的规则有没有可能是 <strong>{0，2}-&gt;{3}</strong> 。或者是说我们从这个项集中没有什么证据可以说明那条规则? 没有得到一些明确的解释，所以我暂且坚持自己的思考，即认可这样一个观点： 基于一个项集<strong>Y</strong>的信息，将 <strong>Y</strong> 分成两个互补的子集 <strong>X, Y-X</strong> ; 那么只能计算 <strong>X <span class="math inline">\(\rightarrow\)</span> Y-X</strong> 或者 <strong>Y-X <span class="math inline">\(\rightarrow\)</span> X</strong> 的confidence</p></blockquote><p>基于上面的思想，我们可以这样生成关联规则，每次只让后件的元素个数加 <strong>1</strong>，可以有效快速的剪枝 我们可以看如下这幅图： <img src="http://static.zybuluo.com/Preke/58ercogjbi6x5j76d3oyid25/image_1bp6fn3vh1m1p1vth1hfn9n8nh79.png" alt="image_1bp6fn3vh1m1p1vth1hfn9n8nh79.png-101.5kB"> 假设图中深色的关联规则为不符合要求的，那么我们在第二轮发现了一个不符合要求的规则，那么我们就可以把它的所有子节点（子孙节点）全部标为深色不用再考虑，意思就是，我们就不用在下一轮考虑这个结点生成的子节点了，这样就省掉了好多计算量。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVM part2 核函数与SMO算法</title>
      <link href="/2017/09/03/svm_part2/"/>
      <url>/2017/09/03/svm_part2/</url>
      
        <content type="html"><![CDATA[<p>回顾我们之前的问题： 之前我们说到，假设我们了 <span class="math inline">\(\alpha\)</span>, 又有样本点数据，我们很容易由 <span class="math inline">\(w=\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}\)</span> 得出 <span class="math inline">\(w\)</span>, 同时也求得了 <span class="math inline">\(b\)</span></p><p>那么就得到了这个分类面 <span class="math inline">\(w^Tx+b\)</span> ,我们换一种表示方法： <span class="math display">\[w^Tx+b=\begin{pmatrix}\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}\end{pmatrix}^Tx+b\\=\sum_{i=1}^m\alpha_iy^{(i)}&lt;x^{(i)},x&gt;+b\]</span></p><p>这里我们可以看出，新的数据点只需与那些 <span class="math inline">\(\alpha_i =0\)</span> 的临界点做内积，便可以得到分类结果。</p><h2 id="核函数kernels">核函数（Kernels)</h2><p>先说一下直观理解吧（不一定正确，仅作直观理解），核函数就是把低维特征映射到高维，从而使得在低维情况下线性不可分的数据在高维情况下<strong>有可能</strong>能够找到那个分类面</p><p>特定情况下，如果我们需要，可以将一个一维特征加以变换成多维 比如说要拟合曲线的时候，我们可以用一个自变量的多次方程去拟合 比如在一个回归问题中，观察到 <span class="math inline">\(y\)</span> 可以用关于 <span class="math inline">\(x\)</span> 的 3次多项式来拟合 就可以用如下变换，将 <span class="math inline">\(x\)</span> 扩展到高维，使得 <span class="math inline">\(y\)</span> 成为一个关于 <span class="math inline">\(\phi(x)\)</span> 的函数： <span class="math display">\[\phi(x)= \begin{bmatrix} x\\x^2\\x^3 \end{bmatrix}\]</span></p><p>如果有原来变量的内积，如 <span class="math inline">\(&lt;x,z&gt;\)</span> , 那么映射之后为 <span class="math inline">\(&lt;\phi(x), \phi(z)&gt;\)</span> , 那么这个核函数的形式化表出就为：</p><p><span class="math display">\[K(x,z)=\phi(x)^T\phi(z)\]</span></p><blockquote><p>这里只是简单列举一个形式化表出的例子，没有直接关联原问题，但是这里可以理解，我们可以通过这个和函数，让SVM学习更高维度的特征</p><p><font color="red"> 这里还产成了一个问题：是不是每一个核函数都能表出为类似 <span class="math inline">\(\phi(x)^T\phi(z)\)</span> 的形式呢？</font></p><p>其实我初步的思考为，只有在原低维特征向量有做内积的运算时，我们这样构造和转化会方便</p><p>（好像有点废话，因为后面看到高斯核的时候在想是不是也能转化…不过是可以转化的，高斯核将低维特征转化成了无线维的特征）</p><p>Andrew 在课上讲了这样一句话：每当你在原问题中遇到 <span class="math inline">\(&lt;x,z&gt;\)</span> ，也就是内积的形式时，你都可以把它转化成 <span class="math inline">\(K(x,z)\)</span> ，当你在做这个转化的时候，你就将低维特征映射到了高维。</p><p>嗯，学到后面就清晰了，这个就是核函数有效性的问题</p></blockquote><p>这里说明一个核函数减少计算复杂度的例子：</p><p>假设，<span class="math inline">\(x,z\)</span> 都是 <span class="math inline">\(n\)</span> 维向量, 核函数为：<span class="math inline">\(K(x,z)=(x^Tz)^2\)</span> , 整理：</p><p><span class="math inline">\(K(x,z) = \sum_{i=1}^n\sum_{j=1}^n(x_i,x_j)(z_i,z_j)=\phi(x)^T\phi(z)\)</span></p><blockquote><p>不要疑惑，这里的 <span class="math inline">\(\phi(x)\)</span> 并不是上文的，而是如下：</p><p><span class="math display">\[if \quad n = 3:\]</span></p><p><span class="math display">\[\phi(x)=\begin{bmatrix}x_1x_1\\x_1x_2\\.\\.\\.\\x_3x_3\end{bmatrix}\]</span></p></blockquote><p>显然，<span class="math inline">\(\phi(x)\)</span> 有 <span class="math inline">\(n^2\)</span> 项，所以计算复杂度为 <span class="math inline">\(O(n^2)\)</span> ;</p><p>但是如果我们直接计算 <span class="math inline">\(K(x,z)=(x^Tz)^2\)</span> 这里的复杂度就变成了 <span class="math inline">\(O(n)\)</span></p><blockquote><p>这里的理解：虽然我们还是用映射之后的高维特征来让模型学习，但是由于这个核函数的存在，我们可以从低维特征（简单的计算）同样得到高维特征的结果，这就是它<strong>减少复杂度的原因</strong></p></blockquote><hr><p>这是一个简单的核函数，接下来就会介绍一些其他常用的核函数：</p><ul><li><p><span class="math inline">\(K(x,z)=(x^Tz+c)^2\\ \quad\quad\quad=\sum_{i,j=1}^n(x_ix_j)(z_iz_j)+\sum_{i=1}^n(\sqrt{2c}x_i)(\sqrt{2c}z_j)+c^2\)</span></p><p>这个核函数对应的 <span class="math inline">\(\phi(x)\)</span> 为：</p><p><span class="math display">\[if \quad n = 3:\]</span></p></li></ul><p><span class="math display">\[\quad\quad\quad \phi(x)=\begin{bmatrix}x_1x_1\\x_1x_2\\.\\.\\.\\x_3x_3\\\sqrt{2c}x_1\\\sqrt{2c}x_2\\\sqrt{2c}x_3\\c\end{bmatrix}\]</span></p><p>如果继续扩展，扩展到 <span class="math inline">\(K(x,z)=(x^Tz+c)^d\)</span> ，对应的 <span class="math inline">\(\phi(x)\)</span> 的维度为 <span class="math inline">\(C_{n+d}^d\)</span> ，$ n $ 是 <span class="math inline">\(x, z\)</span> 的维度</p><blockquote><p>为什么介绍这样一个一般化的向量内积形式的核函数呢？</p><p>这里有一个直观的理解（不一定严格正确）</p><p>可以看到，计算向量的内积，其实就是在衡量向量的相似度</p><p>所以基于这样一种形式，如果两个特征向量，关联性很大，这样形式的核函数也会很大</p><p>当然这里暂时忽略 <span class="math inline">\(\phi(x)\)</span> 对相似性的影响，所以说不一定严格正确</p></blockquote><ul><li><p><span class="math inline">\(K(x,z)=exp\begin{pmatrix}-\frac{||x-z||^2}{2\sigma^2}\end{pmatrix}\)</span></p><p>这个叫做，高斯核函数（因为形式很像高斯分布，也叫做径向基函数（Radial Basis Function)</p><p>能够把原始特征映射到无限维。</p><p>那我们现在来看一下 <span class="math inline">\(x,z\)</span> 的相似性：</p><ul><li>如果 <span class="math inline">\(x,z\)</span> 相似：<span class="math inline">\(||x-z|| \approx 0\)</span> ，<span class="math inline">\(K(x,z)\)</span> 趋向1</li><li>如果 <span class="math inline">\(x,z\)</span> 相差很多：<span class="math inline">\(||x-z|| \gg 0\)</span> ，<span class="math inline">\(K(x,z)\)</span> 趋向0</li></ul><p>这里理解的时候可以想象正态分布的钟形图</p><blockquote><ol type="1"><li><p>参数 <span class="math inline">\(\sigma\)</span> 是什么？</p><p>在这里是没有标准差的含义的，只是一个参数而已，我们可以用交叉验证调整出最好的参数</p></li></ol></blockquote><p>​</p></li></ul><h3 id="核函数有效性">核函数有效性</h3><p>这里就是上文中讨论的问题，是不是每一个核函数 <span class="math inline">\(K(x,z)\)</span> 都能表出为 <span class="math inline">\(\phi(x)^T\phi(z)\)</span> 的形式呢？</p><p>先给出判定定理吧：</p><blockquote><p><strong>Mercer 定理</strong></p><p>如果函数 <span class="math inline">\(K\)</span> 是 <span class="math inline">\(R^n*R^n\rightarrow R\)</span> 的映射，即两个<span class="math inline">\(n\)</span> 维向量映射到实数域，那么 <span class="math inline">\(K\)</span> 是一个有效核函数 等价于 对 <span class="math inline">\(\{x^{(1)},x^{(1)}...,x^{(m)}\}\)</span> ，其对应的核函数矩阵是对称半正定的。</p></blockquote><p>其中涉及到两个概念：核函数矩阵，对称半正定</p><p>核函数矩阵：</p><p>给定 <span class="math inline">\(m\)</span> 个训练样本（特征向量） <span class="math inline">\(\{x^{(1)},x^{(1)}...,x^{(m)}\}\)</span> ，对任意的 <span class="math inline">\(x^{(i)}, x^{(j)}\)</span> 带入 <span class="math inline">\(K\)</span> ，得到 <span class="math inline">\(K_{i,j} = K(x^{(i)}, x^{(j)})\)</span> ，得到一个 <span class="math inline">\(m*m\)</span> 的矩阵如下：</p><p><span class="math display">\[\begin{bmatrix}K_{1,1}&amp; K_{1,2}&amp; \cdots&amp;K_{1,m}\\K_{2,1}&amp;K_{2,2}&amp; \cdots&amp;K_{2,m}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\K_{m,1}&amp;k_{m,2}&amp;\cdots&amp;K_{m,m}\end{bmatrix}\]</span></p><p>就是核函数矩阵，也用 <span class="math inline">\(K\)</span> 来表示</p><p>对称半正定，这里先不展开讲，就是一个矩阵的性质，简单理解呢，就是：</p><p>如果是一个合法的核函数，即<span class="math inline">\(K(x,z)\)</span> 能表出为 <span class="math inline">\(\phi(x)^T\phi(z)\)</span></p><ol type="1"><li><p>首先如上矩阵肯定是对称的，因为 <span class="math inline">\(K(x,z) = K(z,x)\)</span></p></li><li><p>半正定，对于任意的 <span class="math inline">\(m\)</span> 维向量 <span class="math inline">\(z\)</span> ，<span class="math inline">\(z^TKz\ge0\)</span></p></li></ol><p>核函数就先到这吧…</p><h2 id="正则化不可分情况">正则化，不可分情况</h2><ul><li>如果数据线性可分，可以直接用线性分类器解决</li><li>如果数据线性不可分，用核函数映射到高维尝试找到分类面</li><li>如果用了核函数也没找到…那就允许一些错误样例吧</li></ul><figure><img src="http://static.zybuluo.com/Preke/m7yszo1luzfakmk6r84gfbhg/image_1bp0kkllk1b9g1fer1r1d1vcg1mq69.png" alt="image_1bp0kkllk1b9g1fer1r1d1vcg1mq69.png-17.5kB"><figcaption>image_1bp0kkllk1b9g1fer1r1d1vcg1mq69.png-17.5kB</figcaption></figure><p>上图中，一个离群点的出现就会使分类面偏移很多，这样分类面很容易过拟合；更有甚者，有些离群点在另一个类中导致分类面不可分，所以我们需要正则化，也就是说，允许一小部分错误分类点，但是对于这些错误分类点，我们在目标函数中加上一些惩罚项</p><p>所以设计一个新的模型： <span class="math display">\[min_{\gamma ,w,b}\quad\frac{1}{2}||w||^2+C\sum_{i=1}^m\xi_i \\\quad\quad\quad\quad\quad\quad\quad\quad s.t.  \quad y^{(i)}(w^Tx^{(i)}+b)\geq1-\xi_i ,\quad\quad i=1,...,m\\\quad\quad\xi_i\geq0,\quad\quad i=1,..,m.\]</span></p><p>引入一个松弛变量 <span class="math inline">\(\xi\)</span> 从限制条件来看，我们允许一部分点的函数间隔小于1甚至有可能是负数，对于这些点，对应的 <span class="math inline">\(\xi_i\)</span> 会很大(可以通过限制条件计算的）,那么在目标函数中表现的就很明显，这样求目标函数的最小值的约束也就会体现出来。同时，<span class="math inline">\(C\)</span> 是离群点的权重，也是一个可以调的参数</p><p>这种情况也称为 L1软间隔，因为正则化项是 L1-norm;</p><p>再按照之前的方法求解： 拉格朗日公式就变成了：</p><p><span class="math display">\[\mathcal{L}(w,b,\xi,\alpha,r)=\frac{1}{2}w^Tw+C\sum_{i=1}^m\xi_i-\sum_{i=1}^m\alpha_i[y^{(i)}(w^Tx^{(i)}+b) - 1+\xi_i]-\sum_{i=1}^mr_i\xi_i.\]</span> 有两个不等式约束,所以 <span class="math inline">\(\alpha_i,r_i\)</span>都为拉格朗日乘子 对偶问题为：</p><p><span class="math display">\[\max_\alpha\quad W(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j&lt;x^{(i)},x^{(j)}&gt;\\s.t.\quad 0\leq\alpha_i\leq C,\quad i=1,...,m\\\sum_{i=1}^m\alpha_iy^{(i)}=0\]</span> 与之前对偶问题的区别只有第一个限制条件变成了 <span class="math inline">\(\quad 0\leq\alpha_i\leq C\)</span>, 同时， <span class="math inline">\(b\)</span> 的求解也发生了变化，在后面的章节中会解释；</p><p><img src="http://static.zybuluo.com/Preke/s75438242r63zi5wqjlqm4sz/image_1bp353tpi1res1bsaqlrt8i17c69.png" alt="image_1bp353tpi1res1bsaqlrt8i17c69.png-22.2kB"> 讲义中给出如上三个公式，并且吴恩达也没有详细讲，只说是用KKT条件推导来的， 我就自己翻了资料理解了一下，参考周志华的《机器学习》和李航的《统计学习方法》</p><p>如果要得到上述对偶问题，需要满足KKT条件：</p><p>分别对三个变量 <span class="math inline">\(w,b,\xi\)</span> 求偏导为 0, 就有： <span class="math display">\[w=\sum_{i=1}^m\alpha_iy_ix_i\\\sum_{i=1}^m\alpha_iy_i=0\\\alpha_i+r_i=C\]</span> 对拉格朗日公式用一下K.K.T.条件，得到： <span class="math display">\[\left\{\begin{aligned}\alpha_i\geq0 \\r_i\geq0\\y^{(i)}(w^Tx^{(i)}+b)-1+\xi_i\geq0\\\alpha_i(y^{(i)}(w^Tx^{(i)}+b)-1+\xi_i)=0\\\xi_i\geq0\\r_i\xi_i=0\end{aligned}\right.\]</span></p><p>在这里：</p><ul><li><p><span class="math inline">\(\alpha_i&gt;0\)</span> 的那些点，和以前一样，还是支持向量，不过这里呢，是这个软间隔分类器的支持向量(包括在最大间隔边界，和一些函数间隔小于1的点）;</p></li><li><p>如果 <span class="math inline">\(\alpha_i&lt;C\)</span>，那么 <span class="math inline">\(r_i&gt;0\)</span> , 接着就有 <span class="math inline">\(\xi_i=0\)</span>, 这些点恰好就是在最大分类间隔边界上的支持向量;</p></li><li>如果 <span class="math inline">\(\alpha_i=C\)</span> , 则有 <span class="math inline">\(r_i = 0\)</span>, 此时：<ul><li>如果 <span class="math inline">\(\xi_i &lt; 1\)</span>，则样本落在最大间隔内部</li><li>如果 <span class="math inline">\(\xi_i&gt;1\)</span> , 则样本就被错误分类</li><li>如果 <span class="math inline">\(\xi_i = 1\)</span>，样本就落在超平面上</li></ul></li></ul><blockquote><p>这里，李航的《统计学习方法》里面提到我们可以用 <span class="math inline">\(\frac{\xi_i}{||w||}\)</span> 来表示样本到最大分类间隔边界的距离，也是很巧妙。</p></blockquote><hr><blockquote><p>对于这种SVM，假设我们求到了分类超平面（其实就是那些支持向量）在我们分类的时候，还是通过本文开头的那个判别方法来分类</p></blockquote><h2 id="坐标上升法">坐标上升法</h2><p>其实是为了引出SMO的一个过渡方法</p><p>其实原理很简单，假设要求： <span class="math display">\[\max_\alpha W(\alpha_1,\alpha_2,...,\alpha_m)\]</span> 我们每次控制其他变量，只在一个变量的维度求最大值，得到之后，再在下一个变量的维度来求，循环直到最优解。伪码表示如下： <img src="http://static.zybuluo.com/Preke/ala1chzobgineiquo5m18lr8/image_1bp35g42uskc18271bji14s17eqm.png" alt="image_1bp35g42uskc18271bji14s17eqm.png-20.6kB"></p><h2 id="smo算法">SMO算法</h2><p>被称为最快的二次规划优化算法, 有点6 特别在用于线性SVM和数据稀疏时性能更优 原始paper: 《 Sequential Minimal Optimization A Fast Algorithm for Training Support Vector Machines》</p><p>之前说最后要解决这个对偶问题： <span class="math display">\[\max_\alpha\quad W(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j&lt;x^{(i)},x^{(j)}&gt;\\s.t.\quad 0\leq\alpha_i\leq C,\quad i=1,...,m\\\sum_{i=1}^m\alpha_iy^{(i)}=0\]</span> 这里 <span class="math inline">\(x^{(i)}, y^{(i)}, C\)</span>都是已知的，所以我们的问题就是在参数 <span class="math inline">\(\{\alpha_1,\alpha_2,...,\alpha_m\}\)</span> 上求 <span class="math inline">\(W\)</span> 的最大值 按照坐标上升的思路，我们是要固定其他维度，先选取一个参数做优化的，但是，由于我们有第二个等式约束，是无法只变动一个参数而其他参数都不改变的，所以我们至少也要选两个参数优化才可以。 算法思路如下： <img src="http://static.zybuluo.com/Preke/qsk30gn1eisq3wppvn89cw3g/image_1bp39pgg3bnf17jv9v81tp1sjo9.png" alt="image_1bp39pgg3bnf17jv9v81tp1sjo9.png-60kB"> 循环内： 1. 用启发式算法去选取一对 <span class="math inline">\(\alpha_i, \alpha_j\)</span>， 控制其他 <span class="math inline">\(\alpha\)</span> 不变 2. 通过 <span class="math inline">\(\alpha_i, \alpha_j\)</span> 优化得到的 <span class="math inline">\(W(\alpha)\)</span> 更新原来的值</p><p>这里我们有这个约束： <span class="math inline">\(\sum_{i=1}^m\alpha_iy^{(i)}=0\)</span>, 假设我们选取 <span class="math inline">\(\alpha_1, \alpha_2\)</span>, 我们就能够得到： <span class="math display">\[\alpha_1y^{(1)}+\alpha_2y^{(2)} = -\sum_{i=3}^m\alpha_iy^{(i)} = \zeta\]</span> 其中 <span class="math inline">\(\zeta\)</span> 是一个固定值，因为我们固定后面的参数不变 我们可以显式的把这个约束条件表示成一条直线，如下： <img src="http://static.zybuluo.com/Preke/lp0zyfluulhsa4b11z339g14/image_1bp3cpicr1gsikhahi81ucajofm.png" alt="image_1bp3cpicr1gsikhahi81ucajofm.png-16.3kB"></p><p>横轴为 <span class="math inline">\(\alpha_1\)</span>，纵轴为 <span class="math inline">\(\alpha_2\)</span>， <span class="math inline">\(0\leq\alpha_i\leq C\)</span> 对应了 <span class="math inline">\(\alpha\)</span> 对应在这个方框内 其中的 L，H 就是 <span class="math inline">\(\alpha_2\)</span> 的 lowerbond 和 upperbond</p><p>继续我们可以得到： <span class="math inline">\(\alpha_1 = (\zeta-\alpha_2y^{(2)})y^{(1)}\)</span> (这里因为 <span class="math inline">\(y^{(i)}\in\{1,-1\}\)</span> )是类别标签，所以乘和除是一样的，那么目标问题就表示成： <span class="math display">\[W(\alpha_1,\alpha_2,...,\alpha_m)=W((\zeta-\alpha_2y^{(2)})y^{(1)}, \alpha_2,...,\alpha_m).\]</span> 这就变成了对一个变量 <span class="math inline">\(\alpha_2\)</span> 的优化问题，带入 <span class="math inline">\(W(\alpha)\)</span> 的公式，可以将 <span class="math inline">\(W(\alpha)\)</span> 表示成类似 $a_2^2+b_2+c $ 的形式, 其中 <span class="math inline">\(L\leq \alpha_2 \leq H\)</span></p><p>我们用 <span class="math inline">\(\alpha_2^{new,unclipped}\)</span> 来表示上述二次规划得到的最优值对应的 <span class="math inline">\(\alpha_2\)</span>, 我们容易得到 <span class="math inline">\(\alpha_2\)</span> 的更新规则： <img src="http://static.zybuluo.com/Preke/lqms4kjkf47fo5pzevrodbjy/image_1bp3gbbne3r1gb813nb1f7naf1p.png" alt="image_1bp3gbbne3r1gb813nb1f7naf1p.png-22.5kB"></p><p>得到之后再求出 <span class="math inline">\(\alpha_1^{new}\)</span> 就好了。</p><hr><h2 id="后记">后记</h2><p>至此，我们还有几个问题：</p><ul><li>上面提到说，软间隔中 <span class="math inline">\(b\)</span> 的更新规则变了，如何变？</li><li>SMO中说到的启发式规则寻找 <span class="math inline">\(\alpha_i, \alpha_j\)</span>，（感觉应该能简化一些计算量） 暂时只能先去参考JerryLead大神的笔记了…</li></ul><p>说实话，写到这里很累… 一度怀疑为什么网上有这么详尽的笔记，自己还要再写一遍… 才感觉到这才只是一个开始，SVM的世界好大，完全弄懂一个算法真的不容易… 以 JerryLead 大神的一个深入浅出的总结来暂时结束吧…毕竟以后还有代码实现不是~ <img src="http://static.zybuluo.com/Preke/lpquhwg7j2nzs8oke0816qfw/image_1bp3h9i15h82ovh1qka17m91dqh16.png" alt="image_1bp3h9i15h82ovh1qka17m91dqh16.png-354.5kB"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVM part1 线性SVM</title>
      <link href="/2017/08/26/svm_part1/"/>
      <url>/2017/08/26/svm_part1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>先说我对线性SVM的整体理解吧： 其实就是一个最优间隔的二分类器（如下图） <img src="http://static.zybuluo.com/Preke/hivu0uxdn4u9d34hr9pk1bdl/image_1bntvf1v43lt7391rpb1l5n1u6r2h.png" alt="image_1bntvf1v43lt7391rpb1l5n1u6r2h.png-25.1kB"></p><p>目标就是找到中间那个最优的分类超平面，而如图在虚线上的点，就是所谓的支持向量（support vectors) 。在求解这一问题的时候用到了对偶问题来帮助解决（为什么要用对偶问题？）而想要这样的话，我们定义了一个满足KKT条件的原问题，这里很巧妙的一点在于，<strong>KKT 的总体思想是认为极值会在可行域边界上取得</strong>，我觉得这一点是线性SVM的核心思想。</p><p>在总结下来这些的时候，我主要是根据</p><ul><li>Andrew Ng的机器学习公开课（Stanford cs229 2003)</li><li>当年课程的学习讲义</li><li>JerryLead 的中文笔记</li><li>Matrix cookbook</li></ul><p>其实很多问题，很多人都阐述的非常清楚透彻，自己写下来只是理一下自己的思路，确认是否真正的理解； 我在看讲义和听课的过程中，经常暂停下来提一些奇怪的问题，有些能够解决，有些却不能，我也会在下面写出，供大家思考，这部分我用引用格式表示，如：</p><blockquote><h2 id="为什么最优间隔">为什么最优间隔？</h2><p>最优间隔，就是这个分类器（超平面）要尽可能的离两边的样本数据都很远才好； 我们知道，分类器是根据已有样本数据来训练得到，以便预测测试数据的，在这一点上，我们考虑到，最优间隔的分类器，在未来的测试数据上，更保险，更不容易出错。</p></blockquote><h2 id="问题的数学表示">问题的数学表示</h2><p>我们有很多样本点： <span class="math display">\[(x^{(i)}, y^{(i)}), i = 1...m\]</span> 其中 <span class="math inline">\(x^{(i)} = [x_{1}^{(i)},x_{2}^{(i)},...,x_{n}^{(i)}]\)</span> 是一个向量， <span class="math inline">\(y^{(i)} \in \left\{ 1, -1 \right\}\)</span> 是类别标签；</p><p>既然还是一个线性的分类器，那么我们可以将这个分类超平面定义为：</p><p><span class="math display">\[ h_{w,b} = g(w^Tx+b)\\\\g(z)=\left\{ \begin{aligned} 1, &amp;z&gt;=0\\-1, &amp;z&lt;0\\\end{aligned}\right.\]</span></p><p>其中 <span class="math inline">\(w, b\)</span> 就是我们需要学习到的分类器的参数</p><blockquote><p>这里我想说一下这个 <span class="math inline">\(g(z)\)</span> ，我们知道： 在LR里面，这个 <span class="math inline">\(g(z)\)</span> 就是 sigmoid 函数 在softmax里面，这个 <span class="math inline">\(g(z)\)</span> 就是softmax函数 那么在这里，写到这的时候还没有一个 <span class="math inline">\(g(z)\)</span> 的显式表出； 我们考虑一个特殊情况，因为我上面说到,KKT条件的整体思想，引发了我的思考，也就是SVM的核心思想就是边界值，那么，我们索性假设有一个边界值点 <span class="math inline">\((x^{(j)}, y^{(j)}) , y^{(j)} = 0\)</span> ：那也就是说 <span class="math inline">\(g(z) = 0\)</span> 当然，这个边界值点在被分类数据中不会存在，但是我们先继续， <span class="math inline">\(g(z) = 0\)</span> 的唯一取到的可能就是 <span class="math inline">\(z = 0\)</span>, 那此时，也就是说 <span class="math inline">\(w^Tx+b = 0\)</span>; 既然对于一个合法的分类器来说，这个点不会存在于被分类数据中，那么在这个空间内，它唯一存在的地方就是在这个分类超平面上，也就是说，这个超平面的性质就是： <span class="math display">\[w^Tx+b = 0\]</span></p></blockquote><h2 id="函数间隔functional-margin-几何间隔geometric-margin">函数间隔（functional margin) &amp; 几何间隔（geometric margin)</h2><h3 id="函数间隔">函数间隔</h3><p>对于一个样本点 <span class="math inline">\((x^{(i)}, y^{(i)})\)</span>，函数间隔为：<span class="math inline">\(\hat \gamma^{(i)} = y^{(i)}(w^Tx^{(i)}+b)\)</span> 考虑到 $y $ 的正负性，则有： <span class="math inline">\(\hat \gamma^{(i)} = |w^Tx^{(i)}+b|\)</span> 下面这点我觉得讲的很好： 如果<span class="math inline">\(y_{(i)} &gt; 0\)</span> , <span class="math inline">\(w^Tx^{(i)}+b\)</span>应该是一个大正数，反之是个大负数 <strong>因此函数间隔代表了我们认为特征是正例还是反例的确信度</strong></p><blockquote><p>这里，函数间隔还没有什么具体的实际数值意义，因为随着参数 <span class="math inline">\(w,b\)</span> 的改变，函数间隔可以随意变化</p></blockquote><h3 id="几何间隔">几何间隔</h3><p>简单来说，几何间隔就是归一化的函数间隔 归一化之后，几何间隔就是某个样本点到分类平面的距离 <img src="http://static.zybuluo.com/Preke/3xkz3sjhntpf9erb5621afg7/image_1bnu4e0u11fqut6917rh1jos62j3o.png" alt="image_1bnu4e0u11fqut6917rh1jos62j3o.png-23.4kB"></p><p>上图中， B 点位于 <span class="math display">\[w^Tx+b = 0\]</span> 这个分割面上， 设 A 点到 B 点的距离用 <span class="math inline">\(\gamma^{(i)}\)</span> 表示，即 A 的几何间隔， 那么， <span class="math inline">\(w\)</span> 就是 BA 方向上的梯度， 单位向量为 $ $ ;</p><blockquote><p>这里我又瞎想了一下， <span class="math inline">\(w\)</span> 为啥是梯度呢？ 首先这个问题是一个线性的问题，也就是 <span class="math inline">\(w^Tx+b = 0\)</span> 是一个 <span class="math inline">\(x\)</span> 的线性组合，那么我们先从二维着手， 假设 <span class="math inline">\(x\)</span> 就是一个一维变量，那么 <span class="math inline">\(w^Tx+b = 0\)</span> 就是一条分类直线， <strong><span class="math inline">\(w\)</span> 就是斜率</strong> 那么二维空间的梯度，不就是斜率吗？ 换句话讲，梯度就是函数的走势方向，因为想想梯度下降，不就是选择函数下降最快的方向作为梯度吗？ 或者你也可以这么理解，梯度是怎样得到的？求导得到的 你在二维空间，将 <span class="math inline">\(wx+b\)</span> 对 <span class="math inline">\(x\)</span> 求导就是 <span class="math inline">\(w\)</span> 或者直接在高维将 <span class="math inline">\(w^Tx+b = 0\)</span> 对 <span class="math inline">\(x\)</span> 求导， 得到的就是 <span class="math inline">\(w\)</span> 这里可以参考matrix cookbook, 有一系列的求导规则，这里用到了： <img src="http://static.zybuluo.com/Preke/18uzcewxcnack2et745ntqml/image_1bnu5ddne111f1jcq1649ebe1qpq45.png" alt="image_1bnu5ddne111f1jcq1649ebe1qpq45.png-5.9kB"></p></blockquote><p>回归正题：所以如果 A 是 <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> ， B 就是 <span class="math inline">\((x^{(i)} - \hat \gamma^{(i)}\frac {w}{||w||}, 0)\)</span>。 这里我一度将这个运算和上面的示意图混淆，不知道为什么这样计算 B， 但是，当我意识到， <span class="math inline">\(y\)</span> 并不是示意图的一个维度，知识一个类别标签，把这个运算单纯作为向量运算，就很明了了。 把 B 代入 <span class="math inline">\(w^Tx+b = 0\)</span>， 运算得到： <span class="math display">\[ \gamma^{(i)} = y^{(i)} \big( (\frac {w}{||w||})^Tx^{(i)}+\frac{b}{||w||} \big) \]</span></p><p>当 <span class="math inline">\(||w|| = 1\)</span> 时， 几何间隔就是函数间隔了，所以说几何间隔就是归一化的函数间隔；</p><h2 id="最优间隔分类器">最优间隔分类器</h2><p>我们的目标是寻找一个超平面，使得离超平面比较近的点能有更大的间距。也就是我们不考虑所有的点都必须远离超平面，我们关心求得的超平面能够让所有点中离它最近的点具有最大间距。将问题形式化表示： <span class="math display">\[max_{\gamma ,w,b}\quad\gamma \\\quad\quad\quad\quad\quad\quad\quad\quad s.t.  \quad y^{(i)}(w^Tx^{(i)}+b)\quad\geq\quad\gamma \\\quad\quad\quad\quad||w|| = 1\]</span> 这里约束了 <span class="math inline">\(||w|| = 1\)</span>, <span class="math inline">\(w^Tx+b\)</span> 就是几何间隔，也就是点到分类平面的距离 ，至此我们定义出的问题，借助这个优化目标，如果求出了 <span class="math inline">\(w,b\)</span> 就可以借助之前定义的 <span class="math inline">\(h(x), g(z)\)</span> 来判别 <span class="math inline">\(y\)</span> 的类别了。</p><h3 id="其实我觉得这部分是精髓转化的思想也可以应用到其他地方">其实我觉得这部分是精髓，转化的思想也可以应用到其他地方</h3><p>我们虽然有了这个 <span class="math inline">\(||w|| = 1\)</span> 的约束，但是用这个约束，却有点手足无措，因为这不是一个凸函数。所以开始了<strong>奇妙的转化：</strong></p><p>第一步：</p><p>考虑到几何间隔和函数间隔的关系 <span class="math inline">\(\gamma = \frac{\hat \gamma}{||w||}\)</span>, 先将问题转化成： <span class="math display">\[max_{\gamma ,w,b}\quad\frac{\hat \gamma}{||w||} \\\quad\quad\quad\quad\quad\quad\quad\quad s.t.  \quad y^{(i)}(w^Tx^{(i)}+b)\quad\geq\quad\hat\gamma \\\]</span> 这里，把 <span class="math inline">\(||w|| = 1\)</span> 这个约束条件转化到优化目标问题里，我们就不用单独去考虑这个非凸优化的条件，但是优化目标还不是凸函数，那就继续</p><p>第二步：</p><p><img src="http://static.zybuluo.com/Preke/hivu0uxdn4u9d34hr9pk1bdl/image_1bntvf1v43lt7391rpb1l5n1u6r2h.png" alt="image_1bntvf1v43lt7391rpb1l5n1u6r2h.png-25.1kB"> 之前简略提过，分类面的确立只与那些离分类面最近的那些点有关； 我们在上图中，考虑这些离分类面最近的那些点，把他们的函数间隔设为1， <span class="math inline">\(\hat \gamma = 1\)</span> ;（这里只是对 <span class="math inline">\(w,b\)</span> 进行了缩放，因为本身函数间隔的大小就是可以改变的） 那么他们的几何间隔 <span class="math inline">\(\gamma = \frac{\hat \gamma}{||w||} = \frac{1}{||w||}\)</span> 也即他们到分类面的距离。 那么问题就转化成： <span class="math display">\[max_{\gamma ,w,b}\quad\frac{1}{||w||} \\\quad\quad\quad\quad\quad\quad\quad\quad s.t.  \quad y^{(i)}(w^Tx^{(i)}+b)\quad\geq\quad 1 \\\]</span> 由于 <span class="math inline">\(||w||\)</span> 是二范数，本质还是一个数值，我们求 <span class="math inline">\(\frac{1}{||w||}\)</span> 的最大值，也就是求 <span class="math inline">\(\frac{1}{2}||w||^2\)</span>的最小值 所以问题转化为： <span class="math display">\[min_{\gamma ,w,b}\quad\frac{1}{2}||w||^2 \\\quad\quad\quad\quad\quad\quad\quad\quad s.t.  \quad y^{(i)}(w^Tx^{(i)}+b)\quad\geq\quad 1 \\\]</span></p><p>这就成了一个带有不等式约束的二次规划问题，带条件的优化问题，我们可以用拉格朗日乘子法来解决；</p><h2 id="拉格朗日乘数法-对偶问题-kkt-条件">拉格朗日乘数法， 对偶问题， KKT 条件</h2><p>先看一下一般形式的拉格朗日乘数法： 假设有问题： <span class="math display">\[min_w \quad f(w)\\\quad\quad\quad\quad\quad\quad\quad s.t. \quad g_i(w) \leq0 \quad i = 1,...,k\\\quad\quad\quad\quad\quad\quad\quad\quad\quad h_i(w)=0 \quad i = 1,...,l\]</span></p><p>为了解决有两个约束（一个等式约束 <span class="math inline">\(h_i(w)=0\)</span>, 一个不等式约束 $ g_i(w) 0$ )的优化问题，引入拉格朗日乘数法，先定义拉格朗日公式： <span class="math display">\[\mathcal{L}(w,\alpha, \beta) = f(w) + \sum_{i=1}^k\alpha_ig_i(w)+\sum_{i=1}^l\beta_ih_i(w)\]</span> 其中 <span class="math inline">\(\alpha_i, \beta_i\)</span> 是拉格朗日算子，然后联立如下方程组，就可以解出<strong>可能的</strong>极值点： <span class="math display">\[\frac {\partial\mathcal{L}(w,\alpha, \beta)}{\partial w} = 0\\\frac {\partial\mathcal{L}(w,\alpha, \beta)}{\partial \alpha_i} = 0\\\frac {\partial\mathcal{L}(w,\alpha, \beta)}{\partial \beta_i} = 0\]</span></p><blockquote><p>(这部分我自己理解的时候也有点乱，条理可能不太清晰，如果看一次没看懂，再看一遍可能会有帮助） 至于为什么能够这样的条件约束下的优化问题能够用拉格朗日乘数法求解， 首先，我看到了这样一篇博客 <a href="http://www.cnblogs.com/maybe2030/p/4946256.html" target="_blank" rel="noopener">拉格朗日乘数法</a>，第二部分中数学实例的例子不错，提出了拉格朗日乘数法的思想： <strong>通过引入拉格朗日乘子将原来的约束优化问题转化为无约束的方程组问题</strong> 其次这个 <a href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0" target="_blank" rel="noopener">维基百科</a> 的图，觉得对自己理解很有帮助 <img src="http://static.zybuluo.com/Preke/fw2eels4sf4u6j4eu40gxbvj/image_1bnvinmmk1o99r2hd9e12sh1b7f4i.png" alt="image_1bnvinmmk1o99r2hd9e12sh1b7f4i.png-40.6kB"> 简单来说，虽然还是寻找极值点，但是约束条件的存在使得这个寻找的轨迹只能由上图绿线所决定；绿线就是问题的可行域。那么我们沿着绿线的方向走，向着 <span class="math inline">\(f(x,y)\)</span> 下降（或上升，取决于问题求极大还是极小）的方向走，走到 <span class="math inline">\(g(x,y)\)</span> 的变化率为0的时候，我们就找到了极值点, 这里的图中就表现为与 <span class="math inline">\(f(x,y)\)</span> 的等高线相切。 此时：从图上理解，曲面相切，法向量共线，曲面的法向量就是偏导（偏导是各个分量的变化率，法向量是各个方向变化率叠加的结果，如果不太明白，推荐一篇博客：<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/09/1978280.html" target="_blank" rel="noopener">小谈导数、梯度和极值</a> 或者也可以从一元函数来理解，两个曲线相切，意味着此刻在切点，斜率相同，曲线法向量共线； Anyway, 我们定义一个只有一个不等约束的拉格朗日问题方便理解，这里再从解析的角度去理解： <span class="math display">\[min_w \quad f(w)\\s.t. \quad g(w) \leq0 \]</span> 先看求解过程： <span class="math display">\[\mathcal{L}(w,\alpha) = f(w) + \alpha g(w)\]</span> 联立方程组： <span class="math display">\[\frac {\partial\mathcal{L}(w,\alpha)}{\partial w} = 0\quad\quad\quad\quad\quad(1) \\\frac {\partial\mathcal{L}(w,\alpha)}{\partial \alpha} = 0\quad\quad\quad\quad\quad(2) \]</span> 我们先看 <span class="math inline">\((1)\)</span> , 进一步： <span class="math inline">\(\frac {\partial\mathcal{L}(w,\alpha)}{\partial w} = \frac {\partial f(w)}{\partial w}+\alpha \frac {\partial g(w)}{\partial w} = 0\)</span> 回想我们刚刚说的法向量共线，是不是这个公式就是对于法向量共线的解释呢？ 我们继续看 <span class="math inline">\((2)\)</span>：<span class="math inline">\(\frac {\partial\mathcal{L}(w,\alpha)}{\partial \alpha} =g(w)= 0\)</span>，这说明了什么？ <span class="math inline">\(\mathcal{L}(w,\alpha) = f(w) + \alpha g(w)\)</span>， 现在 <span class="math inline">\(g(w) = 0\)</span> ,那说明在极值点： <span class="math inline">\(\mathcal{L}(w,\alpha) = f(w)\)</span> 现在再回来看 <span class="math inline">\((1)\)</span>， 相当于是在求 <span class="math inline">\(\frac {\partial\mathcal{L}(w,\alpha)}{\partial \alpha}\)</span>的极值点，是不是就是原问题 <span class="math inline">\(f(w)\)</span> 的极值点了呢？ 这样这种解法就能解释得通了吧。 不过有点不严谨的地方是，记得我们在学的时候，是有一条：就是原问题 <span class="math inline">\(f(w)\)</span> 的极值是 $ {(w,)}$ 的极值的子集，之后还需继续验证 扩展到高维不难，如果有多个约束条件无非就是多个条件的线性组合，相信在理解的过程中是一样的。</p></blockquote><p>回到正题，但是我们不能直接求解，有 <span class="math inline">\(g_i(w)\leq0\)</span>，求极小值可以发散到负无穷，这样我们先定义一个这样的函数： <span class="math display">\[\theta_\mathcal{P}(w)=\max_{\alpha,\beta:\alpha_i\geq0} \mathcal{L}(w,\alpha, \beta) \]</span> 作为我们的原问题。由于有 <span class="math inline">\(g_i(w)\leq0\)</span>, 为了求极大值，我们这里约束 <span class="math inline">\(\alpha_i \geq 0\)</span></p><blockquote><p>讲义上这里直接说 <img src="http://static.zybuluo.com/Preke/5mvvg20kv2e0gxdqdp812c8h/image_1bo1eb93hdmqn8gqvm1ts1m3s4v.png" alt="image_1bo1eb93hdmqn8gqvm1ts1m3s4v.png-20kB"> 但是我觉得这个式子只有在提出KKT条件再提比较合适，因为在不满足KKT条件的情况下，<span class="math inline">\(\alpha_ig_i(w)=0\)</span> 是不一定能成立的。但是为了方便表述，这里暂且先默认满足</p></blockquote><p>当 <span class="math inline">\(w\)</span> 满足所有约束时，我们就可以将问题转化如下： <span class="math display">\[\min_w\theta_\mathcal{P}(w)=\min_w\max_{\alpha,\beta:\alpha_i\geq0} \mathcal{L}(w,\alpha, \beta)\]</span></p><p>我们使用 <span class="math inline">\(p^*\)</span> 来表示 <span class="math inline">\(\min_w\theta_\mathcal{P}(w)\)</span> 原问题 <span class="math inline">\(p*\)</span> 是先求最大，再求最小，那么我们定义一个它的对偶问题 <span class="math inline">\(d*\)</span> ，即先求最小，再求最大； 首先： <span class="math display">\[ \theta_{\mathcal{D}}(\alpha, \beta)=\min_w\mathcal{L}(w,\alpha, \beta)\]</span> 那么： <span class="math display">\[ d^* = \max_{\alpha, \beta:\alpha_i\geq0}\theta_{\mathcal{D}}(\alpha, \beta)=\max_{\alpha, \beta:\alpha_i\geq0}\min_w\mathcal{L}(w,\alpha, \beta)\]</span> 就有: <span class="math display">\[d^* =\max_{\alpha, \beta:\alpha_i\geq0}\min_w\mathcal{L}(w,\alpha, \beta)\leq\min_w\max_{\alpha,\beta:\alpha_i\geq0} \mathcal{L}(w,\alpha, \beta)=p^*\]</span></p><blockquote><p>这一点，我们有这样一个事实： <span class="math display">\[\max \min f(x) \leq \min \max f(x)\]</span></p></blockquote><p>那么，再什么时候两者会等价呢？</p><p><strong><em>首先，是三个假设：</em></strong></p><ul><li><span class="math inline">\(f,g\)</span> 是凸函数</li><li><span class="math inline">\(h\)</span> 是仿射函数： $a_i, b_i, s.t. h_i(w)=a_i^Tw+b_i $</li><li>存在 $ w $, 使对于所有 <span class="math inline">\(i\)</span>, <span class="math inline">\(g_i(w)&lt;0\)</span></li></ul><p><strong>满足上述假设，就是一个凸优化问题</strong></p><p>满足之后，一定存在 <span class="math inline">\(w^*\)</span> , <span class="math inline">\(\alpha^*\)</span> , <span class="math inline">\(\beta^*\)</span>：</p><ul><li><span class="math inline">\(w^*\)</span> 是原问题的解</li><li><span class="math inline">\(\alpha^*, \beta^*\)</span> 是对偶问题的解</li><li><span class="math inline">\(p^* = d^* = \mathcal{L}(w^*,\alpha^*, \beta^*)\)</span></li></ul><p>同时 <span class="math inline">\(w^*, \alpha^*, \beta^*\)</span> 满足K.K.T.条件：</p><ul><li><span class="math inline">\(\frac{\partial}{\partial w_i}\mathcal{L}(w^*,\alpha^*, \beta^*)=0, \quad i = 1,...,n\)</span></li><li><span class="math inline">\(\frac{\partial}{\partial \beta_i}\mathcal{L}(w^*,\alpha^*, \beta^*)=0, \quad i = 1,...,l\)</span></li><li><span class="math inline">\(\alpha_i^*g_i(w^*)=0, \quad\quad\quad\quad i=1,...,k\)</span></li><li><span class="math inline">\(g_i(w^*)\leq0,\quad\quad\quad\quad\quad i=1,...,k\)</span></li><li><span class="math inline">\(\alpha^*\geq0, \quad\quad\quad\quad\quad\quad\quad i=1,...,k\)</span></li></ul><p>我们看后三条： 如果我们限定 <span class="math inline">\(\alpha_i &gt; 0\)</span> 那么 <span class="math inline">\(g_i(w) = 0\)</span> 就一定成立。 由于 <span class="math inline">\(g_i(w)\)</span> 是限定条件， <span class="math inline">\(g_i(w) = 0\)</span> 意味着，<span class="math inline">\(w\)</span> 处于可行域的边界上 &gt; 这也就是我之前提到过的 (其实是别人提到过，我只是印象深刻)，K.K.T.条件是在可行域边界上取得极值；至于在可行域内部的点 <span class="math inline">\(g_i(w^*)&lt;0\)</span> 的约束是不起作用的，我总觉得这里和支持向量的思想很像；</p><h2 id="最优间隔分类器-1">最优间隔分类器</h2><p>讲了一大堆，回到刚刚的优化问题： <span class="math display">\[min_{\gamma ,w,b}\quad\frac{1}{2}||w||^2 \\\quad\quad\quad\quad\quad\quad\quad\quad s.t.  \quad y^{(i)}(w^Tx^{(i)}+b)\geq1 ,\quad\quad i=1,...,m\\\]</span> 现在将约束条件改为： <span class="math inline">\(g_i(w) = -y^{(i)}(w^Tx^{(i)}+b) + 1\leq 0\)</span> 现在原问题是凸函数，不等式约束是一个线性函数，也是一个广义的凸函数 所以满足了K.K.T.条件的假设前提。</p><p>约束条件表明了，函数间隔为1是边界上的点 由K.K.T条件得知，(通常来说，在边界上的点，$ g_i(w)=0 $ 时， <span class="math inline">\(\alpha_i&gt;0\)</span>, 而在其余的点上，当，$ g_i(w)&gt;0 $ 时， <span class="math inline">\(\alpha_i=0\)</span> ）</p><p>现在可以构造拉格朗日函数如下： <span class="math display">\[\mathcal{L}(w,b, \alpha)=\frac{1}{2}||w||^2 -\sum_{i=1}^m\alpha_i[y^{(i)}(w^Tx^{(i)}+b) - 1], \quad \alpha_i\geq0\]</span> 注意参数不同了，是 <span class="math inline">\(w,b, \alpha\)</span> ; 和上面一样，我们构造一个 <span class="math inline">\(p^*\)</span> ,然后找到对偶问题 <span class="math inline">\(d^*\)</span>: <span class="math display">\[d^* =\max_{\alpha:\alpha_i\geq0}\min_{w,b}\mathcal{L}(w,b,\alpha)\leq\min_{w,b}\max_{\alpha:\alpha_i\geq0} \mathcal{L}(w,b,\alpha)=p^*\]</span></p><p>先求解$ _{w,b} (w,b, )$， 对 <span class="math inline">\(w\)</span> , <span class="math inline">\(b\)</span> 求偏导： <span class="math display">\[\frac{\partial}{\partial w}\mathcal{L}(w,b,\alpha)=w-\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}=0\\\frac{\partial}{\partial b}\mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_iy^{(i)}=0\]</span> 得到 <span class="math inline">\(w=\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}\)</span>, 代回原拉格朗日函数： （省略计算过程） <span class="math display">\[\mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}-b\sum_{i=1}^m\alpha_iy^{(i)} \]</span> 由于如上： <span class="math inline">\(\frac{\partial}{\partial b}\mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_iy^{(i)}=0\)</span>，最后一项为0，得： <span class="math display">\[\mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}\]</span></p><p>现在用 <span class="math inline">\(W(\alpha)\)</span> 表示 <span class="math inline">\(\min_{w,b}\mathcal{L}(w,b,\alpha)\)</span> , 再来求最大值: <span class="math display">\[d^* = \max_{\alpha:\alpha_i\geq0}\min_{w,b}\mathcal{L}(w,b,\alpha)=\max_{\alpha:\alpha_i\geq0}W(\alpha)\\ \\\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\max_\alpha\quad W(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}\\s.t.\quad \alpha_i\geq0,\quad i=1,...,m\\\sum_{i=1}^m\alpha_iy^{(i)}=0\]</span> 这个问题，留给之后解决，假设我们解决了这个问题，找到了 <span class="math inline">\(\alpha\)</span>, 又有样本点数据，我们很容易由 <span class="math inline">\(w=\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}\)</span> 得出 <span class="math inline">\(w\)</span>, 但是我们没有直接求出 <span class="math inline">\(b\)</span>, 但是我们知道离超平面最近的正的函数间隔要等于离超平面最近的负的函数间隔，据此，可以用其他变量将 <span class="math inline">\(b\)</span> 表出： <span class="math display">\[b=-\frac{\max_{i:y^{(i)}=-1}w^Tx^{(i)}+\min_{i:y^{(i)}=1}w^Tx^{(i)}}{2}\]</span></p><blockquote><p>这里我将 <span class="math inline">\(\alpha^*, w^*, b^*\)</span> 都用 <span class="math inline">\(\alpha,w,b\)</span> 表示，因为原问题的解和对偶问题相同</p></blockquote><p>这样，问题就解决了，我们只要找到那些函数间隔为1的样本点，代入上面的公式求出 <span class="math inline">\(w\)</span>, 和 <span class="math inline">\(b\)</span>，就能确定目标超平面 <span class="math inline">\(w^Tx+b\)</span> 了</p><p>所以，起到作用的只有那些“支持向量”，也就是，只有边界值决定了分类面。</p><p>That’s all for now.</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Softmax</title>
      <link href="/2017/08/15/Softmax/"/>
      <url>/2017/08/15/Softmax/</url>
      
        <content type="html"><![CDATA[<p>自己的一点理解，感觉还是要记下来…</p><p>Softmax 是将LR用于多分类, 就按照类比的思路写下来。</p><p>先简单说一下LR</p><p>首先，还是要基于这个框架（如下）做一个假设函数 <em>（hypothesis function)</em> <img src="http://static.zybuluo.com/Preke/0srm35jug5vuz26h6zlawwtn/image_1bnhtkrng19injn51ch81mah1vhu9.png" alt="image_1bnhtkrng19injn51ch81mah1vhu9.png-38.7kB"></p><p>假设自变量 x 是多维向量，也可以理解为多维特征吧： <span class="math display">\[x = [x_1, x_2, ..., x_n] \]</span> 我们就可以把假设函数设为： <img src="http://static.zybuluo.com/Preke/5apb38y9rbtfiv00lg2uvtcs/image_1bnhu1jfihjv7ebokv1ok311upm.png" alt="image_1bnhu1jfihjv7ebokv1ok311upm.png-9.2kB"> 为了简化，<span class="math inline">\(x_0 = 1\)</span>, 即 $x = [1, x_1, x_2, …, x_n] $， 就有： <img src="http://static.zybuluo.com/Preke/89a1cqx30q71xmzgbz88ensw/image_1bnhu4s0tk8a195a16e57fjulo13.png" alt="image_1bnhu4s0tk8a195a16e57fjulo13.png-10.4kB"> ### 此时 <span class="math inline">\(\theta\)</span> 是一个1 * n+1的向量 然后我们在将这个假设函数代入Sigmoid函数： <img src="http://static.zybuluo.com/Preke/t8f0yq2xipveelhvxb8b6a5d/image_1bnhuejle1ako1thb1vel1mdc71b2g.png" alt="image_1bnhuejle1ako1thb1vel1mdc71b2g.png-11kB"></p><p>然后定义loss function, 做梯度下降。</p><p>那类比到Softmax；因为解决的问题是多分类，Sigmoid函数就解决不了问题了。 但有的时候我不想这样，因为这样会造成分值小的那个饥饿。所以我希望分值大的那一项经常取到，分值小的那一项也偶尔可以取到，那么我用softmax就可以了 现在还是a和b，a&gt;b，如果我们取按照softmax来计算取a和b的概率，那a的softmax值大于b的，所以a会经常取到，而b也会偶尔取到，概率跟它们本来的大小有关。所以说不是max，而是 Soft max 所以这里引入Softmax函数： 假设 <span class="math inline">\(V = [v_1, v_2, ... v_m]\)</span> 那么第i个元素的Softmax值： <span class="math display">\[S_{i} = \frac{e^{v_i}}{\sum_{j=1}^me^{v_j}} \]</span></p><p>那如果现在将softmax应用到多分类： 依然用同样的自变量x, 同样的假设函数，如上： <img src="http://static.zybuluo.com/Preke/5apb38y9rbtfiv00lg2uvtcs/image_1bnhu1jfihjv7ebokv1ok311upm.png" alt="image_1bnhu1jfihjv7ebokv1ok311upm.png-9.2kB"> <img src="http://static.zybuluo.com/Preke/89a1cqx30q71xmzgbz88ensw/image_1bnhu4s0tk8a195a16e57fjulo13.png" alt="image_1bnhu4s0tk8a195a16e57fjulo13.png-10.4kB"></p><h3 id="但是这里的-theta-不再是一个1-n1的向量我们假设目标类别-y-有-k-个类别-这里的-theta-是一个k-n1的矩阵">但是这里的 <span class="math inline">\(\theta\)</span> 不再是一个1 * n+1的向量，我们假设目标类别 y 有 k 个类别， 这里的 <span class="math inline">\(\theta\)</span> 是一个k * n+1的矩阵。</h3><p>和上面一样，<span class="math inline">\(\theta\)</span>就是我们要训练的参数； 这里可以列出LR 和 softmax的loss function 做对比： LR： <img src="http://static.zybuluo.com/Preke/hdvjkow48tifcdafxqr17b4y/image_1bni13u0ec77erp1cbhuqng512t.png" alt="image_1bni13u0ec77erp1cbhuqng512t.png-21.9kB"> Softmax: <img src="http://static.zybuluo.com/Preke/x57fudisitq8helhpzlmgl1b/image_1bni14rgogv81i6orrt14r54gi3q.png" alt="image_1bni14rgogv81i6orrt14r54gi3q.png-22.3kB"></p><p>可以看到，Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的 <span class="math inline">\(\textstyle k\)</span> 个可能值进行了累加。注意在Softmax回归中将 $x <span class="math inline">\(分类为类别\)</span> j$ 的概率为： <img src="http://static.zybuluo.com/Preke/syydz0jjarxeciwlyap7lukc/image_1bni187lu1ddi1rli1d751o3uoim47.png" alt="image_1bni187lu1ddi1rli1d751o3uoim47.png-16.8kB"></p><h2 id="总结">总结：</h2><p>softmax对于LR来说</p><ul><li>目标函数不同（sigmoid 和 softmax）</li><li>优化参数 <span class="math inline">\(\textstyle \theta\)</span> 的形式不同</li></ul><p>至于loss function的推导，暂时还没有深究；</p><h2 id="ref">ref:</h2><p>http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92 http://blog.csdn.net/supercally/article/details/54234115</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Word2vec 入门（skip-gram部分)</title>
      <link href="/2017/08/15/Word2vec%20%E5%85%A5%E9%97%A8%EF%BC%88skip-gram%E9%83%A8%E5%88%86)/"/>
      <url>/2017/08/15/Word2vec%20%E5%85%A5%E9%97%A8%EF%BC%88skip-gram%E9%83%A8%E5%88%86)/</url>
      
        <content type="html"><![CDATA[<h1 id="skip-gram">Skip-gram</h1><p>给定句子中一个特定的词（input word），随机选它附近的一个词。网络的目标是预测 我们选到这个附近词的概率。</p><h2 id="输入输出">输入，输出</h2><p>取窗口大小为2（前后两个词）：得到一些词对： <img src="http://static.zybuluo.com/Preke/d6lc00zoy5k5qxkqvjwz7ad3/image_1bnistvll1f771ivtjos1nh6d459.png" alt="image_1bnistvll1f771ivtjos1nh6d459.png-71.8kB"></p><p>如之中的（quick, brown) 训练神经网络时： <img src="http://static.zybuluo.com/Preke/8z7fokjbulvp8ppecxr077fv/image_1bnit17pvb711tn81mjlpl1mq7m.png" alt="image_1bnit17pvb711tn81mjlpl1mq7m.png-90.7kB"> 输入quick的one-hot编码, 输出层softmax分层的brown的概率应该是最大的</p><hr><h2 id="隐层">隐层：</h2><p>我们训练一个简单的网络来执行一个任务，但是我们实际上并没有用这个网络来预测test中的任务。而是，利用这个任务（目标）去学习网络中的权重W。我们将看到这些学出来的权重W就是我们所要的词向量（wordvectors）。</p><p>假设corpus里有10000个词语， 目标词向量为 300维： 简单做一个图来表示过程： 依然用（quick, brown)训练神经网络 <img src="http://static.zybuluo.com/Preke/l1j7twk5xqthyat1j8cm4b6m/image_1bnivd1134qnd0a7cu1v1k1p7a1g.png" alt="image_1bnivd1134qnd0a7cu1v1k1p7a1g.png-63kB"></p><hr><p>如果两个不同的词有相同的上下文，那么我们的模型会需要预测类似的输出。那么网络为了输出类似的预测，他就会使这两个不同词的word vector尽可能相似。所以，如果两个词有相同的上下文，我们就可以得到比较接近的word vector。</p><p>那么什么词有相同的上下文？ 比如一些近义词 smart 和intelligent 再比如 一些相关的词 engine 和 transmission。</p><hr><h1 id="训练过程">训练过程：</h1><p>所以word2vec采用了降采样（subsampling)的策略。对于每个我们在训练样本中遇到的词，我们有一个概率去删除它。这个概率与单词出现的频率相关。</p><p><strong><em>(这里word2vec)就直接做了一个类似 去掉一些大众词汇，类似tf_idf的工作</em></strong></p><hr><p>训练神经网络 意味着输入一个训练样本调整weight，让它预测这个训练样本更准。换句话说，每个训练样本将会影响网络中所有的weight。像我们之前讨论的一样，我们词典的大小意味着我们有好多weight，所有都要轻微的调整。</p><p>Negative sampling 解决了这个问题，每次我们就修改了其中一小部分weight，而不是全部。 （这里其实我还有一点疑问，因为没有细读paper, 既然输入的是one-hot向量，相当于look-up的话，和别的权重（隐层神经元）有什么关系呢？有必要Negative sampling吗？）</p><p>Ref:</p><p>http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</p><p>http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</p><p>http://blog.csdn.net/Layumi1993/article/details/72866235</p><p>http://blog.csdn.net/Layumi1993/article/details/72868399</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>骊歌</title>
      <link href="/2017/04/17/%E9%AA%8A%E6%AD%8C/"/>
      <url>/2017/04/17/%E9%AA%8A%E6%AD%8C/</url>
      
        <content type="html"><![CDATA[<p>By Gala</p><p>虽然虽然很爱你 却要阉了你</p><p>不然别人抢了去 不留下点滴</p><p>虽然虽然很怨你 却仍然痴迷</p><p>怀了别人的孩子 我也没脾气</p><p>虽然虽然很怕你 却要追随你</p><p>求你别轻言离去 因知音难觅</p><p>虽然虽然很爱你 却不懂珍惜</p><p>任由时光荏苒你 追悔已莫及</p><p>虽然虽然很想你 想联络却迟疑</p><p>命中注定的相遇 都失之交臂</p><p>虽然虽然失去你 梦里仍有你</p><p>求你别轻言离去 因知音难觅</p><p>求你别轻言离去 因知音难觅</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>随记4.17</title>
      <link href="/2017/04/17/%E9%9A%8F%E8%AE%B04.17/"/>
      <url>/2017/04/17/%E9%9A%8F%E8%AE%B04.17/</url>
      
        <content type="html"><![CDATA[<blockquote><p>学士服是租的，毕业照是学校安排的，论文是抄的，答辩是水的，实习报告是假的，三方合同是骗就业率的……一切都是假的，只有时间是真的。他每天都在告诉我们，青春，终究是要散场。</p></blockquote><p>看到上面一句话，感触还挺深的。</p><p>好久没来写些东西了，我都怕我手生了，我都怕我不敢直面自己的想法了。 依然希望这里是一片净土</p><p>最近看了15年，16年的中大的毕业快闪，感慨中大的人文情怀真是名不虚传，也庆幸自己能来，今年的毕业季马上就要来了~红领带，白衬衫，还是gfs特有的黑西装，蛤蟆镜，真是期待。</p><p>唉，真的手生了，写不下去了，想法还没有经过沉淀还是怎样，刻意去写又不知道该写些什么好。</p><p>压力是紧迫感，压力也能转化成动力 然而压力让你每天不能心安，不能静静的享受生活 突然感觉，好长好长一段时间，我都不能放空脑子，抛开一切去做一些事情了 也许这样的场景本身就不应该存在的</p><p>总有担忧和顾虑，总觉得要想尽办法提升自己，美其名曰充实，又何尝不是忙碌得像条狗呢 劳逸结合可真是一个大课题，我初中开始，甚至小学开始就没有学会</p><p>如果有一天我累的完全倒下，当然我知道这是不存在的 没有预想中的轰轰烈烈 生活就像温水煮青蛙一般，一点一点残蚀着你的耐心 就像5千米一样，我也无奈，不能拼尽全力去终点之后倒下再也站不起来 只能沉重的呼吸，拖着酸痛的两条腿，亦步亦趋</p><p>这场修行，可真难 沿途的风景固然美丽，但是总觉得与我无关</p><p>既然选择了远方，便只顾风雨兼程 这句话很早就看过，很早就不屑过 然而前两天重新看到，发现是会有一股温暖的力量涌上心头</p><p>不是所有的结尾都是欲扬先抑，温暖的力量给自己的动力也只是支撑自己走下去这段路。 《云图》里说，自杀是最勇敢的行为 我果然，还没那么勇敢，也没那么懦弱 我也不能给自己贴一个标签</p><p>好吧，不写了。</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯</title>
      <link href="/2017/02/24/Naive_Bayes/"/>
      <url>/2017/02/24/Naive_Bayes/</url>
      
        <content type="html"><![CDATA[<p>回去看完了Andrew Ng的关于这一节的公开课，还有中文的课堂笔记 然后参照了一下《集体智慧编程》这本书，现在对这个算法有一个全面的理解了</p><p>贝叶斯决策，基本上，从直观理解就是做了这样的事情： 给定历史的数据和类别，然后新来一个数据，计算新来的数据属于每个类别的概率，然后将新来的数据归为概率最大的那一类。</p><p>###贝叶斯定理：</p><p><span class="math display">\[p(c|w) =  \frac{p(w|c)p(c)}{p(w)}\]</span></p><p>###为什么说是朴素贝叶斯： * n个特征相互独立 （为了简化条件概率的计算） * n个特征同等重要</p><p>###问题场景： 就举《机器学习实战》的例子吧，做一个文本分类，如果一段文本里，包含侮辱性词汇，标记为不当言论。</p><p>###直观理解： c is category, w is words,可以理解成邮件 c = 0 : 不是垃圾邮件 c = 1 : 是垃圾邮件</p><p>###先看怎样运作： 对一个特定的邮件 w, 我计算 <span class="math inline">\(p(c = 1|w)\)</span> 和 <span class="math inline">\(p(c = 0|w)\)</span> 判断这两个概率哪个大，就把文档归为哪一类。</p><p>本来我们计算两个概率，是应该根据贝叶斯定理的公式（如上）的，但是对于一篇文档来说， <span class="math inline">\(p(w)\)</span> 是相同的，反正我们的目的只是比大小，定性分析，同除 <span class="math inline">\(p(w)\)</span> 并不影响结果，所以</p><p>###目标函数：</p><p><span class="math display">\[p(c|w) =  p(w|c)p(c)\]</span></p><p>以下阐述帮助理解，实际用时要做优化，优化在最后提一下</p><p>该如何建模？</p><p>###生成词集模型： 把所有邮件（假设我把我目前邮箱里的所有邮件都用做训练），分词，将所有出现过的词语包含到一个set当中。</p><p>###生成文本向量： 假设刚刚set有1000个词，那么我将每个邮件做成一个1000维的向量，每一维对应这个词是否在该文档出现，用0，1表示。</p><p>###训练模型： 我们是对这样一个生成模型建模，生成模型求的是联合概率。 先说个简单的，<span class="math inline">\(p(c)\)</span> 是什么？是人工标记训练集中垃圾邮件的频率，是一个固定的数值，比如我的邮箱里50封邮件全都作为训练，我人工标记，有10封为垃圾邮件，那么<span class="math inline">\(p(c = 1) = 0.2\)</span>； <span class="math inline">\(p(c = 0) = 0.8\)</span></p><p>那么$p(w|c) $ 呢，既然朴素贝叶斯做出了特征相互独立的假设，那么我们就假定，在任意一封邮件中，每个词语出现的概率是不会相互影响的。我们设词集中1000个词分别为：<span class="math inline">\(x_1 , x_2, ... x_{1000}\)</span> , 这样就有</p><p><span class="math display">\[p(w|c)  = \prod_{i=1}^{1000}p(x_{i}|y)\]</span></p><p>我们还是分为两部分：</p><h4 id="pwc-1">$p(w|c = 1) $ ：</h4><p>在那些标记为垃圾邮件的文本向量中，假设有n个： 我们把他们全部加起来，得到一个向量，然后再将这个向量的每一位都除以n。</p><p>好的我们就得到了一个特征向量<span class="math inline">\(p_{1}\)</span>， 这个特征向量每一位表示对应的词语在垃圾邮件中出现的概率 同理可得<span class="math inline">\(p_{0}\)</span></p><p>这里就可以看出，生成模型是什么，我们之前不知道参数的大小，是通过训练集生成参数，然后再来求条件概率，换句话说，我们是对参数进行了最大似然估计。</p><p>###验证方法：</p><p>用了向量乘积来判别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">p1 = sum(vec2Classify * p1Vec) + log(pClass1)    <span class="comment">#element-wise mult</span></span><br><span class="line">p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1.0</span> - pClass1)</span><br><span class="line"><span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>: </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>vec2Classify 为待检验的文本向量； 这里做了一下数学处理（log）</p><p>我想说的是，为什么验证方法中的前半部分是文本向量与特征向量对位相乘然后求和呢？ 我们知道，特征向量中是对位该词出现的概率，而文本向量中是是否出现，用0，1表示 所以，对位相乘，粗浅理解，就是表现该文档中这些特征词一共出现了多少。</p><p>换句话说吧，我们会不会有这样的疑问？ 为什么我们不建立一个dirty words的dict，然后去扫新邮件中有没有包含dict中的词语，这样不就可以直接确定分类了么？要概率干嘛？</p><p>我个人的理解，特征向量，就是最能够表示该类别特性的一个向量，那些再特征向量中出现的概率高的词，最能代表了该类别文本的特性，也就是说，如果有一个文档，包含了这些词，是有极大可能属于该类别的。</p><p>所以，如果只建立dict的话，有这样的问题，垃圾邮件只是说明包含这些特征词的概率大，但是不能完全确定一封邮件只要有这个词，就是属于垃圾邮件，所以只是概率的问题。</p><p>这样结合考虑，应该能够明白了吧。</p><p>所以，上述只是帮助理解的一个小例子，如果实用，要优化： 1. 换词集为词袋，考虑文本中单词出现的频次，而不是简单用0，1表示是否出现 2. 中间步骤<span class="math inline">\(p(w|c) = \prod_{i=1}^{1000}p(x_{i}|y)\)</span>，有多个概率相乘，如果有一个概率0，则结果为0，不合理，所以用一下Laplace平滑</p><p>还有更多优化步骤，我就不多说了…..</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>随记2.13</title>
      <link href="/2017/02/13/%E9%9A%8F%E8%AE%B02.13/"/>
      <url>/2017/02/13/%E9%9A%8F%E8%AE%B02.13/</url>
      
        <content type="html"><![CDATA[<p>看了一篇影评，看完了这篇影评，原文链接在下面，这篇文章是我的摘抄，我和作者有很多共鸣； 作者在文中写道：我从不认为自己的此文就是终极正解，我愿忍受痛苦写这么多，就是渴盼交流 这与我的想法不谋而合，我也在之前的文章中写过类似的话。</p><p>那样一个每天都看不清楚明天的格局的时代，那个变革的时代，青年人们一定会迷茫，思索，也必然造就了青春的别样色彩 我不觉得他们和当时民国时期探索救国之路的戊戌六君子，李大钊，陈独秀有什么两样，他们都是勇敢的先驱。</p><p>我想起我的父亲当时也差点登上了开往北京的列车 我也想起父辈们跟我讲，我们这一代工农阶级确实是改革开放的既得利益者 摘抄的第一段很好的诠释了我们的思想 我们从来都爱祖国。</p><p>我也感谢我的每个阶段，都有这样一个朋友能给我一些心灵上的启迪，我就是在各种思想不断碰撞中思索。</p><p>我也审视过自己，写出想法有什么用？ 是为了宣扬自己和别人不一样？ 还是渴求遇到知音能够共同探讨？ 或许有，但又或许都不是； 我们都是欲望的动物，表达本身就是一种欲望</p><p>自由之路上必定鲜血淋漓 死在巴士底狱，死在反抗黑奴贸易，死在奥斯维辛，死在二战战场，死在南京大屠杀，死在抗日战场，死在柏林墙下，死在广场上的那些人，都是英雄</p><p>社会在进步，反贪反腐有了很大的成效； 建立政权几十年的时候，正是我们的盛世；</p><p>愿有生之年能看到GFW的倒塌</p><p>我不怀疑过分解读，虽然有些想法被赤裸裸的说出来是有悖于文艺片本身的 但是我相信每一种想法都有它被表达出来的权利。 原文地址：http://weibo.com/p/1001603906301808764265</p><h2 id="摘抄">摘抄</h2><hr><p>为什么经历了文革、上山下乡、七六年“四五”天安门事件、西单民主墙、八六学潮的无情洗礼之后，这新一代年轻人仍然敢于为了祖国的未来勇敢地站出来抗争？</p><hr><p>“假如一间铁屋子，是绝无窗户而万难破毁的，里面有许多熟睡的人们，不久都要闷死了，然而是从昏睡入死灭，并不感到就死的悲哀。现在你大嚷起来，惊起了较为清醒的几个人，使这不幸的少数者来受无可挽救的临终的苦楚，你倒以为对得起他们么？” “然而几个人既然起来，你不能说决没有毁坏这铁屋的希望。”</p><p>鲁迅《呐喊》</p><hr><p>“在1983年“严打”中，一个王姓女子因与10多名男子发生性关系而以流氓罪被判处死刑。面对死刑判决，这王姓女子说了这么一段话：性自由是我选择的一种生活方式，我的这种行为现在也许是超前的，但20年以后人们就不会这样看了。 一语成谶。在20年后的今天，尽管性自由仍未成为主流的社会道德，但人们对于性行为已经宽容多了。在刑法中流氓罪已经取消，与多人发生性关系，只要不妨害公共秩序，连犯罪也构不成了，更不用说判处死刑。 《1983：偶像的清晨》</p><hr><p>八九年事件表面上的触发点是胡耀邦逝世；中层的原因是1985年起，戈尔巴乔夫在苏联推行以人道主义为核心的新思维运动，对社会主义阵营产生了广泛的影响；深层原因是单纯的经济改革遇到瓶颈，造成了高达26％的城市通胀与超过20％的全国通胀，在农村，由于政府低价强制收购粮食并且因为财政短缺而不给现金、采用“打白条”的形式进行收购，造成农民不愿种粮食，而造成粮食减产，每年约八百万农民涌向城市。在城市，由于改革的推进，一批国营企业关闭，造成全国约有数百万工人失业，而在职的工人也要承受苛刻的工作条件以及低收入面对高通胀的巨大压力，工人开始骚动不安。其实，苦日子刚过去不久，中国老百姓不是不能吃苦，但是在老百姓捱苦日子的同时，官员们却通过贪污、官倒大发其财，所谓“民不患贫而患不均”，这就是邓小平所谓“这场风波迟早要来，这是国际的大气候和中国自己的小气候所决定了的，是一定要来的，是不以人们的意志为转移的”这话的注 脚。 　　 所以，“为何对眼前的一切漠然，而去注目永不可期的事物呢？”这句话指出了学生失败之处：对社会的真实情况并不深刻了解，对老百姓真正关心的利益由于知识分子的清高而不识民间疾苦而漠然，所以并未获得全国民众的最广泛支持，最支持学生的，除了知识分子以外，主要是文化水平较高的部分市民。实际上，最能获得人民共鸣的口号是反贪污、反官倒，但是随着国家不断地拖延与拒绝，知识分子与学生很快心急地放弃了这些口号，转而要求民主自由、政治体制改革，这些诉求，老百姓不懂，也觉得不可能</p><hr><p>　Whether there is freedom and love or not, in death everyone is equal. I hope that death is not your end. You adored the light, so you will never fear the darkness.</p><hr><p>《颐和园》基本上是一部情绪化的电影，它不是一部历史电影，它是一部关于情爱的、关于冲动的（电影），这个冲动，就是说爱情的冲动和政治的冲动是一样的，那是一个冲动的年代。”</p><hr><p>89年整个就像是学生和政府的一次做爱，然后这次做爱没做好，做得很难受，很不舒服，然后政府打了学生一个耳光，然后打得太重了，流血了，政府也知道，然后他觉得那个耳光打得太重了，然后他用之后的十年来找回这个过份的举动。你从这个角度来看，六四和两个人谈恋爱是一模一样的，中国用很短的时间结束了六四的混乱、八九年的混乱，是用强硬的、军队进入的方式、军事管制的方式，那么，实际上中国是用之后的十年拼命地在不断地向前发展经济、更加地开放、更加地民主，所有这一切就象他们之间发生的爱情一样。所以我觉得就是说爱情实际上是整个世界的一片叶子——如果整个世界是一棵树的话，爱情是这棵树上的一片叶子。那么，一片叶子上面你就可以读到整棵树的信息。所以，不用这么麻烦，我只要表达爱情就可以了。我说清楚了爱情，也就说清楚了这个世界。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>随记1.18</title>
      <link href="/2017/01/18/%E9%9A%8F%E8%AE%B01.18/"/>
      <url>/2017/01/18/%E9%9A%8F%E8%AE%B01.18/</url>
      
        <content type="html"><![CDATA[<h2 id="不忮不求">不忮不求</h2><p>诗经有：不忮不求，何用不臧 意思是不嫉妒不贪求，什么行为能不好呢</p><p>偶然看到这句话，然后百度了一下，看到一所中学的校训： <strong>有为有守，不忮不求</strong> 妙不可言，不禁感慨这所中学的文化底蕴，校训比太多太多的大学的校训要好太多了</p><p>君子，有作为，有操守，不嫉妒，不贪求 泰然自若，是内心对自己的一种肯定，是一种强大的自信</p><h2 id="议论文">议论文</h2><p>初中看过一篇励志演讲，说有个人中考作文满分，写的议论文就是请来一堆古今名家来一起讨论文章的主题；印象深刻的是，当时班里还有同学模仿这样的新奇模式，但是好像并没有收到良好的效果</p><p>个人觉得，议论文，就是议论为主，议论就是要多人参与 高中议论文，800字要求 一个观点，阐述清楚之后，还要多个事例佐证，难道是对自己的观点不自信么？ 苦思冥想找例证，旁征博引凑篇幅，好不牵强，好不啰嗦 我不认为提出自己的一个看法需要很长的篇幅，条理清晰，简明扼要，就够了 其他的方面是和他人的思想交流中再产生的，既然考场上不能交流，又怎能是议论文呢？ 自己议论吗？</p><p>一家之言 应试应试，无奈改变不了现状 八股八股，庆幸我走出了</p><h2 id="本我">本我</h2><p>人是喜欢的人和不喜欢的人的集合，是拿来主义的结果 那什么又是纯净的自己呢？ 是这一套拿来主义的原则吗？或许这套原则本身也是习得的，也不是原有的 弗洛伊德提出的三个概念，本我，自我，和超我 本我是指人格中最早，也是最原始的部分，是唯乐主义 那是否抛开了生理结构差异，智商差异，人人在一开始都一样了呢？ 在自我意识高度觉醒的现在，在这个追求个性的现在，这种观点看似不好被人接受 那究竟是什么？ 我欲抛砖引玉，却发现手中连块砖头都没有</p><hr><p>就这样吧，闲言碎语不想多讲，随性就好</p><p><strong>你住的城市下雨了，很想问你有没有带伞，可是我忍住了。因为我怕你说没带 而我又无能为力，就像是我爱你 却给不到你想要的陪伴。</strong> 宫崎骏说过这一句话 道出了多少苹果和香蕉的无奈 合适的才是最好的 愿爱无忧</p><p>苹果和香蕉：我爱你，我可以给你我的全部，可是我只有香蕉，可是你只要苹果</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>随记1.13</title>
      <link href="/2017/01/13/%E9%9A%8F%E8%AE%B01.13/"/>
      <url>/2017/01/13/%E9%9A%8F%E8%AE%B01.13/</url>
      
        <content type="html"><![CDATA[<h2 id="section">1200</h2><p>终于在夜里来了一次广州，马路上果然大多是货车。 路上没什么人，飘着微微细雨。 这个点，几乎所有华丽的建筑的装饰灯光已经关闭。 偶尔见到穿着塑料袋的流浪汉对着垃圾桶撒尿；疲惫的环卫工人裹着厚厚的工作服坐在报亭旁睡着。 冷？冷清</p><p>先去了体育东的1200，狭小却挤满了读者，几个人没有座位索性坐在楼梯上就开始阅读；一位大叔在角落里架着摄像机，也许是在准备微博上的一天一人一个故事？温馨的钢琴曲，免费的柠檬水，再狭小也有一间沙发客的住所，一隅能够躲避屋外寒冷细雨的屋舍，一盏温暖的城市里的明灯。</p><p>传言这里不卖畅销书？然而我进去之后看到《谁的青春不迷茫》这种，满满失望，更失望的是，我本以为能够找到《写在人生边上》或者《走在人生边上》，我当然可以去亚马逊邮寄过来，然而我更抱着一丝深夜咖啡，暖灯下阅读一篇篇小品文的幻想，呵。</p><p>没有就撤吧，再去天河北，路上看到马路对面的西西弗书店，哦？万菱汇早就关门了。跟着百度地图走了大概45分钟，到达了这里，宽敞了许多，人少了许多。我进来直奔书架，两个品类映入眼帘，<strong>诗和远方</strong>，呵。</p><p>书架上，我看到了《洗澡》，看到了《我们仨》，看到了《围城》，希望一点一点燃起，然而最终还是没有看到我想阅读的那两本书。罢了，无缘无缘，店里响起《one of us》的吉他就想起方渐鸿来，呵。</p><h2 id="再见乌托邦">再见，乌托邦</h2><p>一个月前看了网易出的国摇30年的纪录片，看完说不出话来。 前两天看了纪录片《再见，乌托邦》，看完只能默默的抽烟。</p><p>张炬死了，剩下刘老五的唐朝，也难再听到《国际歌》了； 崔健因为一首戏谑的《南泥湾》被央视封杀20年； 同样是唱西藏，郑钧的《风马》和《回到拉萨》简直不能比； 窦唯现在拒绝一切公共场合的发言了，也许是古乐，也许是后摇了，虽然没有那么闪闪发亮，但也是一个艺术家； 黑豹，怎么说呢，就算我们不拿窦唯在的时候对比，现在同时代的音乐也就一般般了吧，新歌歌名《键盘·狭》，我快以为是网络歌曲了； 张楚，他说他还在做音乐吧，也许吧。 何勇…… 长江后浪推前浪，他们都已经不再是如今摇滚圈里当红的人物了，也许他们�活得不差，也许他们也在做自己喜欢的事情，没有必要为了我们的喜好再去怎样怎样。</p><p>然而，我看过最难受的是，吴柯的父母谈及自己儿子24就自杀的无奈与自责，抑或是茫然，他父亲还帮忙制作了《一无所有》，他们对摇滚，又是怎样的态度呢。</p><p>我看过最难受的是，94年红磡魔岩三杰里台风最牛逼的何勇，戴着红领巾在台上大喊姑娘漂亮的何勇，父亲为其伴奏三弦儿，窦唯为其吹笛子的何勇，发福了，疯了，精神出问题了。</p><p>吃饭那段，镜头里的他是那么不安，与这个现代化的社会是那么格格不入； 他说没钱，一年只能去小酒吧演出3场，能不能再去找魔岩要点钱，由头则是10年间他们超出了授权期一直在卖他的唱片。他说，他就想去个安静的地方，养条狗，看书弹琴，什么都不想，然而也没钱去实现，后来，后来住进精神病医院了吧。</p><p>当年台上叱咤风云，如今温饱也许都成问题。 除了转型不错的，你说历史残酷，你说产业剧变，唱片业衰落，电子音乐崛起；你说张楚何勇没跟得上时代的发展，那么那么国摇的未来呢？你去看草莓，你去看迷笛，现在不是没有好听的摇滚歌曲，不是没有躁动的摇滚现场，可是，哪个乐队能再唱出《垃圾场》？哪个乐队能再唱出《高级动物》？也许我接触的不够多，但是放眼望去，除了万青还在控诉着工业时代的矛盾，还在呐喊着理想主义，我再看不到别人。</p><p>对，摇滚，也许关系最为紧密的词语，就是理想主义。 我之前写过：摇滚是思想碰撞的产物，摇滚牛逼是因为产生了思想的共鸣，大家把摇滚乐手视为意见领袖，在社会制度和解放思想矛盾没有那么尖锐的今天，在大多数人已经小康的今天，或是教育水平越来越发达的今天，拿起鼓棒，敲给谁听呢？</p><p>如果将来的将来，我不相信每个阶段社会都会平稳过渡；矛盾总会有，如果我们都学会以更理性的，更温和的方式去看待，去接受，去改进了；没有人呐喊，没有人激起愤青骨子里的热血，甚至没有愤青了，那说明，社会主义精神文明建设见到成效了，我们都变成文明人了。</p><p>第一个纪录片《少年心气》中，大家一致看好商业化的完善会让中国的摇滚走向更好的未来，是有一个更公平的竞争平台，好听的音乐能够脱颖而出；然而，理想主义的缺失，再好听的歌曲，没有了从思想上打动人的地方，商业化也能解决吗？</p><h2 id="痛仰">痛仰</h2><p>痛苦的信仰变成了痛仰； 自刎哪吒变成了闭目哪吒； 《哪里有压迫，哪里就有反抗》变成了《愿爱无忧》；</p><p>虎哥的歌儿没有变得不好听，相比早期的嘶吼； 我反而更喜欢《两个人的假期》和《带着镣铐起舞》</p><p>痛仰，是让我对意见领袖这个词语思考最多的一个乐队。 直到前一段田然出事儿，虎哥在微博上发的言论，依然让我觉得他像一个小孩儿一样耍赖皮。</p><p>之所以对虎哥印象深刻；他发的状态，言论涉及的话题；我觉得像极了有一段时间的我，处处叛逆，觉得个人自由大于一切，觉得一些教育就是洗脑；然而一样，从另一个角度考虑，制度的制订是为了群体利益的最大化，过分强调个人只会显得不成熟。至于洗脑，哈哈，见仁见智吧。你怎么能说接受的教育是洗脑，而接受与此相反的一些观点就不是呢？能够独立思考的人不会被洗脑，真正被洗脑的人唤不醒，何苦何苦。</p><p>不喜欢的一点就是，网易云下，虎哥发动态，下面人开始撕，然后虎哥开始骂，到最后偏了题，开始互喷，最后演变成了：</p><p>你就是傻逼 我就是这样，不喜欢的滚。 …</p><p>还是爱呀，毕竟 社会需要愤青， 社会需要有人从另一方面激发人们的思考， 社会需要有人提醒人们不要轻信一切。</p><h2 id="自我意识">自我意识</h2><p>存钱干嘛，最应该舍得的就是要投资自己； 来一次说走就走的旅行； 男人（女人）就要对自己好一点； 不能说错，然而类似的观点多得让我觉得我们被这种观念洗了脑。</p><p>每个人都应该努力奋斗，买车买房； 中国人在做American Dream？ 要知道08年的经济危机的诱因之一就是过于乐观的美国梦，有了这个支持，大家才会觉得房价永不会跌，那些信用评级机构才敢放心大胆的去将一些金融衍生品以次充好，导致最后巨大的杠杆崩盘的。</p><p>如何应对不时之需呢？ 如何考虑孝敬父母呢？ 如何做一个长期的可持续的规划呢？</p><p>这一点我只是有感性的触动，目前没有怎样深入的思考。</p><p>##生活不止眼前的苟且，还有远方的苟且 生活不止眼前的苟且，过完了眼前的苟且，远方依然是苟且； 因为，至少像我这样的人，有生之年是见不到自己填平欲望的沟壑了； 诗和远方，也许只存在于幻想中吧。</p><h2 id="变节">变节</h2><p>GayScript，现在直接叫徐沪生了，江苏高考200分的数学卷子（不知道他考多少，但是高中做江苏卷的经历告诉我江苏那边的人，数学个顶个的变态），信息学竞赛保送到上交读软工，多好一个程序员，后来弃坑写文学作品去了…</p><p>我当时还不屑呢，现在发现自己写软文也快感十足，欲罢不能…变节变节</p><hr><p><strong>观点仅代表此刻想法，不成熟之处，还望指教</strong></p><p>完成，没读到书，那就走吧，继续逛~ 一杯死贵的咖啡。。。不过看环境这么好就原谅了吧。 写完，我自己都有点不肯定自己的观点，偏激和片面，然而我更希望的是有人可以交流。 我一直信奉一句话：</p><p><strong>思想因碰撞而深刻</strong></p><p>我的无脑叛逆就是跟我爸一次次辩论中扭过来的。而且这些碰撞之后形成的观点让我更能有体会。</p><p>That’s all.</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>修行</title>
      <link href="/2017/01/01/%E4%BF%AE%E8%A1%8C/"/>
      <url>/2017/01/01/%E4%BF%AE%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<p>不知道为什么，我喜欢的一个简书的作者停更了。所以每次打开简书些许也有些寂寞。</p><p>这是跨年夜，也或许是个巧合，其实看着朋友圈里一个个在抒发情感，去年的新年愿望实现了吗，去年收获了什么，今年又要继续加油…..也颇有感慨。</p><p>我只是要毕业了。</p><p>今天长隆，草莓跨年，买了通票，今天却没去成，错过了万青，错过了新裤子，错过了痛仰，几乎是我对这次草莓的全部期待了。</p><p>9个小时之前，补考5000米，我这周临时抱佛脚，中间练了一次跑进了23分，觉得稳了，然而结果还是23分41秒。</p><p>一周之前，期末考核，5000米，23分30过，我23分42；成为了当天我们年级第一个没过的人；朋友说，要是你再努力一点呢，说不定就过了，结果那次的结果导致了补考，又不知为什么补考从周五推迟到了今天；导致了今天去不了草莓。</p><p>一年半之前，大二结束，按照中大的政策，大三的成绩不计入保研评测，我随意玩，随意选，想学什么学什么，玩游戏想通宵就通宵，睡过头，课想翘就翘，期末复习也没什么动力，于是我收获了大学的第一个60分，大三一年绩点渣到爆；gfs这边政策一变，我的保研就变成了飘忽不定的事情。</p><p>晚上回来，我恨恨的说，要是我就24分多，我现在屁都不放一个，那就是没练，怎么着都不行，偏偏就这十几秒，而且我自己练还能过，我要是中间知道，再逼自己紧一点，说不定就过了。</p><p>今天考核，一个哥们儿跑休克了，我们去医院看他，回来的路上聊到另一个哥们儿，也是考核的时候，跑完整个人虚脱，我说，我啥时候有些决心把自己逼到这份儿上，能快多少呢。</p><p>亦或是说，虽然保研这件事情对我来说可能目前是最好的结果，但是如果我大三没那么放纵，也不会卡在这个位置这么尴尬。我就是活的太安逸了，我不想说，优秀的人都是最努力的，这就是屁话，我只是想看看竭尽全力的我到底是个什么样子。</p><p>因为我心里还是太难受了。人啊，找准自己的位置很重要，但是我真正审视一下自己，我追求的东西，出路，感情，太多太多都不是属于我的世界的东西。那我能怎么办，接受自己永远得不到的事实？还是抱有幻想的再去拼一把，再去接受？</p><p>5000多简单，就那二十几分钟，到了终点线，过了就是过了，没过就是没过。然而生活中好多事情根本不存在终点，我又怎样知道什么时候该停止脚步，把现状当成结果呢？</p><p>生存就已经很艰难，再去追求一些不属于自己的世界的人和事，谁知道又要付出多少的艰辛呢？美好的事物人人都喜欢，然而若不是天资聪颖，卓尔不群，真正有勇气有毅力的人才有一定几率得到。同样，信仰因为付出的努力而变得高贵。</p><p>室友说，人活着就是遭罪。我以前觉得纨绔子弟很帅，老子什么都不在乎，把一切都不放在眼里，多快活，甚至别人说不定还羡慕我。也许，我现在过了那个年龄吧，也许只是吃了太多亏了，不再觉得那是我想要的。我说，5000没过，心里总是个事儿呀，不敢放开了吃，不敢不练了，可是回过头一想，本来就应该这样呀，谁说除了考核前那几周控制饮食，其他时间就要大吃大喝呢？谁说除了考试的复习周，其他时间都要昏天黑地呢？唉，这样活着什么乐趣呢，活着真累。是呀，活着就是遭罪，人生本来就是一次修行呀。</p><p>偶然跟同学交流，她说到我的想法很悲观。我也承认，最近失意的事情确实太多，修行的结果，或是说人生的意义，哦，那就又要回到上一篇文章了。</p><p>总之，这一年挺失败的，从头到尾，挺失败的，然而这一年过去了，没获得什么，就是离这次艰难的修行的终点又近了一步。如果说，我也陈词滥调，要给2017年的自己一个忠告，哦，那请对自己狠一点，再狠一点。</p><p>瞎写，仅代表此时此刻的观点，That’s all.</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Scrapy爬取大众点评</title>
      <link href="/2016/12/15/Scrapy%E7%88%AC%E5%8F%96%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84/"/>
      <url>/2016/12/15/Scrapy%E7%88%AC%E5%8F%96%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="内容">内容：</h2><p>美食品类750个商家：</p><blockquote><ul><li>商家的页面url</li><li>商家的主要信息</li><li>商家所有的促销信息（1200+条）</li><li>商家所有的评论信息（没爬完，14w+条）</li><li>发表这些评论的所有用户的信息（没爬完，5w+条）</li></ul></blockquote><p>代码：https://github.com/preke/Spiders/tree/master/DaZhongDianPing</p><h2 id="总结和学习的一些点">总结和学习的一些点：</h2><ol type="1"><li><p>大众点评有反爬机制，直接运行爬虫，会返回403 forbidden, 所以要换UA, 这里我参考了这篇文章 http://blog.csdn.net/haipengdai/article/details/48545231 （亲测这个中间件太厉害了，同学遇到的输入验证码的问题我都没遇到）</p></li><li><p>scrapy高度集成，设置完 <code>start_urls</code> 之后, <code>start_request()</code> 根据 <code>start_urls</code> 的url生成 <code>Request</code> 对象,然后访问网页返回一个<strong>可迭代</strong>的 <code>Response</code>对象，直接默认回调<code>parse()</code>；<code>parse()</code>返回的<strong>可迭代</strong>的<code>Request</code>对象，直接默认回调<code>start_request()</code>。</p></li></ol><p><strong>注: 这里的Request对象和Response对象是scarpy里定义的：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Response</span><br></pre></td></tr></table></figure><p>具体说明：http://scrapy-chs.readthedocs.io/zh_CN/stable/topics/request-response.html#module-scrapy.http 而具体这个可迭代，因为内部实现时用迭代器，所以返回的时候，可以在返回对象上加一个’[]’</p><ol start="3" type="1"><li>多个爬虫pipline的问题，这样解决：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> spider.name == <span class="string">'url'</span>:</span><br><span class="line">        do url things...</span><br><span class="line">    <span class="keyword">elif</span> spider.name == <span class="string">'shop'</span>:</span><br><span class="line">        sdo url things...</span><br><span class="line">    <span class="keyword">elif</span> spider.name == <span class="string">'promotion'</span>:</span><br><span class="line">        do url things...</span><br><span class="line">    <span class="keyword">elif</span> spider.name == <span class="string">'comment'</span>:</span><br><span class="line">        str1 = type(item)</span><br><span class="line">        <span class="keyword">if</span> str(str1) == <span class="string">"&lt;class 'Xiuxian.items.CommentItem'&gt;"</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'ok'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.saveOrUpdate(self.user_collection, item)</span><br></pre></td></tr></table></figure><ol start="4" type="1"><li><p>遇到ajax要自己定义请求发送，然后遇到302，原因还是ua的问题，我通过<code>requests</code>库的<code>request</code>直接发get请求，发现返回的时重定向的页面，然后把请求委托给scrapy封装的<code>Request</code>,就可以获取信息（因为会通过中间件来换UA）。 推荐使用<code>postman</code>先试一下，如果请求需要参数中有时间戳，可以自己随便设（仅限于本例，其他的还不清楚）</p></li><li><p>后来遇到最大的问题是应对大众点评的反爬机制，中间折腾了各种各样的方法，不再累述，只说问题的结论：</p></li></ol><blockquote><ul><li>我也遇到了输入验证码和403两个问题</li><li><code>crawlera</code>开始收费了</li><li>西次网的代理ip不靠谱，或者说是首页的ip不靠谱</li><li>自己用vmware开了6个虚拟机桥接网络，然后装上<code>polipo</code>做代理</li><li>自己写代理中间件，从代理池中<code>random.choice()</code>的效果不如我写一个时间片轮转的效果好，而且桥接网络的代理ip质量好像也不太高，有些也会被临时禁用。（本质感觉还是代理质量不高）</li><li>最后用几个可用的ip代理做轮转，一个连续发3个请求，然后换另一个，<code>DOWNLOAD_DELAY</code> 设为0.25</li></ul></blockquote><h2 id="总结">总结：</h2><ul><li>自己写的程序质量不高，发送请求的地方应该可以继续优化；</li><li>有高质量的代理肯定是好事，会省很多工作，或许说是偷懒的一种方式</li><li>师兄说了一个点，弄清楚程序发送请求和浏览器发送请求的区别，这是努力的一个点</li><li>引发了一个思考，可能是懂的太少：既然可以用代理池，分布式爬虫的意义在哪里？</li></ul>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>剑指offer 面试题25 二叉树中和为某一值的路径</title>
      <link href="/2016/09/10/aim_to_offer_25/"/>
      <url>/2016/09/10/aim_to_offer_25/</url>
      
        <content type="html"><![CDATA[<p>题目：输入一棵二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。二叉树结点的定义如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">BinaryTreeNode</span>&#123;</span></span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">  BinaryTreeNode* left;</span><br><span class="line">  BinaryTreeNode* right;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="想法">想法：</h2><p>先理解路径，就是树根到叶子节点的条路，那我们先实现出打印出路径的函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindPath</span><span class="params">(BinaryTreeNode* root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root) &#123;</span><br><span class="line">    v.push_back(root-&gt;value);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (!root-&gt;left &amp;&amp; !root-&gt;right) &#123; <span class="comment">//到叶子结点</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); i ++) &#123;</span><br><span class="line">          <span class="built_in">cout</span> &lt;&lt; v[i] &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">      </span><br><span class="line">      v.pop_back();</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (root-&gt;left) </span><br><span class="line">      FindPath(root-&gt;left);</span><br><span class="line">    <span class="keyword">if</span> (root-&gt;right)</span><br><span class="line">      FindPath(root-&gt;right);</span><br><span class="line">    v.pop_back(); <span class="comment">//访问过该节点，就pop掉</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后再想办法加入一个current变量记录当前的和，主要是为了剪枝： 如果中途（还没有到叶子节点）我们的current就已经比目标值sum大（或者相等）的话，就不必去访问该节点的叶子节点了。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v;</span><br><span class="line"><span class="keyword">int</span> current = <span class="number">0</span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FindPath</span><span class="params">(BinaryTreeNode* root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (sum &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">if</span> (root) &#123;</span><br><span class="line">    v.push_back(root-&gt;value);</span><br><span class="line">    current += root-&gt;value;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!root-&gt;left &amp;&amp; !root-&gt;right) &#123;  <span class="comment">//到叶子结点</span></span><br><span class="line">      <span class="keyword">if</span> (current == sum) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); i ++) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; v[i] &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      current -= root-&gt;value;</span><br><span class="line">      v.pop_back();</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//剪枝</span></span><br><span class="line">    <span class="keyword">if</span> (current &gt;= sum) &#123;</span><br><span class="line">      current -= root-&gt;value;</span><br><span class="line">      v.pop_back();</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (root-&gt;left) </span><br><span class="line">      FindPath(root-&gt;left, sum);</span><br><span class="line">    <span class="keyword">if</span> (root-&gt;right)</span><br><span class="line">      FindPath(root-&gt;right, sum);</span><br><span class="line"></span><br><span class="line">    current -= root-&gt;value;</span><br><span class="line">    v.pop_back(); <span class="comment">//访问过该节点，就pop掉</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剑指offer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程同步</title>
      <link href="/2016/08/31/%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5/"/>
      <url>/2016/08/31/%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<p>在多道程序环境下，进程是并发执行的，不同进程之间存在着不同的相互制约关系。为了协调进程之间的相互制约关系，引入了进程同步的概念。</p><h2 id="临界资源">临界资源</h2><p>虽然多个进程可以共享系统中的各种资源，但其中许多资源一次只能为一个进程所使用，我们把一次仅允许一个进程使用的资源称为临界资源。许多物理设备都属于临界资源，如打印机等。此外，还有许多变量、数据等都可以被若干进程共享，也属于临界资源。</p><p>对临界资源的访问，必须互斥地进行，在每个进程中，访问临界资源的那段代码称为临界区。为了保证临界资源的正确使用，可以把临界资源的访问过程分成四个部分：</p><ul><li>进入区。为了进入临界区使用临界资源，在进入区要检查可否进入临界区，如果可以进入临界区，则应设置正在访问临界区的标志，以阻止其他进程同时进入临界区。</li><li>临界区。进程中访问临界资源的那段代码，又称临界段。</li><li>退出区。将正在访问临界区的标志清除。</li><li>剩余区。代码中的其余部分。</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">    entry section;  <span class="comment">//进入区</span></span><br><span class="line">    critical section;  <span class="comment">//临界区</span></span><br><span class="line">    <span class="built_in">exit</span> section;  <span class="comment">//退出区</span></span><br><span class="line">    remainder section;  <span class="comment">//剩余区</span></span><br><span class="line">&#125; <span class="keyword">while</span> (<span class="literal">true</span>)</span><br></pre></td></tr></table></figure><h2 id="同步">同步</h2><p>同步亦称<strong>直接制约关系</strong>，它是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。进程间的直接制约关系就是源于它们之间的相互合作。</p><p>例如，输入进程A通过单缓冲向进程B提供数据。当该缓冲区空时，进程B不能获得所需数据而阻塞，一旦进程A将数据送入缓冲区，进程B被唤醒。反之，当缓冲区满时，进程A被阻塞，仅当进程B取走缓冲数据时，才唤醒进程A。</p><h2 id="互斥">互斥</h2><p>互斥亦称<strong>间接制约关系</strong>。当一个进程进入临界区使用临界资源时，另一个进程必须等待, 当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。</p><p>例如，在仅有一台打印机的系统中，有两个进程A和进程B，如果进程A需要打印时, 系统已将打印机分配给进程B,则进程A必须阻塞。一旦进程B将打印机释放，系统便将进程A唤醒，并将其由阻塞状态变为就绪状态。</p><p>为禁止两个进程同时进入临界区，同步机制应遵循以下准则：</p><ul><li>空闲让进。临界区空闲时，可以允许一个请求进入临界区的进程立即进入临界区。</li><li>忙则等待。当已有进程进入临界区时，其他试图进入临界区的进程必须等待。</li><li>有限等待。对请求访问的进程，应保证能在有限时间内进入临界区。</li><li>让权等待。当进程不能进入临界区时，应立即释放处理器，防止进程忙等待。</li></ul><p>进一步阐释： http://blog.csdn.net/theone10211024/article/details/14052035</p><h1 id="实现临界区互斥的基本方法">实现临界区互斥的基本方法</h1><h2 id="算法一单标志法">算法一：单标志法。</h2><p>该算法设置一个公用整型变量turn,用于指示被允许进入临界区的进程编号，即若turn=0，则允许P0进程进入临界区。该算法可确保每次只允许一个进程进入临界区。但两个进程必须交替进入临界区，如果某个进程不再进入临界区了，那么另一个进程想再次进入临界区，则它将无法进入临界区（违背“空闲让进”）。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// P0进程</span></span><br><span class="line"><span class="keyword">while</span>(turn!=<span class="number">0</span>);</span><br><span class="line">critical section;</span><br><span class="line">turn=<span class="number">1</span>;</span><br><span class="line">remainder section;</span><br><span class="line"></span><br><span class="line"><span class="comment">// P1进程</span></span><br><span class="line"><span class="keyword">while</span>(turn!=<span class="number">1</span>);  <span class="comment">// 进入区</span></span><br><span class="line">critical section;  <span class="comment">// 临界区</span></span><br><span class="line">turn = <span class="number">0</span>;  <span class="comment">// 退出区</span></span><br><span class="line">remainder section;  <span class="comment">// 剩余区</span></span><br></pre></td></tr></table></figure><h2 id="算法二双标志法先检查">算法二：双标志法先检查。</h2><p>该算法的基本思想是在每一个进程访问临界区资源之前，先查看一下临界资源是否正被访问，若正被访问，该进程需等待；否则，进程才进入自己的临界区。为此，设置了一个数据flag[i]，如第i个元素值为FALSE，表示Pi进程未进入临界区，值为TRUE，表示Pi进程进入临界区。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Pi 进程</span></span><br><span class="line"><span class="keyword">while</span>(flag[j]);  <span class="comment">// ①    </span></span><br><span class="line">flag[i]=TRUE;  <span class="comment">// ③  </span></span><br><span class="line">critical section;   </span><br><span class="line">flag[i] = FALSE; </span><br><span class="line">remainder section;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Pj 进程</span></span><br><span class="line"><span class="keyword">while</span>(flag[i]);  <span class="comment">// ② 进入区</span></span><br><span class="line">flag[j] =TRUE;  <span class="comment">// ④ 进入区</span></span><br><span class="line">critical section;  <span class="comment">// 临界区</span></span><br><span class="line">flag[j] = FALSE;  <span class="comment">// 退出区</span></span><br><span class="line">remainder section;  <span class="comment">// 剩余区</span></span><br></pre></td></tr></table></figure><p>优点：不用交替进入，可连续使用；缺点：Pi和Pj可能同时进入临界区。按序列①②③④ 执行时，会同时进入临界区（违背“忙则等待”)。即在检查对方flag之后和切换自己flag 之前有一段时间，结果都检查通过。这里的问题出在检查和修改操作不能一次进行。</p><h2 id="算法三双标志法后检查">算法三：双标志法后检查。</h2><p>算法二是先检测对方进程状态标志后，再置自己标志，由于在检测和放置中可插入另一个进程到达时的检测操作，会造成两个进程在分别检测后，同时进入临界区。为此，算法三釆用先设置自己标志为TRUE后,再检测对方状态标志，若对方标志为TURE，则进程等待；否则进入临界区。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Pi进程</span></span><br><span class="line">flag[i] =TRUE;</span><br><span class="line"><span class="keyword">while</span>(flag[j]);</span><br><span class="line">critical section;</span><br><span class="line">flag[i] =FLASE;</span><br><span class="line">remainder section;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Pj进程</span></span><br><span class="line">flag[j] =TRUE;  <span class="comment">// 进入区</span></span><br><span class="line"><span class="keyword">while</span>(flag[i]);  <span class="comment">// 进入区</span></span><br><span class="line">critical section;  <span class="comment">// 临界区</span></span><br><span class="line">flag [j] =FLASE;   <span class="comment">// 退出区</span></span><br><span class="line">remainder section;  <span class="comment">// 剩余区</span></span><br></pre></td></tr></table></figure><p>当两个进程几乎同时都想进入临界区时，它们分别将自己的标志值flag设置为TRUE，并且同时检测对方的状态（执行while语句），发现对方也要进入临界区，于是双方互相谦让，结果谁也进不了临界区，从而导致“饥饿”现象。</p><h2 id="算法四petersons-algorithm">算法四：Peterson’s Algorithm。</h2><p>为了防止两个进程为进入临界区而无限期等待，又设置变量turn，指示不允许进入临界区的进程编号，每个进程在先设置自己标志后再设置turn 标志，不允许另一个进程进入。这时，再同时检测另一个进程状态标志和不允许进入标志，这样可以保证当两个进程同时要求进入临界区，只允许一个进程进入临界区。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Pi进程</span></span><br><span class="line">flag[i]=TURE; turn=j;</span><br><span class="line"><span class="keyword">while</span>(flag[j]&amp;&amp;turn==j); </span><br><span class="line">critical section;</span><br><span class="line">flag[i]=FLASE;</span><br><span class="line">remainder section;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Pj进程</span></span><br><span class="line">flag[j] =TRUE;turn=i;  <span class="comment">// 进入区</span></span><br><span class="line"><span class="keyword">while</span>(flag[i]&amp;&amp;turn==i);   <span class="comment">// 进入区</span></span><br><span class="line">critical section;  <span class="comment">// 临界区</span></span><br><span class="line">flag[j]=FLASE;  <span class="comment">// 退出区</span></span><br><span class="line">remainder section;  <span class="comment">// 剩余区</span></span><br></pre></td></tr></table></figure><p>本算法的基本思想是算法一和算法三的结合。利用flag解决临界资源的互斥访问，而利用turn解决“饥饿”现象。</p><h2 id="reference">Reference:</h2><p>http://c.biancheng.net/cpp/html/2596.html http://c.biancheng.net/cpp/html/2597.html</p>]]></content>
      
      
      <categories>
          
          <category> 理解计算机 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【Unix网络编程读书笔记】第四章 基本TCP套接字编程</title>
      <link href="/2016/08/30/%E3%80%90Unix%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%91%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%9F%BA%E6%9C%ACTCP%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/"/>
      <url>/2016/08/30/%E3%80%90Unix%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%91%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%9F%BA%E6%9C%ACTCP%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BC%96%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="socket函数">socket函数</h2><p>指定期望的通信协议类型 socket()创建套接字，指定期望的通信协议类型；</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">socket</span><span class="params">(<span class="keyword">int</span> family, <span class="keyword">int</span> type, <span class="keyword">int</span> protocal)</span></span>;</span><br></pre></td></tr></table></figure><ul><li>参数：</li></ul><p>family指明协议族（协议域） type指明套接字类型 protocal某个协议类型常值，或者设为0</p><ul><li>返回值： 非负描述符（sockfd） – 成功，-1 – 出错</li></ul><p>单纯调用socket函数：</p><ul><li>指定了协议族和套接字类型</li><li>没有指定本地协议地址或远程协议地址</li></ul><h2 id="connect函数">connect函数</h2><p>TCP客户用于建立与TCP服务器的连接,可以理解为发送SYN</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">connect</span><span class="params">(<span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr* servaddr, <span class="keyword">socklen_t</span> addrlen)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><p>参数: sockfd: socket函数返回的一个套接字描述符 servaddr: 一个指向套接字地址结构的指针(该结构包括IP地址和端口号) addrlen: 该结构的大小</p></li><li><p>返回值： 若无错误发生，则connect()返回0。 否则的话，返回SOCKET_ERROR错误</p></li></ul><h3 id="客户在调用connect函数之前不必非得调用bind函数需要的话内核会确定源ip地址并选择一个临时端口作为源端口">客户在调用connect函数之前不必非得调用bind函数，需要的话，内核会确定源IP地址，并选择一个临时端口作为源端口。</h3><p>错误返回：</p><ul><li>若TCP客户没有收到SYN分节的相应，则返回ETIMEDOUT错误</li><li>若客户收到RST,表明服务器上没有进程等待与之连接（如服务器进程没在运行）。这是一种<strong>硬错误（hard error）</strong>,用户已接受到RST就马上返回ECONNERFUSED错误</li><li>若客户发出的SYN在中间的某个路由器上引发了一个“destination unreachable”的ICMP错误，则认为是一种<strong>软错误（soft error）</strong>。客户按照时间间隔继续发SYN，如果在规定时间内还没有得到响应，则返回EHOSTUNREACH或ENETUNREACH给客户端进程。 引发该错误的两种原因，1是按照本地转发表到不了服务器的路径，2是connect调用根本不等待就返回。</li></ul><p>如果connect失败，则要close当前的sockfd，并且重新调用socket函数创建新的套接字</p><h2 id="bind函数">bind函数</h2><p>bind函数将一个本地协议地址赋予一个套接字。 协议地址： 32位的IPv4地址或128位的IPv6地址 + 16位TCP/UDP端口号</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bind</span><span class="params">(<span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr* myaddr, <span class="keyword">socklen_t</span> addrlen)</span></span></span><br></pre></td></tr></table></figure><p>参数表和connect很像，同一个sockfd，先bind本机地址，再connect对端地址 后两个参数，可以指定一个，也可以不指定，如上述：客户在调用connect函数之前不必非得调用bind函数，需要的话，内核会确定源IP地址，并选择一个临时端口作为源端口。</p><p>客户机：IP地址为源IP地址 服务器：IP地址意味着服务器只接受那些目的地为这个IP地址的客户机连接</p><p>客户机通常不绑定IP地址到套接字，而是建立连接时，内核将根据所用的外出网络接口来选择源IP地址。 如果服务器没有绑定IP地址，则选用收到的客户机的SYN请求的目的地作为服务器的源IP地址</p><p>如果两者都不指定，则设置IP地址为通配地址，端口号为0</p><p>如果想要知道内核选择的临时的端口值，必须调用getsockname</p><p>返回值：成功为0，不成功为-1</p><p>bind常见的返回错误为EADDRINUSE(Address already in use)地址已使用</p><h2 id="listen函数">listen函数</h2><p>仅由TCP服务器调用，做两件事情：</p><ol type="1"><li>listen函数将一个未连接的主动套接字转换为被动套接字（监听套接字），将CLOSE状态转换到LISTEN状态。</li><li>第二个参数规定了内核应该为相应套接字排队(见下)的最大连接个数</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">listen</span><span class="params">(<span class="keyword">int</span> sockfd, <span class="keyword">int</span> backlog)</span></span>;</span><br></pre></td></tr></table></figure><p>返回值： 若成功返回0，若出错则为-1</p><p>内核为任何一个监听套接字维护两个队列（队列里存的是SYN分节）</p><ul><li><p>未完成连接队列（incomplete connection queue） 每个这样的SYN分节处于TCP三次握手过程中，处于SYN_RCVD状态</p></li><li><p>已完成连接队列（completed connection queue） 已经完成TCP三次握手，处于ESTABLISHED状态</p></li></ul><p>一般来说，两个队列之和不超过backlog</p><p>如果未完成序列满了之后，TCP客户端发送一个SYN分节，服务端不响应，也不发送RST，让TCP期望下一次重传，有可能未完成序列会有位置</p><p>在此理解SYN洪泛攻击就比较清楚了。预防的一种方法是，我们将backlog指定为某个给定套接字上内核为之排队的最大已完成连接数。这样就不必为了提供SYN洪泛的防护而设定一个很大的backlog值。</p><h2 id="accept函数">accept函数</h2><p>由TCP服务器调用，用于从已完成连接队列队头返回一个已完成连接</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span><span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">accept</span><span class="params">(<span class="keyword">int</span> sockfd, struct sockaddr* cliaddr, <span class="keyword">socklen_t</span>* addrlen)</span></span></span><br></pre></td></tr></table></figure><p>第一个参数为监听套接字描述符 后两个参数都是（返回参数，就是我们传入后两个参数，后两个参数会将我们传入的信息（包含客户机地址信息的一个地质结构）记录在一个本地的地址结构里）客户端的信息，标识客户端的协议类型，IP地址，端口号。 （若对客户端信息不感兴趣，可以置空，也就是不记录） 为什么可以置空呢？如果我们在并发服务器上，有多个进程在accept，那如果不保留客户端信息，我们怎么知道该回给哪一个呢？ 我暂时先瞎理解：accept函数是处理的客户端的SYN请求（从已完成连接队列中取出一个SYN分节），那么该分节里本身包含了客户端的源IP地址和端口号，所以即使置空，我们解析包的时候也能够提取到客户端的信息</p><p>若成功返回非负描述符（已连接套接字描述符），不成功返回-1。</p><p>一个服务器（个人觉得是对于一个服务，不保证一个服务器上不同的IP地址和端口号可以处理不同的服务）通常只有一个&lt;font color=“red”-*监听套接字**，然后内核为每个由服务器进程接受的客户创建(通过accept函数)一个&lt;font color=“red”-*已连接套接字**， 当服务完成的时候，相应的已连接套接字关闭。</p><h2 id="fork和exec函数">fork和exec函数</h2><h3 id="fork函数">fork函数：</h3><p>是Unix中派生新进程的唯一方法。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="keyword">pid_t</span> fork(<span class="keyword">void</span>);</span><br></pre></td></tr></table></figure><p>返回值： 在子进程中为0 在父进程中为子进程ID 出错为-1</p><p>父进程调用accept之后调用fork，accept创建的已连接套接字与fork出的子进程共享，之后，子进程继续读写这个已连接套接字，父进程关闭这个已连接套接字。</p><p>fork两个典型用法： 1. 一个进程创建自身的副本，然后两个进程并发执行 2. 一个进程想要执行另一个程序。先fork出自身的一个副本，然后副本调用exec函数，把自身替换成新的程序。</p><h3 id="exec函数">exec函数</h3><p>有6个exec函数，统称为exec函数。 放在硬盘上的可执行文件被Unix执行的唯一方法是：由一个现有进程调用6个exec函数中的某一个，把当前的进程映像替换成新的程序文件，而且新程序同main函数开始执行，进程ID不改变。 调用exec的进程叫做 <strong>调用进程</strong> 新执行的程序为 <strong>新程序</strong></p><h2 id="并发服务器">并发服务器</h2><p>我觉得这段用书上的代码解释应该非常清楚： 都用了包裹函数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Listen(listenfd, backlog);</span><br><span class="line"><span class="keyword">for</span> (;;) &#123;</span><br><span class="line">  connfd = Accept(listenfd, ...); </span><br><span class="line">  <span class="keyword">if</span> ( (pid = Fork()) == <span class="number">0</span> ) &#123; <span class="comment">//成功创建子进程</span></span><br><span class="line">    Close(listenfd); <span class="comment">//子进程关闭监听套接字, 父进程可以继续监听</span></span><br><span class="line">    doit(connfd); <span class="comment">//子进程在已连接套接字上读写</span></span><br><span class="line">    Close(connfd); <span class="comment">//完成与客户机的交互，断开连接</span></span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);<span class="comment">//正常退出</span></span><br><span class="line">  &#125;</span><br><span class="line">  Close(connfd); <span class="comment">//父进程关闭已连接套接字</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>但是我觉得如果做到并发的话，第6~9行和第11行应该同时执行。</p><h2 id="close函数">close函数</h2><p>Unix中close函数也用来关闭套接字，断开TCP连接</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">close</span><span class="params">(<span class="keyword">int</span> sockfd)</span></span>;</span><br></pre></td></tr></table></figure><p>返回值： 成功为0 出错为-1</p><p>close将一个套接字标记为关闭，然后返回调用进程； 被标记为关闭的套接字不能再由调用进程使用，也就是不能再作为read和write的第一个参数。</p><h3 id="描述符引用计数">描述符引用计数</h3><p>通俗理解的话：并发中，fork会让对应的套接字引用计数加1，close函数会让对应的套接字引用计数减1，该计数被父进程和子进程共享（可读写），只有当该计数为0时，才会终止TCP连接，4次挥手。</p><h2 id="getsockname和getpeername函数">getsockname和getpeername函数</h2><p>getsockname返回某个套接字的本机协议地址 getpeername返回某个套接字所关联的外地协议地址 返回在这里，是返回参数的意思，即将信息填充到参数指向的结构中</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span><span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getsockname</span><span class="params">(<span class="keyword">int</span> sockfd, struct sockaddr* localaddr, <span class="keyword">socklen_t</span>* addrlen)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getpeername</span><span class="params">(<span class="keyword">int</span> sockfd, struct sockaddr* peeraddr, <span class="keyword">socklen_t</span>* addrlen)</span></span>;</span><br></pre></td></tr></table></figure><p>返回值： 均为若成功：返回0 若失败：返回-1</p><p>用途：</p><ul><li>没有显式bind时，connect成功后，getsockname用于返回内核赋予该连接的IP地址和端口号；</li><li>bind时端口号参数为0时，connect成功后，getsockname用于返回内核赋予该连接的本地端口号；</li><li>getsockname用于获取套接字地址的地址族</li><li>服务器采用通配地址bind时，对已连接套接字调用getsockname也可以得到IP地址和端口号</li><li>服务器通过调用accept的进程通过exec执行程序时，获取客户身份的唯一途径是getpeername。</li><li>Telnet服务器首先调用的函数之一就是getpeername</li></ul>]]></content>
      
      
      <categories>
          
          <category> UNP读书笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>剑指offer 面试题6 重建二叉树</title>
      <link href="/2016/08/28/aim_to_offer_06/"/>
      <url>/2016/08/28/aim_to_offer_06/</url>
      
        <content type="html"><![CDATA[<p>题目就是输入前序和中序来实现二叉树的重建，虽然感觉自己人为操作思路很清晰，但是写到程序里会觉得很繁琐，以前实现过，但是如果是面试的话，还是要熟练才好，真正理解每一句代码的思路。所以也算是克服恐惧来仔细分析一次吧。</p><p>（我没有用书上的代码来分析，而是用之前自己参照网上的版本写的）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">BinaryTreeNode</span>&#123;</span></span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">  BinaryTreeNode* pLeft;</span><br><span class="line">  BinaryTreeNode* pRight;</span><br><span class="line">&#125;BinaryTreeNode;</span><br></pre></td></tr></table></figure><p>结构体如上；</p><p>之前的实现（也是参照网上流传很广的版本）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">BinaryTreeNode* <span class="title">buildByPreAndIn</span><span class="params">(<span class="keyword">int</span>* pre_order, <span class="keyword">int</span>* in_order, <span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (pre_order == <span class="literal">NULL</span> || in_order == <span class="literal">NULL</span> || num &lt;= <span class="number">0</span>)  <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    BinaryTreeNode* root = <span class="keyword">new</span> BinaryTreeNode;</span><br><span class="line">    root-&gt;value = *pre_order;</span><br><span class="line">    root-&gt;pLeft = root-&gt;pRight = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> rootPositionInOrder = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (in_order[i] == root-&gt;value) &#123;</span><br><span class="line">            rootPositionInOrder = i;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> num_Left  = rootPositionInOrder;</span><br><span class="line">    <span class="keyword">int</span> num_Right = num - num_Left - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span>* pre_order_left  = pre_order + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span>* in_order_left   = in_order;</span><br><span class="line">    root-&gt;pLeft          = buildByPreAndIn(pre_order_left, in_order_left, num_Left);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span>* pre_order_right = pre_order + num_Left + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span>* in_order_right  = in_order + num_Left + <span class="number">1</span>;</span><br><span class="line">    root-&gt;pRight         = buildByPreAndIn(pre_order_right, in_order_right, num_Right);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>好的我们先看函数头：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">BinaryTreeNode* <span class="title">buildByPreAndIn</span><span class="params">(<span class="keyword">int</span>* pre_order, <span class="keyword">int</span>* in_order, <span class="keyword">int</span> num)</span></span>;</span><br></pre></td></tr></table></figure><p>前两个参数就是前序和中序数组的首地址，那么num呢？就是数组的长度（两个一样长）。</p><p>然后是终止条件，也很清晰；</p><p>接着用前序数组的第一个元素，创建一个节点，既是根节点，又是之后每次递归的时候创建的新节点。</p><p>然后去找中序数组中，前序数组第一个节点的位置，以此来区分左右子树。</p><p>然后用两个变量： <code>num_Left</code>记录左子树节点的个数； <code>num_Right</code>记录右子树节点的个数；</p><p>知道了长度，还要知道前序中序两个数组的首地址吧，然后再用两个变量： <code>pre_order_left</code>记录左子树前序子数组的起始位置； <code>in_order_left</code>记录左子树中序子数组的起始位置； 两个长度相等，所以递归调用函数创建左子树。</p><p>右子树是相同的过程，重要的是找起始位置的时候要空出根节点。</p><p>然后返回创建的树的根节点。</p><p>这样一分析，就觉得思路更加明晰了，我觉得就算是面试，也是要记下这个思路吧。</p>]]></content>
      
      
      <categories>
          
          <category> 面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剑指offer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>剑指offer 面试题1 赋值运算符函数</title>
      <link href="/2016/08/24/aim_to_offer_01/"/>
      <url>/2016/08/24/aim_to_offer_01/</url>
      
        <content type="html"><![CDATA[<p>题目要求为下面一个类实现一个赋值运算符的函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CMyString</span> &#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">    CMyString(<span class="keyword">char</span>* pData = <span class="literal">NULL</span>);</span><br><span class="line">    CMyString(<span class="keyword">const</span> CMyString&amp; str);</span><br><span class="line">    ~CMyString();</span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">char</span>* m_pData;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>我们要实现任意的一个类的赋值运算符函数，都要注意一些方面：</p><ul><li>返回值类型是类引用，返回*this</li><li>参数类型是常量引用</li><li>如果有动态内存，是否发生内存泄露</li><li>如果传入参数为调用者自身，直接返回</li></ul><p>书上的解法给的确实很完备，而且后来用了一种巧妙的方法让程序自动释放内存。但我觉得重点是他提到的异常安全性的处理，这里的话如果我们在new的时候内存不够，而我们又已经delete掉原来的内存，就会导致调用者的内存被无故释放了；所以书上采用新建临时变量，然后再让*this的m_pData指针和临时变量的m_pDatam做交换，就算如果新建临时变量的时候内存不够，也可以保证调用者调用失败也没有被改变。</p><p>但是我想了一下，异常安全性的思想是很宝贵，但是实现起来其实可以用try catch这样也很直观：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CMyString &amp; CMyString::<span class="keyword">operator</span> = (<span class="keyword">const</span> CMyString&amp; str) &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span> == &amp;str) &#123;</span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="keyword">delete</span> [] <span class="keyword">this</span>-&gt;m_pData;</span><br><span class="line">    <span class="keyword">this</span>-&gt;m_pData = <span class="keyword">new</span> <span class="keyword">char</span> [<span class="built_in">strlen</span>(str.m_pData) + <span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(<span class="keyword">this</span>-&gt;m_pData, str.m_pData);</span><br><span class="line">  &#125; <span class="keyword">catch</span>(<span class="built_in">std</span>::exception&amp; e) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; e.what() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"申请内存错误，赋值失败，返回原对象"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里提到赋值运算符的话，就要顺便说一下拷贝构造函数；</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CMyString s2;</span><br><span class="line">CMyString s1 = s2; <span class="comment">//这一行其实调用的是拷贝构造函数</span></span><br></pre></td></tr></table></figure><p>区别就在于，拷贝构造函数的调用，之前是没有对象的，所以如果有动态内存，拷贝构造函数肯定不会先去delete； 而赋值运算符是讲一个对象赋给另外一个对象，之前是有对象的，这么一来就有了对之前对象的处理。</p><p>我把所有函数的实现和一些注意事项放在这里啦~</p><p>https://github.com/preke/AimToOffer/blob/master/01.c%2B%2B</p>]]></content>
      
      
      <categories>
          
          <category> 面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 剑指offer </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
